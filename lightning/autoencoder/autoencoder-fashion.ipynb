{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lit-autoencoder.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/lightning/autoencoder/autoencoder-fashion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLG3Z2JhFlnF"
      },
      "source": [
        "<img src=\"https://i.imgur.com/gb6B4ig.png\" width=\"400\" alt=\"Weights & Biases\" />\n",
        "\n",
        "# Autoencoder Networks for FashionMNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Opcs1SGdFHbh"
      },
      "source": [
        "%%capture\n",
        "!pip install pytorch-lightning==1.3.8 torchviz wandb\n",
        "!git clone https://github.com/wandb/lit_utils\n",
        "!cd \"/content/lit_utils\" && git pull\n",
        "\n",
        "import math\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import wandb\n",
        "\n",
        "import lit_utils as lu\n",
        "\n",
        "lu.utils.filter_warnings()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwsgRtTxy6iJ"
      },
      "source": [
        "wandb.login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDQX48F_Fw_o"
      },
      "source": [
        "class LitAE(lu.nn.modules.LoggedLitModule):\n",
        "  \"\"\"Generic autoencoder class for PyTorch Lightning.\n",
        "  \n",
        "  Also includes some under-the-hood Weights & Biases logging.\n",
        "\n",
        "  Provide an encoder and decoder (both pl.LightningModules)\n",
        "  and a config with information about the optimizer,\n",
        "  and this class will create the autoencoder, set up the optimizers,\n",
        "  and add output quality metric tracking via the Peak Signal-to-Noise Ratio.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, encoder, decoder, config):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    \n",
        "    self.loss = config[\"loss\"]\n",
        "    self.optimizer = config[\"optimizer\"]\n",
        "    self.optimizer_params = config[\"optimizer.params\"]\n",
        "\n",
        "    # quality metric: peak signal-to-noise ratio, in decibels\n",
        "    psnr = pl.metrics.PSNR()\n",
        "    self.training_metrics = torch.nn.ModuleList([psnr.clone()])\n",
        "    self.validation_metrics = torch.nn.ModuleList([psnr.clone()])\n",
        "\n",
        "  def forward(self, x):  # produce outputs\n",
        "    return self.decoder(self.encoder(x))\n",
        "\n",
        "  def configure_optimizers(self):  # âš¡: setup for .fit\n",
        "    return self.optimizer(self.parameters(), **self.optimizer_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXRVKx-AxjrB"
      },
      "source": [
        "## Fully-Connected Encoder and Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KF2skzAaGtCY"
      },
      "source": [
        "class EncoderFC(pl.LightningModule):\n",
        "  \"\"\"Fully-connected/torch.nn.Linear encoder.\n",
        "\n",
        "  Pass in any configuration hyperparameters via the config argument.\n",
        "  Applies resizing to inputs via AdapativeAvgPool2d so that it\n",
        "  can work on images of varying size. Produces an encoding of the image\n",
        "  as a vector with dimension config[\"encoding_dim\"].\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.resize_layer = torch.nn.AdaptiveAvgPool2d(output_size=self.config[\"target_size\"])\n",
        "    self.flat_input_size = get_flat_size(self.config[\"target_size\"])\n",
        "    self.layers = torch.nn.Sequential(\n",
        "        # add modules here\n",
        "        torch.nn.Linear(self.flat_input_size, 128),\n",
        "        self.config[\"activation\"](),\n",
        "        torch.nn.Linear(128, 64),\n",
        "        self.config[\"activation\"](),\n",
        "        torch.nn.Linear(64, self.config[\"encoding_dim\"]),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.resize_layer(x)\n",
        "    x = torch.flatten(x, start_dim=1)  # flatten all except batch dimension\n",
        "\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "class DecoderFC(pl.LightningModule):\n",
        "  \"\"\"Fully-connected/torch.nn.Linear decoder.\n",
        "\n",
        "  Pass in any configuration hyperparameters via the config argument.\n",
        "  Applies resizing to outputs via AdapativeAvgPool2d so that it\n",
        "  can work on images of varying size. Consumes an encoding of the image\n",
        "  as a vector with dimension config[\"encoding_dim\"].\n",
        "  Applies resizing to inputs so that it can work on images of varying size.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.flat_output_size = get_flat_size(self.config[\"target_size\"])\n",
        "    self.layers = torch.nn.Sequential(\n",
        "      # add modules here                                  \n",
        "      torch.nn.Linear(self.config[\"encoding_dim\"], 64),\n",
        "      self.config[\"activation\"](),\n",
        "      torch.nn.Linear(64, 128),\n",
        "      self.config[\"activation\"](),\n",
        "      torch.nn.Linear(128, self.flat_output_size),\n",
        "      self.config[\"activation\"](),\n",
        "    )\n",
        "    self.resize_layer = torch.nn.AdaptiveAvgPool2d(output_size=self.config[\"image_size\"])\n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "\n",
        "    x = torch.reshape(x, get_new_dims(x))  # reverse of flatten\n",
        "    x = self.resize_layer(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def get_new_dims(x):\n",
        "  assert len(x.shape) == 2, \"expects a batch of vectors\"\n",
        "  batch, length = x.shape\n",
        "  rows = int(math.sqrt(length))\n",
        "  new_dims = (x.shape[0], 1, rows, -1)\n",
        "\n",
        "  return new_dims\n",
        "\n",
        "\n",
        "def get_flat_size(image_size):\n",
        "  return np.prod(image_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQ85egfSxoEa"
      },
      "source": [
        "## Convolutional Encoder and Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emfFF0B3hrPp"
      },
      "source": [
        "class EncoderConv(pl.LightningModule):\n",
        "  \"\"\"Convolutional encoder.\n",
        "\n",
        "  Pass in any configuration hyperparameters via the config argument.\n",
        "  Applies resizing to inputs via AdapativeAvgPool2d so that it\n",
        "  can work on images of varying size. Produces an encoding of the image\n",
        "  as a 3d Tensor with channel dimension config[\"encoding_dim\"].\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.resize_layer = torch.nn.AdaptiveAvgPool2d(output_size=self.config[\"image_size\"])\n",
        "    self.layers = torch.nn.Sequential(\n",
        "      # add modules here\n",
        "      torch.nn.Conv2d(1, 32, kernel_size=5),\n",
        "      self.config[\"activation\"](),\n",
        "      torch.nn.Conv2d(32, self.config[\"encoding_dim\"], kernel_size=3),\n",
        "      self.config[\"activation\"](),\n",
        "      torch.nn.MaxPool2d(kernel_size=2),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.resize_layer(x)\n",
        "\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class DecoderConv(pl.LightningModule):\n",
        "  \"\"\"Convolutional decoder.\n",
        "\n",
        "  Pass in any configuration hyperparameters via the config argument.\n",
        "  Applies resizing to outputs via AdapativeAvgPool2d so that it\n",
        "  can work on images of varying size. Consumes an encoding of the image\n",
        "  as a 3d Tensor with channel dimension config[\"encoding_dim\"].\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.layers = torch.nn.Sequential(\n",
        "        # add modules here\n",
        "        torch.nn.ConvTranspose2d(self.config[\"encoding_dim\"], 32, kernel_size=3, stride=2),\n",
        "        self.config[\"activation\"](),\n",
        "        torch.nn.ConvTranspose2d(32, 1, kernel_size=5),\n",
        "        self.config[\"activation\"](),\n",
        "    )\n",
        "    self.resize_layer = torch.nn.AdaptiveAvgPool2d(output_size=self.config[\"image_size\"])\n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "\n",
        "    x = self.resize_layer(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wr7gwLvxxsXg"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cn1c8XLNxtyg"
      },
      "source": [
        "To run training, execute the cell below.\n",
        "You can configure the network and training procedure\n",
        "by changing the values of the `config` dictionary.\n",
        "\n",
        "Use the value of `erase` to switch tasks:\n",
        "when `erase` is `True`,\n",
        "a random portion of the input (but not the output!)\n",
        "is erased before being fed to the network,\n",
        "which makes the task a form of\n",
        "[image in-painting](https://wandb.ai/site/articles/introduction-to-image-inpainting-with-deep-learning).\n",
        "When it is `False`,\n",
        "the input is unaltered,\n",
        "and the task is a vanilla reconstruction task.\n",
        "\n",
        "In between training runs,\n",
        "especially runs that crashed,\n",
        "you may wish to restart the notebook\n",
        "and re-run the preceding cells\n",
        "to get rid of accumulated state\n",
        "(`Runtime > Restart runtime`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwjG6u9iLfaC"
      },
      "source": [
        "###\n",
        "# Setup Hyperparameters, Data, and Model\n",
        "###\n",
        "\n",
        "\n",
        "config = {  # dictionary of configuration hyperparameters\n",
        "  \"batch_size\": 32,  # number of examples in a single batch\n",
        "  \"max_epochs\": 10,  # number of times to pass over the whole dataset\n",
        "  \"image_size\": (28, 28),  # size of images in this dataset\n",
        "  \"target_size\": (28, 28),  # size of resized images fed to network\n",
        "  \"encoding_dim\": 16,  # size/channel count of encoding of input\n",
        "  \"loss\": torch.nn.MSELoss(),  # loss function\n",
        "  \"erase\": False,  # set to False to deactivate input erasing, True to activate\n",
        "  \"activation\": torch.nn.ReLU,  # activation function class (instantiated later)\n",
        "  \"optimizer\": torch.optim.Adam,  # optimizer class (instantiated later)\n",
        "  \"optimizer.params\":  # dict of hyperparameters for optimizer\n",
        "    {\"lr\": 0.0001,  # learning rate to scale gradients\n",
        "     \"weight_decay\": 0}  # if non-zero, reduce weights each batch\n",
        "}\n",
        "\n",
        "# ðŸ“ if activated erases part of the image on each load\n",
        "eraser = torchvision.transforms.RandomErasing(\n",
        "    p=config[\"erase\"], scale=[0.1, 0.2], ratio=[0.3, 3.3], value=0.13)\n",
        "transform = torchvision.transforms.Compose(\n",
        "    [torchvision.transforms.ToTensor(), eraser])\n",
        "\n",
        "# ðŸ“¸ set up the dataset of images\n",
        "dmodule = lu.datamodules.AutoEncoderFashionMNISTDataModule(\n",
        "    batch_size=config[\"batch_size\"],\n",
        "    transform=transform)\n",
        "dmodule.prepare_data()\n",
        "dmodule.setup()\n",
        "\n",
        "# grab samples to log outputs on\n",
        "samples = next(iter(dmodule.val_dataloader()))\n",
        "\n",
        "# ðŸ¥… instantiate the network\n",
        "encoder = EncoderFC(config)\n",
        "decoder = DecoderFC(config)\n",
        "ae = LitAE(encoder, decoder, config)\n",
        "\n",
        "###\n",
        "# Train the model\n",
        "###\n",
        "\n",
        "\n",
        "with wandb.init(project=\"lit-ae\", entity=\"wandb\", config=config) as run:\n",
        "\n",
        "  # ðŸªµ configure logging\n",
        "  cbs = [lu.callbacks.WandbCallback(),  # callbacks add extra features, like better logging\n",
        "         lu.callbacks.FilterLogCallback(image_size=(28, 28), log_input=True, log_output=True), # logs the input and output weights to Weights & Biases\n",
        "         lu.callbacks.ImageLogCallback(samples) # logs inputs and outputs to Weights & Biases\n",
        "        ]\n",
        "\n",
        "  wandblogger = pl.loggers.WandbLogger(log_model=True, save_code=True)\n",
        "  # ðŸ‘Ÿ configure Trainer \n",
        "  trainer = pl.Trainer(gpus=1,  # use the GPU for .forward\n",
        "                      logger=wandblogger,  # log to Weights & Biases\n",
        "                      max_epochs=config[\"max_epochs\"], log_every_n_steps=1,\n",
        "                      callbacks=cbs,\n",
        "                      progress_bar_refresh_rate=50)\n",
        "                      \n",
        "  # ðŸƒâ€â™€ï¸ run the Trainer on the model\n",
        "  trainer.fit(ae, dmodule)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfCsy5odrCYG"
      },
      "source": [
        "### Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "872a5Cba1szs"
      },
      "source": [
        "The cell above will output links to Weights & Biases dashboards\n",
        "where you can review the training process and the final resulting model.\n",
        "\n",
        "These dashboards will be useful in working through the exercises below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eCNHuIWwRR1"
      },
      "source": [
        "#### 1. Choosing an Output Activation\n",
        "\n",
        "The default configuration uses a\n",
        "[`ReLU` activation](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU)\n",
        "on all layers.\n",
        "This activation has a high probability of outputting exactly zero.\n",
        "Review the logged outputs of your neural network\n",
        "in the Weights & Biases interface and look\n",
        "for issues caused by this choice of activation.\n",
        "If you notice any, try correcting them by using a different activation:\n",
        "[`LeakyReLU`](https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html#torch.nn.LeakyReLU),\n",
        "[`Sigmoid`](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html#torch.nn.Sigmoid), or\n",
        "[`SiLU`](https://pytorch.org/docs/stable/generated/torch.nn.SiLU.html#torch.nn.SiLU).\n",
        "\n",
        "> _Note_: Outputs are typically quite different\n",
        "from the internal parts of the network,\n",
        "like activation values.\n",
        "For example, here they are always between 0 and 1,\n",
        "to match the data they are compared against,\n",
        "while activation values have no such restriction.\n",
        "Therefore, it's not necessary or even advisable to use\n",
        "the same activation for the output layer as for the hidden layers.\n",
        "This will require you to slightly change the network code above!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIYm4qLm4Y5n"
      },
      "source": [
        "#### 2. Change Hyperparameters\n",
        "\n",
        "Better results can be obtained by tweaking the hyperparameters.\n",
        "Try out different values in the `config`\n",
        "and in the model definition and see what happens\n",
        "to your results.\n",
        "\n",
        "Here are some suggestions:\n",
        "- Does increasing `batch_size` from `32` help or hurt?\n",
        "What happens to the runtime if you decrease it to `1`?\n",
        "What happens if you increase it to the maximum of `50_000`?\n",
        "- If you increase `max_epochs` from `10` and train for longer,\n",
        "does the model get better or worse?\n",
        "- What happens when you increase the size of the model? You can do this by\n",
        "increasing the value of `encoding_dim`,\n",
        "increasing the number of hidden layers,\n",
        "or increasing the hidden layer sizes\n",
        "(set by the first two arguments of `Linear` and `Conv2D`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRQmbRtixeex"
      },
      "source": [
        "#### 3. Convolutional Autoencoders\n",
        "\n",
        "Try out the convolutional version of the autoencoder\n",
        "(`encoder = EncoderConv` and `decoder = DecoderConv`).\n",
        "Compare its performance on the usual autoencoder task (`erase=0.`)\n",
        "and on the in-filling task (`erase=1.`)\n",
        "to that of the fully-connected network.\n",
        "Which style of network does better?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5P-Azo9J0KaP"
      },
      "source": [
        "#### **Challenge**: Regularization and Learned Weights\n",
        "In addition to logging the inputs, outputs, and metrics,\n",
        "the training run also logs the \"filters\" for the network --\n",
        "the first and last weights, which are applied directly to the inputs\n",
        "and directly produce the outputs, respectively.\n",
        "\n",
        "These filters can be interpreted as images,\n",
        "helping us see what the network is looking for in the inputs\n",
        "and using to construct the outputs.\n",
        "\n",
        "In a well-trained neural network,\n",
        "these filters would look like the \"units\" from which our inputs are built:\n",
        "tiny edges or patches of brightness and/or darkness.\n",
        "\n",
        "With the default settings, the learned filters don't look like that at all.\n",
        "They either look like [white noise](https://en.wikipedia.org/wiki/White_noise),\n",
        "like the static pattern that appears on a detuned television screen,\n",
        "or they look like entire inputs memorized from the training set.\n",
        "\n",
        "Getting the weights to converge to good filters is challenging.\n",
        "You might use any or all of the following techniques:\n",
        "- Increase `max_epochs` and continue training\n",
        "long after the loss has stopped decreasing\n",
        "- Add\n",
        "[DropOut](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html)\n",
        "([DropOut2d](https://pytorch.org/docs/stable/generated/torch.nn.Dropout2d.html#torch.nn.Dropout2d)\n",
        "for convolutional networks)\n",
        "after the activation layers.\n",
        "Use a drop probability between `0.1` and `0.5`.\n",
        "- Increase the `weight_decay` parameter of the optimizer,\n",
        "trying values between `1e-12` and `1e-2`.\n",
        "Weight decay is similar to ridge regression\n",
        "or $\\ell_2$-regularization from traditional ML\n",
        "([read more here](https://towardsdatascience.com/this-thing-called-weight-decay-a7cd4bcfccab))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JE5lBoOScKh0"
      },
      "source": [
        "#### **Challenge**: Skip Connections\n",
        "\n",
        "A common technique for improving performance in computer vision models is adding\n",
        "[skip connections](https://theaisummer.com/skip-connections/) --\n",
        "transformations that \"skip over\" intervening layers.\n",
        "These \"shortcuts\" allow information to flow more smoothly through the network\n",
        "and stabilize training -- enabling more choices of optimizer,\n",
        "layer size, and nonlinearity to reach good performance.\n",
        "\n",
        "In a small autoencoder like this one, we might write one skip connection\n",
        "from the input to the hidden layer\n",
        "and another from the hidden layer to the output.\n",
        "Try adding these to `EncoderConv` and `DecoderConv`.\n",
        "\n",
        "> _Note_: In the style of the encoder and decoder modules above,\n",
        "a module with a skip connection might have a `.forward` method like this one:\n",
        "```python\n",
        "def forward(self, xs):\n",
        "  skip = self.skip(xs)\n",
        "  for layer in self.layers:\n",
        "    xs = layer(xs)\n",
        "  return xs + skip\n",
        "```\n",
        "where `.skip` is a `torch.nn.Linear` or `torch.nn.Conv2D` layer. Notice that `xs` and `skip` are added together, and so need to be the same shape!\n",
        "\n",
        "> _Note_: In the approach popularized by\n",
        "[Residual Networks](https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035),\n",
        "the skip connection doesn't transform the input at all\n",
        "(akin, in the example above, to using `torch.nn.Identity` for `skip`).\n",
        "This allows for very efficient and stable gradient flow\n",
        "in networks of extreme depth,\n",
        "but requires that the skipped-over layers don't change the shape."
      ]
    }
  ]
}