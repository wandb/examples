{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/tensorboard/Accelerator_W&B_Tensorboard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook accompanies the report titled \"[Tensorboard with Accelerators - A Guide](https://wandb.ai/sauravmaheshkar/Accelerator-TensorBoard/reports/Tensorboard-with-Accelerators-A-Guide--Vmlldzo5Nzk2MzM)\"\n",
    "\n",
    "---\n",
    "##### Most of the code in this notebook is ported from the [**\"Transfer learning and fine-tuning\"**](https://www.tensorflow.org/tutorials/images/transfer_learning) tutorial from the Tensorflow website in order to demonstrate easy integration of the wandb client into pre-existing workflows.\n",
    "\n",
    "Author: [@SauravMaheshkar](https://twitter.com/MaheshkarSaurav)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages üì¶ and Basic Setup\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Clear any TensorBoard logs from previous runs\n",
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Install the latest version of wandb client üî•üî•\n",
    "!pip install -q --upgrade wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Configuration using **`wandb.config`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# log to Weights and biases\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Feel free to change these and experiment !!\n",
    "config = wandb.config\n",
    "config.AUTOTUNE = tf.data.AUTOTUNE\n",
    "config.BATCH_SIZE = 32\n",
    "config.IMG_SIZE = (160, 160)\n",
    "config.base_learning_rate = 0.0001\n",
    "config.seed = 2021\n",
    "config.randomfliptype = \"horizontal_and_vertical\"\n",
    "config.randomrotationfactor = 0.2\n",
    "config.randomcontrastfactor = 0.2\n",
    "config.IMG_SHAPE = config.IMG_SIZE + (3,)\n",
    "config.epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üíø The Dataset\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code cell we:-\n",
    "\n",
    "* download and extract a zip file containing images\n",
    "* create a `tf.data.Dataset` using `tf.keras.preprocessing.image_dataset_from_directory`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract\n",
    "_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n",
    "path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)\n",
    "PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')\n",
    "\n",
    "# Create path variables for training and validation images\n",
    "train_dir = os.path.join(PATH, 'train')\n",
    "validation_dir = os.path.join(PATH, 'validation')\n",
    "\n",
    "# Create train and validation tf.data.Dataset's\n",
    "train_dataset = image_dataset_from_directory(train_dir,\n",
    "                                             shuffle=True,\n",
    "                                             batch_size=config.BATCH_SIZE,\n",
    "                                             image_size=config.IMG_SIZE)\n",
    "\n",
    "validation_dataset = image_dataset_from_directory(validation_dir,\n",
    "                                                  shuffle=True,\n",
    "                                                  batch_size=config.BATCH_SIZE,\n",
    "                                                  image_size=config.IMG_SIZE)\n",
    "\n",
    "class_names = train_dataset.class_names\n",
    "# Add Class Names as config to wandb\n",
    "config.class_names = train_dataset.class_names\n",
    "\n",
    "# Split the dataset\n",
    "val_batches = tf.data.experimental.cardinality(validation_dataset)\n",
    "test_dataset = validation_dataset.take(val_batches // 5)\n",
    "validation_dataset = validation_dataset.skip(val_batches // 5)\n",
    "train_dataset = train_dataset.prefetch(buffer_size=config.AUTOTUNE)\n",
    "validation_dataset = validation_dataset.prefetch(buffer_size=config.AUTOTUNE)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=config.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚úçÔ∏è Model Architecture \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèó Transfer Learning\n",
    "> Excerts from Datacamp Tutorials and MachineLearningMastery blogs\n",
    "\n",
    "First things first; Transfer learning(TL) is not a machine learning model or technique; it is rather a \"**design methodology**\" within machine learning. The general idea of transfer learning is to use knowledge learned from tasks for which a lot of labelled data is available in settings where only little labelled data is available. Creating labelled data is expensive, so optimally leveraging existing datasets is key.\n",
    "\n",
    "In a traditional machine learning model, the primary goal is to generalise to unseen data based on patterns learned from the training data. With transfer learning, you attempt to kickstart this generalisation process by starting from patterns that have been learned for a different task. Essentially, instead of starting the learning process from a (often randomly initialised) blank sheet, you start from patterns that have been learned to solve a different task.\n",
    "\n",
    "Convolutional Neural Networks' features are more generic in early layers and more original-dataset-specific in later layers. Thus, we often use these as a backbone / starting point while creating new models. A common practice is to make these base models non-trainable and just learn the later layers. You might think that this will decrease the performance, but as we'll see from training. Transfer Learning is still a viable option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è DenseNet121\n",
    "\n",
    "![](https://pytorch.org/assets/images/densenet1.png)\n",
    "\n",
    "> From PyTorch Hub\n",
    "\n",
    "**Dense Convolutional Network (DenseNet)**, connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with $L$ layers have $L$ connections - one between each layer and its subsequent layer - our network has $L(L+1)/2$ direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters.\n",
    "\n",
    "In this project we'll use [**`DenseNet121`**](https://www.tensorflow.org/api_docs/python/tf/keras/applications/densenet) for training our Binary Classifier. The Model can easily be instantiated using the **`tf.keras.applications`** Module, which provides canned architectures with pre-trained weights. For more details kindly visit this [link](https://www.tensorflow.org/api_docs/python/tf/keras/applications/densenet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "\n",
    "  base_model = tf.keras.applications.densenet.DenseNet121(include_top=False, \n",
    "                                                          weights='imagenet',\n",
    "                                                          input_shape=config.IMG_SHAPE\n",
    "  )\n",
    "\n",
    "  preprocess_input = tf.keras.applications.densenet.preprocess_input\n",
    "  \n",
    "  data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(mode=config.randomfliptype, seed = config.seed),\n",
    "    tf.keras.layers.RandomRotation(factor = config.randomrotationfactor, seed = config.seed),\n",
    "    tf.keras.layers.RandomContrast(factor = config.randomcontrastfactor, seed=config.seed)\n",
    "  ])\n",
    "\n",
    "  base_model.trainable = True\n",
    "\n",
    "  inputs = tf.keras.Input(shape=(160, 160, 3))\n",
    "\n",
    "  x = data_augmentation(inputs)\n",
    "  x = preprocess_input(x)\n",
    "  x = base_model(x, training=False)\n",
    "  x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "  x = tf.keras.layers.Dropout(0.2)(x)\n",
    "  outputs = tf.keras.layers.Dense(1)(x)\n",
    "  model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß± + üèó = üè† Training\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W&B automatically logs metrics from TensorBoard into dashboards. If you've got a pre-existing training workflow based around Tensorboard, switching to wandb requires just 2 lines of code.\n",
    "\n",
    "1. `wandb.tensorboard.patch(root_logdir=\"...\")`\n",
    "2. `run = wandb.init(..., sync_tensorboard = True)`\n",
    "\n",
    "W&B client will then automatically log all the metrics generated during training from the logs directory into it's amazing dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## You don't even have to worry about accelerator's !!!\n",
    "\n",
    "Just create a distribute strategy for either GPUs or TPUs and **`wandb`** automatically logs all the system metrics like GPU Power Usage, GPU Memory Allocated, GPU Temp, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Easy Model Versioning with W&B Artifacts üè™\n",
    "\n",
    "It's no secret that training deep learning models takes a lot of time, that's why we use Weights and Biases Artifacts to store our models weights and graphs for easy reproducibility. \n",
    "\n",
    "W&B artifacts allows you to store different versions of your datasets and models in the cloud as Artifacts.\n",
    "\n",
    "For example all the model files for this project can be found [here](https://wandb.ai/sauravmaheshkar/Accelerator-TensorBoard/artifacts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"tensorboard-accelerator-wandb\"\n",
    "ENTITY = None # Replace with your entity name or Team name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point wandb client to the log dir of Tensorboard\n",
    "wandb.tensorboard.patch(root_logdir=\"/content/logs\")\n",
    "\n",
    "# Create a wandb run to log all your metrics\n",
    "run = wandb.init(project=PROJECT_NAME, entity=ENTITY, reinit=True, sync_tensorboard=True)\n",
    "\n",
    "# Just 3 lines of code to utilize and run training on GPUs\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "gpus = tf.config.list_logical_devices('GPU')\n",
    "strategy = tf.distribute.MirroredStrategy(gpus)\n",
    "\n",
    "def train_model(model_dir = '/content/model_dir'):\n",
    "\n",
    "  # Training\n",
    "  history = model.fit(train_dataset,\n",
    "                      validation_data=validation_dataset,\n",
    "                      epochs=config.epochs,\n",
    "                      callbacks=[tf.keras.callbacks.TensorBoard()])\n",
    "  \n",
    "  # Save the Model and Upload it to the cloud as a Artifact\n",
    "  trained_model_artifact = wandb.Artifact(\"DenseNet121-GPU-Adadelta\", type = \"model\", description = \"Baseline DenseNet121 model trained on GPUs using Adadelta as the optimizer\")\n",
    "  model.save(model_dir)\n",
    "  trained_model_artifact.add_dir(model_dir)\n",
    "  run.log_artifact(trained_model_artifact)\n",
    "\n",
    "  # Finish the run\n",
    "  run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "  model = get_model()\n",
    "\n",
    "  model.compile(optimizer=tf.keras.optimizers.Adadelta(learning_rate=config.base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "  \n",
    "  train_model()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
