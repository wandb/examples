{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/gb6B4ig.png\" width=\"400\" alt=\"Weights & Biases\" />\n",
    "<!--- @wandbcode{audit-artifacts-colab} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf574853",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/wandb-artifacts/W&B_artifacts_for_auditing_purposes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Weights & Biases](https://wandb.ai/site) makes running collaborative machine learning projects a breeze. You can focus on what you're trying to experiment with, and W&B will take on the burden of keeping track of everything. If you want to review a loss plot, download the latest model for production, or just see which configurations produced a certain model, W&B is your friend. There's also a bunch of features to help you and your team collaborate, like having a shared dashboard and sharing interactive reports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Weights and Biases can help you with Audits and Regulatory Guidelines\n",
    "\n",
    "This notebook accompanies and implements a\n",
    "[blog post](http://wandb.me/audit-artifacts-report)\n",
    "on using W&B Artifacts to help teams in regulation-heavy industries share their Machine Learning models with clients.\n",
    "\n",
    "Run the cells below to train an image classifier and upload the model checkpoints as W&B Artifacts. Then you can reliably know which models you've given to your clients and happily share this information with any regulators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please make sure that you set CUDA device before running the following colab. This can be done by changing `Runtime Type` to use GPU hardware accelerator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install awscli --ignore-installed six\n",
    "!pip install timm wandb boto3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install packages and prepare dataset\n",
    "!wget https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-160.tgz -q\n",
    "!tar -xf imagenette2-160.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✍️ Login to W&B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import boto3\n",
    "import wandb\n",
    "import torch\n",
    "import operator\n",
    "import os\n",
    "import logging\n",
    "import warnings\n",
    "import tempfile\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from timm.utils.log import setup_default_logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_logger = logging.getLogger('TrainEval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Config = dict(\n",
    "    PROJECT='artifacts',\n",
    "    DATA_DIR=\"./imagenette2-160\",\n",
    "    TRAIN_DATA_DIR=\"./imagenette2-160/train\",\n",
    "    TEST_DATA_DIR=\"./imagenette2-160/val\",\n",
    "    DEVICE=\"cuda\",\n",
    "    MODEL=\"efficientnet_b3\",\n",
    "    PRETRAINED=False,\n",
    "    LR=3e-4,\n",
    "    EPOCHS=3,\n",
    "    IMG_SIZE=160,\n",
    "    FILENAME='checkpoint-1.pth.tar',\n",
    "    ALIAS='v0',\n",
    "    BS=96,\n",
    "    TRAIN_AUG=transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomCrop(160),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "        ]\n",
    "    ),\n",
    "    TEST_AUG=transforms.Compose(\n",
    "        [\n",
    "            transforms.CenterCrop(160),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "        ]\n",
    "    ),\n",
    "    NUM_CHECKPOINTS=2,\n",
    "    BUCKET='test-bucket-wandb'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()\n",
    "DEVICE = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏋️‍♀️ Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, train_data_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    fin_loss = 0.0\n",
    "    tk = tqdm(train_data_loader, desc=\"Epoch\" + \" [TRAIN] \" + str(epoch + 1))\n",
    "\n",
    "    for t, data in enumerate(tk):\n",
    "        data[0] = data[0].to(DEVICE)\n",
    "        data[1] = data[1].to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data[0])\n",
    "        loss = nn.CrossEntropyLoss()(out, data[1])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        fin_loss += loss.item()\n",
    "        tk.set_postfix(\n",
    "            {\n",
    "                \"loss\": \"%.6f\" % float(fin_loss / (t + 1)),\n",
    "                \"LR\": optimizer.param_groups[0][\"lr\"],\n",
    "            }\n",
    "        )\n",
    "    return fin_loss / len(train_data_loader), optimizer.param_groups[0][\"lr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fn(model, eval_data_loader, epoch):\n",
    "    model.eval()\n",
    "    fin_loss = 0.0\n",
    "    tk = tqdm(eval_data_loader, desc=\"Epoch\" + \" [VALID] \" + str(epoch + 1))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for t, data in enumerate(tk):\n",
    "            data[0] = data[0].to(DEVICE)\n",
    "            data[1] = data[1].to(DEVICE)\n",
    "            out = model(data[0])\n",
    "            loss = nn.CrossEntropyLoss()(out, data[1])\n",
    "            fin_loss += loss.item()\n",
    "            tk.set_postfix({\"loss\": \"%.6f\" % float(fin_loss / (t + 1))})\n",
    "        return fin_loss / len(eval_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 🏁 Checkpoint Saver\n",
    "\n",
    "Track top-n training checkpoints and maintain recovery checkpoints on specified intervals.\n",
    "Hacked together by / Copyright 2020 Ross Wightman\n",
    "\n",
    "This script has been adapted from `pytorch-image-models` checkpoint saver script\n",
    "written by Ross Wightman.\n",
    "This script adds Weights and Biases artifact integration on top.\n",
    "(https://github.com/rwightman/pytorch-image-models/blob/master/timm/utils/checkpoint_saver.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckpointSaver:\n",
    "    def __init__(\n",
    "            self,\n",
    "            model,\n",
    "            optimizer,\n",
    "            config=None,\n",
    "            checkpoint_prefix='checkpoint',\n",
    "            checkpoint_dir='',\n",
    "            decreasing=False,\n",
    "            max_history=2,\n",
    "            wandb_run=None):\n",
    "\n",
    "        # wandb run\n",
    "        self.wandb_run = wandb_run if wandb_run is not None else wandb.init(job_type='model-artifact')\n",
    "\n",
    "        # objects to save state_dicts of\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.config = config\n",
    "\n",
    "        # state\n",
    "        self.checkpoint_files = []  # (filename, metric) tuples in order of decreasing betterness\n",
    "        self.best_epoch = None\n",
    "        self.best_metric = None\n",
    "        self.curr_recovery_file = ''\n",
    "        self.last_recovery_file = ''\n",
    "\n",
    "        # config\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.save_prefix = checkpoint_prefix\n",
    "        self.extension = '.pth.tar'\n",
    "        self.decreasing = decreasing  # a lower metric is better if True\n",
    "        self.cmp = operator.lt if decreasing else operator.gt  # True if lhs better than rhs\n",
    "        self.max_history = max_history\n",
    "        assert self.max_history >= 1\n",
    "\n",
    "    def save_checkpoint(self, epoch, metric=None):\n",
    "        assert epoch >= 0\n",
    "        tmp_save_path = os.path.join(self.checkpoint_dir, 'tmp' + self.extension)\n",
    "        last_save_path = os.path.join(self.checkpoint_dir, 'last' + self.extension)\n",
    "        self._save(tmp_save_path, epoch, metric)\n",
    "        if os.path.exists(last_save_path):\n",
    "            os.unlink(last_save_path)  # required for Windows support.\n",
    "        os.rename(tmp_save_path, last_save_path)\n",
    "        worst_file = self.checkpoint_files[-1] if self.checkpoint_files else None\n",
    "        if (len(self.checkpoint_files) < self.max_history\n",
    "                or metric is None or self.cmp(metric, worst_file[1])):\n",
    "            if len(self.checkpoint_files) >= self.max_history:\n",
    "                self._cleanup_checkpoints(1)\n",
    "            filename = '-'.join([self.save_prefix, str(epoch)]) + self.extension\n",
    "            save_path = os.path.join(self.checkpoint_dir, filename)\n",
    "            os.link(last_save_path, save_path)\n",
    "            self.log_artifact(filename, save_path)\n",
    "            self.checkpoint_files.append((save_path, metric))\n",
    "            self.checkpoint_files = sorted(\n",
    "                self.checkpoint_files, key=lambda x: x[1],\n",
    "                reverse=not self.decreasing)  # sort in descending order if a lower metric is not better\n",
    "\n",
    "            checkpoints_str = \"Current checkpoints:\\n\"\n",
    "            for c in self.checkpoint_files:\n",
    "                checkpoints_str += ' {}\\n'.format(c)\n",
    "            _logger.info(checkpoints_str)\n",
    "\n",
    "            if metric is not None and (self.best_metric is None or self.cmp(metric, self.best_metric)):\n",
    "                self.best_epoch = epoch\n",
    "                self.best_metric = metric\n",
    "                best_save_path = os.path.join(self.checkpoint_dir, 'model_best' + self.extension)\n",
    "                if os.path.exists(best_save_path):\n",
    "                    os.unlink(best_save_path)\n",
    "                os.link(last_save_path, best_save_path)\n",
    "\n",
    "        return (None, None) if self.best_metric is None else (self.best_metric, self.best_epoch)\n",
    "\n",
    "    def _save(self, save_path, epoch, metric=None):\n",
    "        save_state = {\n",
    "            'epoch': epoch,\n",
    "            'arch': type(self.model).__name__.lower(),\n",
    "            'state_dict': self.model.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "        }\n",
    "        if metric is not None:\n",
    "            save_state['metric'] = metric\n",
    "        torch.save(save_state, save_path)\n",
    "\n",
    "    def _cleanup_checkpoints(self, trim=0):\n",
    "        trim = min(len(self.checkpoint_files), trim)\n",
    "        delete_index = self.max_history - trim\n",
    "        if delete_index < 0 or len(self.checkpoint_files) <= delete_index:\n",
    "            return\n",
    "        to_delete = self.checkpoint_files[delete_index:]\n",
    "        for d in to_delete:\n",
    "            try:\n",
    "                _logger.debug(\"Cleaning checkpoint: {}\".format(d))\n",
    "                # Optionally, only keep top N artifacts in W&B.\n",
    "                # self.delete_artifact(os.path.basename(d[0]))\n",
    "                os.remove(d[0])\n",
    "            except Exception as e:\n",
    "                _logger.error(\"Exception '{}' while deleting checkpoint\".format(e))\n",
    "        self.checkpoint_files = self.checkpoint_files[:delete_index]\n",
    "\n",
    "    def log_artifact(self, filename, save_path):\n",
    "        try: \n",
    "            artifact = wandb.Artifact(filename, type='model')\n",
    "            artifact.add_file(save_path)\n",
    "            self.wandb_run.log_artifact(artifact)\n",
    "        except Exception as e:\n",
    "            _logger.error(\"Exception '{}' while logging wandb artifact\".format(e))\n",
    "\n",
    "    def delete_artifact(self, filename, alias='v0'):\n",
    "        api = wandb.Api()\n",
    "        artifact = api.artifact(f'{Config[\"PROJECT\"]}/{filename}:{alias}')\n",
    "        try: \n",
    "            artifact.delete(delete_aliases=True)\n",
    "        except Exception as e:\n",
    "            _logger.error(\"Exception '{}' while deleting wandb artifact {}\".format(e, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💡Bring it all together! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(wandb_run=None):\n",
    "    # train and eval datasets\n",
    "    train_dataset = torchvision.datasets.ImageFolder(\n",
    "        Config[\"TRAIN_DATA_DIR\"], transform=Config[\"TRAIN_AUG\"]\n",
    "    )\n",
    "    eval_dataset = torchvision.datasets.ImageFolder(\n",
    "        Config[\"TEST_DATA_DIR\"], transform=Config[\"TEST_AUG\"]\n",
    "    )\n",
    "\n",
    "    # train and eval dataloaders\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=Config[\"BS\"],\n",
    "        shuffle=True,\n",
    "    )\n",
    "    eval_dataloader = torch.utils.data.DataLoader(\n",
    "        eval_dataset, batch_size=Config[\"BS\"],\n",
    "    )\n",
    "\n",
    "    # model\n",
    "    model = timm.create_model(Config[\"MODEL\"], pretrained=Config[\"PRETRAINED\"])\n",
    "    model = model.cuda()\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=Config[\"LR\"])\n",
    "\n",
    "    # setup checkpoint saver\n",
    "    saver = CheckpointSaver(model=model, optimizer=optimizer, config=Config, decreasing=True, \n",
    "                        wandb_run=wandb_run, max_history=Config['NUM_CHECKPOINTS'])\n",
    "\n",
    "    for epoch in range(Config[\"EPOCHS\"]):\n",
    "        avg_loss_train, lr = train_fn(\n",
    "            model, train_dataloader, optimizer, epoch\n",
    "        )\n",
    "        avg_loss_eval = eval_fn(model, eval_dataloader, epoch)\n",
    "        wandb.run.log({\n",
    "            \"epoch\": epoch, \n",
    "            \"learning rate\": lr, \n",
    "            \"train loss\": avg_loss_train, \n",
    "            \"evaluation loss\": avg_loss_eval\n",
    "            })\n",
    "        saver.save_checkpoint(epoch, metric=avg_loss_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_default_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model and Log Artifacts to W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=Config['PROJECT'], config=Config)\n",
    "wandb.config = Config\n",
    "main(wandb_run=run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Artifact to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup AWSCLI https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html\n",
    "!aws configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "api = wandb.Api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_artifact_to_s3(config):\n",
    "    artifact = api.artifact(f\"{config['PROJECT']}/{config['FILENAME']}:{config['ALIAS']}\")\n",
    "    digest = artifact.digest\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        path = artifact.download(tmpdir)\n",
    "        fname = os.listdir(path)[0]\n",
    "        fpath = path + '/' + fname\n",
    "\n",
    "        _logger.info(f\"Downloaded artifact {fname} to {fpath} locally.\")\n",
    "\n",
    "        try: \n",
    "            metadata = s3.head_object(Bucket=Config['BUCKET'], Key=fname)['Metadata']\n",
    "        except: \n",
    "            warnings.warn(f\"\"\"File {fname} does not already exist in Bucket {Config['BUCKET']} on AWS.\\\n",
    "            Cleaning up AWS bucket for any existing files, and uploading new \\\n",
    "            artifact.\"\"\")\n",
    "            bucket = boto3.resource('s3').Bucket(Config['BUCKET'])\n",
    "            bucket.objects.all().delete()\n",
    "            metadata = {'digest': -1}\n",
    "        \n",
    "        # upload files to S3 if digests are different \n",
    "        if metadata['digest']!=digest:\n",
    "            s3.upload_file(fpath, Config['BUCKET'], fname, ExtraArgs={\"Metadata\": {\"digest\": digest}})\n",
    "        else: \n",
    "            _logger.info(f\"File {fname} already exists in Bucket {Config['BUCKET']} on AWS with same digest. Nothing to do.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_artifact_to_s3(config=Config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
