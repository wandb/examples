{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
    "<!--- @wandbcode{splitting-tabular-data} -->\n",
    "\n",
    "# Tabular Data Versioning and Deduplication with Weights & Biases\n",
    "\n",
    "<img src=\"http://wandb.me/mini-diagram\" width=\"600\" alt=\"Weights & Biases\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-artifacts/train_val_test_split_with_tabular_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This walkthrough focuses on using W&B to version control and iterate on tabular data. We will use [Artifacts](https://docs.wandb.ai/guides/artifacts) and [Tables](https://docs.wandb.ai/guides/data-vis/tables-quickstart) to load in a dataset and split it into train, validation, and test subsets. Thanks to the versioning capability of Artifacts, we will use minimal storage space and have persistent version labels to easily share dataset iterations with colleagues.\n",
    "\n",
    "For this project, we will be working with tabular medical data that has great potential for predicting outcomes of heart attack patients. If you'd rather learn about similar features applied to classification tasks on image data, see this [other example](https://wandb.ai/stacey/mendeleev/reports/Tables-Tutorial-Visualize-Data-for-Image-Classification--VmlldzozNjE3NjA).\n",
    "\n",
    "You can also find an overview [report on this topic here](https://wandb.ai/dpaiton/splitting-tabular-data/reports/Dataset-Version-Control-and-Deduplication-with-Tabular-Data-with-W-B-Artifacts-and-Tables--VmlldzoxNDIzOTA1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About the data\n",
    "Our dataset is a collection of measurements and survey answers from hospital patients that have heart attack-related symptoms. It's not a stretch to say datasets like these -- and the models built from them -- can improve patient outcomes and save lives. On a purely machine learning level, this particular dataset is interesting because it has:\n",
    "* **Mixed types**: entries can be binary, ordinal, numeric, or categorical\n",
    "* **Missing data**: almost all features have some fraction of missing data\n",
    "* **Real-world complexity**: feature values are not uniformly distributed and have outliers\n",
    "* **Limited size**: collecting data is difficult and so the dataset is small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "Below we will keep all of our imports and helper functions. Reading through them is optional, and only recommended after you have looked over the rest of the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade wandb -qqq\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "import json\n",
    "import requests\n",
    "import csv\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "DEVICE = 'cpu'\n",
    "PROJECT_NAME = 'splitting-tabular-data'\n",
    "\n",
    "# Set the random seeds to improve reproducibility by removing stochasticity\n",
    "def set_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False # Force cuDNN to use a consistent convolution algorithm\n",
    "    torch.backends.cudnn.deterministic = True # Force cuDNN to use deterministic algorithms if available\n",
    "    torch.use_deterministic_algorithms(True) # Force torch to use deterministic algorithms if available\n",
    "\n",
    "set_seeds(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_split_artifact(run, raw_data_table, train_rows, val_rows, test_rows):\n",
    "    \"\"\"\n",
    "    Creates a w&b artifact that contains a singular reference table (aka a ForeignIndex table).\n",
    "    The ForeignIndex table has a single column that we are naming 'source'.\n",
    "    It contains references to the original table (raw_data_table) for each of the splits.\n",
    "    Arguments:\n",
    "        run (wandb run) returned from wandb.init()\n",
    "        raw_data_table (wandb Table) that contains your original tabular data\n",
    "        train_rows (list of ints) indices that reference the training rows in the raw_data_table\n",
    "        val_rows (list of ints) indices that reference the validation rows in the raw_data_table\n",
    "        test_rows (list of ints) indices that reference the test rows in the raw_data_table\n",
    "    \"\"\"\n",
    "    split_artifact = wandb.Artifact(\n",
    "        'data-splits', type='dataset',\n",
    "        description='Train, validation, test dataset splits')\n",
    "\n",
    "    # Our data split artifact will only store index references to the original dataset table to save space\n",
    "    data_table_pointer = raw_data_table.get_index() # ForeignIndex automatically references the source table\n",
    "    split_artifact.add(wandb.Table(\n",
    "        columns=['source'],\n",
    "        data=[[data_table_pointer[i]] for i in train_rows]), 'train-data')\n",
    "    split_artifact.add(wandb.Table(\n",
    "        columns=['source'],\n",
    "        data=[[data_table_pointer[i]] for i in val_rows]), 'val-data')\n",
    "    split_artifact.add(wandb.Table(\n",
    "        columns=['source'],\n",
    "        data=[[data_table_pointer[i]] for i in test_rows]), 'test-data')\n",
    "    run.log_artifact(split_artifact)\n",
    "\n",
    "\n",
    "def make_loaders(config):\n",
    "    \"\"\"\n",
    "    Makes data loaders using a artifact containing the dataset splits (created using the make_split_artifact() function)\n",
    "    The function assumes that you have created a data-splits artifact and a data-transforms artifact\n",
    "    Arguments:\n",
    "        config [dict] containing keys:\n",
    "            data_columns (list of ints) referencing which columns are to be treated as data\n",
    "            label_columns (list of ints) referencing which columns are to be treated as labels\n",
    "            num_classes (int) number of possible label classes in the dataset\n",
    "            batch_size (int) amount of rows (i.e. data instances) to be delivered in a single batch\n",
    "    Returns:\n",
    "        train_loader (PyTorch DataLoader) containing the training data\n",
    "        val_loader (PyTorch DataLoader) containing the validation data\n",
    "        test_loader (PyTorch DataLoader) containing the test data\n",
    "    \"\"\"\n",
    "    with wandb.init(project=PROJECT_NAME, job_type='package-data', config=config) as run:\n",
    "        # Load the transforms\n",
    "        transform_dir = run.use_artifact('data-transforms:latest').download()\n",
    "        transform_dict = json.load(open(os.path.join(transform_dir, 'transforms.txt')), object_pairs_hook=OrderedDict)\n",
    "        composed_transforms = get_transforms(transform_dict)\n",
    "        split_artifact = run.use_artifact('data-splits:latest')\n",
    "        # Reformat data to (inputs, labels)\n",
    "        train_loader = DataLoader(\n",
    "            MyocardialInfarctionDataset(\n",
    "                split_artifact.get('train-data'), config['data_columns'],\n",
    "                config['label_columns'], config['num_classes'], composed_transforms\n",
    "            ),\n",
    "            batch_size=config['batch_size'],\n",
    "            drop_last=True,\n",
    "            shuffle=True,\n",
    "            num_workers=0\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            MyocardialInfarctionDataset(\n",
    "                split_artifact.get('val-data'), config['data_columns'],\n",
    "                config['label_columns'], config['num_classes'], composed_transforms\n",
    "            ),\n",
    "            batch_size=config['batch_size'],\n",
    "            batch_sampler=None,\n",
    "            shuffle=False,\n",
    "            num_workers=0)\n",
    "        test_loader = DataLoader(\n",
    "            MyocardialInfarctionDataset(\n",
    "                split_artifact.get('test-data'), config['data_columns'],\n",
    "                config['label_columns'], config['num_classes'], composed_transforms\n",
    "            ),\n",
    "            batch_size=config['batch_size'],\n",
    "            batch_sampler=None,\n",
    "            shuffle=False,\n",
    "            num_workers=0)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "def get_table_row(table, ndx):\n",
    "    \"\"\"\n",
    "    Given a table and index, return the corresponding row\n",
    "    Arguments:\n",
    "        table (wandb.Table) can be a standard table of data or a pointer to a reference table\n",
    "        ndx (int) row index to slice\n",
    "    Returns:\n",
    "        ref_row (list) of data entries for the row referenced by ndx\n",
    "    \"\"\"\n",
    "    # Check if the table's contents are pointers to another table or not\n",
    "    linked_table = np.all([\n",
    "        type(value) is wandb.data_types._ForeignIndexType\n",
    "        for value in table._column_types.params['type_map'].values()\n",
    "    ])\n",
    "    if linked_table: # The table entries reference another table\n",
    "        ref_table = table.get_column(table.columns[0]) # There should only be one reference column\n",
    "        # The pointers are dereferenced using the get_row() function\n",
    "        if type(ndx) is list:\n",
    "            ref_row = [list(ref_table[i].get_row().values()) for i in ndx]\n",
    "        elif type(ndx) is int:\n",
    "            ref_row = list(ref_table[ndx].get_row().values())\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f'Input argument ndx must be of type int or list, not {type(ndx)}'\n",
    "            )\n",
    "        return ref_row\n",
    "    else: # Standard w&b Table containing the data\n",
    "        return table.data[ndx]\n",
    "\n",
    "\n",
    "class MyocardialInfarctionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Myocardial Infarction Dataset\n",
    "    In general columns 2-112 can be used as input data for prediction.\n",
    "    Possible complications (outputs) are listed in columns 113-124.\n",
    "   \n",
    "    There  are  four  possible  time  moments  for  complication  prediction:  on  base  of  the information known at\n",
    "    1. The time of admission to hospital: all input columns (2-112) except 93, 94, 95, 100, 101, 102, 103, 104, 105 can be used for prediction;\n",
    "    2. The end of the first day (24 hours after admission to the hospital): all input columns (2-112) except 94, 95, 101, 102, 104, 105 can be used for prediction;\n",
    "    3. The end of the second day (48 hours after admission to the hospital) all input columns (2-112) except 95, 102, 105 can be used for prediction;\n",
    "    4. The end of the third day (72 hours after admission to the hospital) all input columns (2-112) can be used for prediction.\n",
    "\n",
    "    All of the above column numbers are 1-indexed.\n",
    "    \"\"\"\n",
    "    def __init__(self, table, data_columns, label_columns, num_classes, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            table (wandb.Table): table containing the dataset\n",
    "            data_columns (list): list of column indices corresponding to the data (X)\n",
    "            label_columns (list): list of column indices corresponding to the labels (Y)\n",
    "            num_classes (int): number of possible output classes (for one-hot encoding)\n",
    "            transform (function): receives (data, label) tuple as input and produces transformed (data, label) tuple as output\n",
    "        \"\"\"\n",
    "        super(MyocardialInfarctionDataset, self).__init__()\n",
    "        self.table = table\n",
    "        self.data_columns = data_columns\n",
    "        self.label_columns = label_columns\n",
    "        self.num_classes = num_classes\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.table.data)\n",
    "   \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        label_row = np.array(get_table_row(self.table, idx), dtype=np.float32).take(self.label_columns) \n",
    "        data_row = np.array(get_table_row(self.table, idx), dtype=np.float32).take(self.data_columns) \n",
    "        if self.transform:\n",
    "            data_row, label_row = self.transform((data_row, label_row))\n",
    "        return (data_row, label_row)\n",
    "\n",
    "\n",
    "class NoneToVal(object):\n",
    "    \"\"\"Convert None or NaN entries to usable values\n",
    "    \"\"\"\n",
    "    def __init__(self, fill_value):\n",
    "        self.fill_value = fill_value\n",
    "    \n",
    "    def __call__(self, data_tuple):\n",
    "        data, label = data_tuple\n",
    "        data = np.ma.masked_invalid(data).filled(fill_value=self.fill_value)\n",
    "        return (data, label)\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert numpy arrays to tensor arrays\n",
    "    \"\"\"\n",
    "    def __init__(self, device=None):\n",
    "        if device is None:\n",
    "            device = \"cpu\"\n",
    "        self.device = device\n",
    "    \n",
    "    def __call__(self, data_tuple):\n",
    "        data, labels = data_tuple\n",
    "        return (torch.from_numpy(data).to(self.device), torch.from_numpy(labels).to(self.device))\n",
    "\n",
    "\n",
    "class OneHot(object):\n",
    "    \"\"\"Convert input tensor to one-hot array\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = int(num_classes)\n",
    "    \n",
    "    def __call__(self, data_tuple):\n",
    "        data, labels = data_tuple\n",
    "        device = labels.device\n",
    "        dtype = labels.dtype\n",
    "        num_datapoints = int(labels.ndim)\n",
    "        labels_one_hot = torch.zeros((num_datapoints, self.num_classes), dtype=dtype).to(device)\n",
    "        labels_one_hot[:, labels.long()] = 1\n",
    "        return (data, labels_one_hot.squeeze())\n",
    "\n",
    "\n",
    "def get_transforms(transform_dict):\n",
    "    \"\"\"\n",
    "    Given a dictionary of transform parameters, return a list of class instances for each transform\n",
    "    Arguments:\n",
    "        transform_dict (OrderedDict) with optional keys:\n",
    "            NoneToVal (dict) if present, requires the 'value' key that None/nan will be replaced with\n",
    "            ToTensor (dict) if present, requires the 'device' key that indicates the PyTorch device\n",
    "            OneHot (dict) if present, requires the 'num_classes' key that has an int value for the number of possible data labels\n",
    "    Returns:\n",
    "        composed_transforms (PyTorch composed transform class) containing the requested transform steps in order\n",
    "    \"\"\"\n",
    "    transform_functions = []\n",
    "    for key in transform_dict.keys():\n",
    "        if key=='NoneToVal': # Replace None/nan entries with a given value or array of values\n",
    "            transform_functions.append(NoneToVal(\n",
    "                transform_dict[key]['value']\n",
    "            ))\n",
    "        \n",
    "        elif key=='ToTensor': # Convert array to a PyTorch Tensor\n",
    "            transform_functions.append(ToTensor(\n",
    "                transform_dict[key]['device']\n",
    "            ))\n",
    "        \n",
    "        elif key=='OneHot': # Convert class labels to a one-hot representation\n",
    "            transform_functions.append(OneHot(\n",
    "                transform_dict[key]['num_classes'],\n",
    "            ))\n",
    "    composed_transforms = transforms.Compose(transform_functions)\n",
    "    return composed_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following list contains the data types for each feature in the Myocardial Infarction dataset. \n",
    "\n",
    "More detailed information can be found via this [pdf file](https://s3-eu-west-1.amazonaws.com/pstorage-leicester-213265548798/22803695/Descriptivestatistics.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = [\n",
    "    int,   # 001. ID; numeric, [0, --]\n",
    "    int,   # 002. AGE; numeric, [26, 92]\n",
    "    int,   # 003. SEX; binary\n",
    "    int,   # 004. INF_ANAM; ordinal, 4 levels\n",
    "    int,   # 005. STENOK_AN; ordinal, 7 levels\n",
    "    int,   # 006. FK_STENOK; ordinal, 5 levels\n",
    "    int,   # 007. IBS_POST; ordinal, 3 levels\n",
    "    int,   # 008. IBS_NASL; binary\n",
    "    int,   # 009. GB; ordinal, 4 levels\n",
    "    int,   # 010. SIM_GIPERT; binary\n",
    "    int,   # 011. DLT_AG; ordinal, 8 levels\n",
    "    int,   # 012. ZSN_A; partially ordered, 5 levels\n",
    "    int,   # 013. nr11; binary\n",
    "    int,   # 014. nr01; binary\n",
    "    int,   # 015. nr02; binary\n",
    "    int,   # 016. nr03; binary\n",
    "    int,   # 017. nr04; binary\n",
    "    int,   # 018. nr07; binary\n",
    "    int,   # 019. nr08; binary\n",
    "    int,   # 020. np01; binary\n",
    "    int,   # 021. np04; binary\n",
    "    int,   # 022. np05; binary\n",
    "    int,   # 023. np07; binary\n",
    "    int,   # 024. np08; binary\n",
    "    int,   # 025. np09; binary\n",
    "    int,   # 026. np10; binary\n",
    "    int,   # 027. endocr_01; binary\n",
    "    int,   # 028. endocr_02; binary\n",
    "    int,   # 029. endocr_03; binary\n",
    "    int,   # 030. zab_leg_01; binary\n",
    "    int,   # 031. zab_leg_02; binary\n",
    "    int,   # 032. zab_leg_03; binary\n",
    "    int,   # 033. zab_leg_04; binary\n",
    "    int,   # 034. zab_leg_06; binary\n",
    "    float, # 035. S_AD_KBRIG; numeric, [0, 260] mmHg\n",
    "    float, # 036. D_AD_KBRIG; numeric, [0, 190] mmHg\n",
    "    float, # 037. S_AD_ORIT; numeric, [0, 260] mmHg\n",
    "    float, # 038. D_AD_ORIT; numeric, [0, 190] mmHg\n",
    "    int,   # 039. O_L_POST; binary\n",
    "    int,   # 040. K_SH_POST; binary\n",
    "    int,   # 041. MP_TP_POST; binary\n",
    "    int,   # 042. SVT_POST; binary\n",
    "    int,   # 043. GT_POST;  binary\n",
    "    int,   # 044. FIB_G_POST; binary\n",
    "    int,   # 045. ant_im; ordinal, 5 levels\n",
    "    int,   # 046. lat_im; ordinal, 5 levels\n",
    "    int,   # 047. inf_im; ordinal, 5 levels\n",
    "    int,   # 048. post_im; ordinal, 5 levels\n",
    "    int,   # 049. IM_PG_P; binary\n",
    "    int,   # 050. ritm_ecg_p_01; binary\n",
    "    int,   # 051. ritm_ecg_p_02; binary\n",
    "    int,   # 052. ritm_ecg_p_04; binary\n",
    "    int,   # 053. ritm_ecg_p_06; binary\n",
    "    int,   # 054. ritm_ecg_p_07; binary\n",
    "    int,   # 055. ritm_ecg_p_08; binary\n",
    "    int,   # 056. n_r_ecg_p_01; binary\n",
    "    int,   # 057. n_r_ecg_p_02; binary\n",
    "    int,   # 058. n_r_ecg_p_03; binary\n",
    "    int,   # 059. n_r_ecg_p_04; binary\n",
    "    int,   # 060. n_r_ecg_p_05; binary\n",
    "    int,   # 061. n_r_ecg_p_06; binary\n",
    "    int,   # 062. n_r_ecg_p_08; binary\n",
    "    int,   # 063. n_r_ecg_p_09; binary\n",
    "    int,   # 064. n_r_ecg_p_10; binary\n",
    "    int,   # 065. n_p_ecg_p_01; binary\n",
    "    int,   # 066. n_p_ecg_p_03; binary\n",
    "    int,   # 067. n_p_ecg_p_04; binary\n",
    "    int,   # 068. n_p_ecg_p_05; binary\n",
    "    int,   # 069. n_p_ecg_p_06; binary\n",
    "    int,   # 070. n_p_ecg_p_07; binary\n",
    "    int,   # 071. n_p_ecg_p_08; binary\n",
    "    int,   # 072. n_p_ecg_p_09; binary\n",
    "    int,   # 073. n_p_ecg_p_10; binary\n",
    "    int,   # 074. n_p_ecg_p_11; binary\n",
    "    int,   # 075. n_p_ecg_p_12; binary\n",
    "    int,   # 076. fibr_ter_01; binary\n",
    "    int,   # 077. fibr_ter_02; binary\n",
    "    int,   # 078. fibr_ter_03; binary\n",
    "    int,   # 079. fibr_ter_05; binary\n",
    "    int,   # 080. fibr_ter_06; binary\n",
    "    int,   # 081. fibr_ter_07; binary\n",
    "    int,   # 082. fibr_ter_08; binary\n",
    "    int,   # 083. GIPO_K; binary\n",
    "    float, # 084. K_BLOOD; numeric, [2.3, 8.2] mmol/L\n",
    "    int,   # 085. GIPER_Na; binary\n",
    "    float, # 086. Na_BLOOD; numeric, [117, 169] mmol/L\n",
    "    float, # 087. ALT_BLOOD; numeric, [0.03, 0.48] IU/L\n",
    "    float, # 088. AST_BLOOD; numeric, [0.04, 2.15] IU/L\n",
    "    float, # 089. KFK_BLOOD; numeric, [1.2, 3.6] IU/L\n",
    "    float, # 090. L_BLOOD; numeric, [2, 27.9] billions per liter\n",
    "    float, # 091. ROE; numeric, [1, 140] mm\n",
    "    int,   # 092. TIME_B_S; ordinal, 10 levels\n",
    "    int,   # 093. R_AB_1_n; ordinal, 4 levels\n",
    "    int,   # 094. R_AB_2_n; ordinal, 4 levels\n",
    "    int,   # 095. R_AB_3_n; ordinal, 4 levels\n",
    "    int,   # 096. NA_KB; binary\n",
    "    int,   # 097. NOT_NA_KB; binary \n",
    "    int,   # 098. LID_KB; binary\n",
    "    int,   # 099. NITR_S; binary\n",
    "    int,   # 100. NA_R_1_n; ordinal, 5 levels\n",
    "    int,   # 101. NA_R_2_n; ordinal, 4 levels\n",
    "    int,   # 102. NA_R_3_n; ordinal, 3 levels\n",
    "    int,   # 103. NOT_NA_1_n; ordinal, 5 levels\n",
    "    int,   # 104. NOT_NA_2_n; ordinal, 4 levels\n",
    "    int,   # 105. NOT_NA_3_n; ordinal, 3 levels\n",
    "    int,   # 106. LID_S_n; binary\n",
    "    int,   # 107. B_BLOCK_S_n; binary\n",
    "    int,   # 108. ANT_CA_S_n; binary\n",
    "    int,   # 109. GEPAR_S_n; binary\n",
    "    int,   # 110. ASP_S_n; binary\n",
    "    int,   # 111. TIKL_S_n; binary\n",
    "    int,   # 112. TRENT_S_n; binary\n",
    "    int,   # 113. FIBR_PREDS; binary\n",
    "    int,   # 114. PREDS_TAH; binary\n",
    "    int,   # 115. JELUD_TAH; binary\n",
    "    int,   # 116. FIBR_JELUD; binary\n",
    "    int,   # 117. A_V_BLOCK; binary\n",
    "    int,   # 118. OTEK_LANC; binary\n",
    "    int,   # 119. RAZRIV; binary\n",
    "    int,   # 120. DRESSLER, binary\n",
    "    int,   # 121. ZSN; binary\n",
    "    int,   # 122. REC_IM; binary\n",
    "    int,   # 123. P_IM_STEN; binary\n",
    "    int,   # 124. LET_IS; categorical, 8 categories\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset into an artifact\n",
    "Our first step is to load in the dataset from a CSV file, which we accomplish with Artifacts and Tables. The wandb Artifact has two very useful features for our application: 1) it supports versioning, which will allow us to track changes we make to the original datset and 2) it supports deduplication, which will minimize the amount of storage space we use when generating modified versions of the dataset.\n",
    "\n",
    "In the code below we use the python `requests` and `csv` libararies to load in each line of the CSV file into a wandb Table. Then we store the table in a wandb Artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw dataset into a table and store it as an artifact\n",
    "with wandb.init(project=PROJECT_NAME, job_type='load-data') as run:\n",
    "    # Load data row-by-row & add the rows to the table (Note: the whole table will be stored in memory)\n",
    "    dataset_url = 'https://s3-eu-west-1.amazonaws.com/pstorage-leicester-213265548798/23581310/MyocardialinfarctioncomplicationsDatabase.csv'\n",
    "    with requests.get(dataset_url, stream=True) as r:\n",
    "        # Load each line into the table\n",
    "        lines = (line.decode('utf-8') for line in r.iter_lines())\n",
    "        column_headings = next(csv.reader(lines)) # This assumes that the first CSV line contains the column headings\n",
    "        data_table = wandb.Table(columns=column_headings) # Initialize the table\n",
    "        for index, row in enumerate(csv.reader(lines)): # Starting at the second row\n",
    "            row = [data_types[entry_index](entry) if entry != '' else np.nan for entry_index, entry in enumerate(row)]\n",
    "            if len(row) == len(column_headings):\n",
    "                data_table.add_data(*row)\n",
    "    # Create an artifact for our dataset\n",
    "    dataset_artifact = wandb.Artifact(\n",
    "        'data-library', type='dataset',\n",
    "        description='Table containing the CSV dataset',\n",
    "        metadata={'MD5_checksum': 'd409a89bd7e566da4b82232c3956f576',\n",
    "                'filename': 'MyocardialinfarctioncomplicationsDatabase.csv',\n",
    "                'filesize': '427.31 kB',\n",
    "                'dataset_host': 'University of Leicester',\n",
    "                'dataset_url': dataset_url,\n",
    "                'project_url': 'https://doi.org/10.25392/leicester.data.12045261.v3',\n",
    "                'reference_doi': '10.25392/leicester.data.12045261.v3',\n",
    "                }\n",
    "    )\n",
    "    # Add the table to the artifact & log the artifact\n",
    "    dataset_artifact.add(data_table, 'data-table')\n",
    "    run.log_artifact(dataset_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data into training, validation, and test sets\n",
    "As is typical with most machine learning applications, we want to grab a majority of our data for training and use a smaller subset for validation and testing. The validation set will be used to help us tune our parameters and modify the preprocessing steps.\n",
    "\n",
    "To avoid saving multiple copies of the dataset, we will only store corresponding indices for the train/val/test splits. This is also version controlled in case you decide you need more training data or you want to redo the shuffles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'train_val_test_split': [0.80, 0.10, 0.10], # These must sum to 1.0\n",
    "    'data_columns' : [i for i in range(1, 112)], # All possible training features\n",
    "    'label_columns' : [123], # Lethal outcome\n",
    "    'num_classes' : 8, # Num classes after preprocessing\n",
    "    'batch_size' : 20, # Num samples to average over for gradient updates\n",
    "}\n",
    "\n",
    "# Split data into train, val, test tables\n",
    "with wandb.init(project=PROJECT_NAME, job_type='split-data', config=config) as run:\n",
    "    # Define the data splits\n",
    "    raw_data_table = run.use_artifact('data-library:latest').get('data-table')\n",
    "    # One of many methods to extract random train/val/test rows\n",
    "    num_samples = len(raw_data_table.data)\n",
    "    shuffled_rows = np.random.choice(np.arange(num_samples), num_samples, replace=False)\n",
    "    train_rows, val_rows, test_rows = np.split(shuffled_rows,\n",
    "        np.cumsum([num_samples*split for split in config['train_val_test_split'][:-1]], dtype=int))\n",
    "    # Construct a new artifact for the data splits\n",
    "    make_split_artifact(run, raw_data_table, train_rows, val_rows, test_rows)\n",
    "\n",
    "# Quick test to make sure the slicing worked properly\n",
    "test_slicing = True\n",
    "if test_slicing:\n",
    "    print('num total: ', num_samples)\n",
    "    print('num train:', len(train_rows))\n",
    "    print('num val:', len(val_rows))\n",
    "    print('num test:', len(test_rows))\n",
    "    print('shuffle duplicates: ', len(set(shuffled_rows)) != len(shuffled_rows))\n",
    "    print('val in train:', np.any([row in train_rows for row in val_rows]))\n",
    "    print('test in train:', np.any([row in train_rows for row in test_rows]))\n",
    "    print('val in test:', np.any([row in val_rows for row in test_rows]))\n",
    "    print('train dupliates: ', len(set(train_rows)) != len(train_rows))\n",
    "    print('val dupliates: ', len(set(val_rows)) != len(val_rows))\n",
    "    print('test dupliates: ', len(set(test_rows)) != len(test_rows))\n",
    "    print('all samples accounted for in the shuffled set:', len(shuffled_rows) == num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packaging the data into PyTorch data loaders\n",
    "Next we will package the data into a PyTorch DataLoader to make it easier to work with. The DataLoader includes a list of preprocessing steps that are to be performed on the data. We want to be able to iterate and version control preprocessing pipeline, so we also have to write some code to store it as an artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init(project=PROJECT_NAME, job_type='define-transforms', config=config) as run:\n",
    "    # Define an initial set of transforms that we think will be useful\n",
    "    transform_dict = OrderedDict()\n",
    "    transform_dict['NoneToVal'] = {\n",
    "        'value': 0 # for the first pass we will replace missing values with 0\n",
    "    }\n",
    "    transform_dict['ToTensor'] = {\n",
    "        'device': DEVICE\n",
    "    }\n",
    "    transform_dict['OneHot'] = {\n",
    "            'num_classes': config['num_classes']\n",
    "    }\n",
    "    # Include an operational index to verify the order\n",
    "    for key_idx, key in enumerate(transform_dict.keys()):\n",
    "        transform_dict[key]['order'] = key_idx\n",
    "    # Create an artifact for logging the transforms\n",
    "    data_transform_artifact = wandb.Artifact(\n",
    "        'data-transforms', type='parameters',\n",
    "        description='Data preprocessing functions and parameters.',\n",
    "        metadata=transform_dict) # Optional for viewing on the web app; the data is also stored in the txt file below\n",
    "    # Log the transforms in JSON format\n",
    "    with data_transform_artifact.new_file('transforms.txt') as f:\n",
    "        f.write(json.dumps(transform_dict, indent=4))\n",
    "    run.log_artifact(data_transform_artifact)\n",
    "config.update(transform_dict) # Log the transforms in the config so that we can sweep over them in future iterations\n",
    "\n",
    "# Now we can make the data loaders with the preprocessing pipeline\n",
    "train_loader, val_loader, test_loader = make_loaders(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done! Time to train\n",
    "\n",
    "That concludes this part of the tutorial. In a future tutorial we will use this same data to train and iterate on our model. This will also use Artifacts to version control iterations on the preprocessing pipeline, parameters, and model architecture."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
