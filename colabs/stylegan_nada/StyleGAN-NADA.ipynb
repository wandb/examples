{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/stylegan_nada/StyleGAN-NADA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "<!--- @wandbcode{stylegan-nada-colab} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî•üî• StyelGAN-NADA + WandB Playground ü™Ñüêù\n",
    "\n",
    "<!--- @wandbcode{stylegan-nada-colab} -->\n",
    "\n",
    "**Original Implementation:** https://github.com/rinongal/StyleGAN-nada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Setup required libraries and models. \n",
    "This may take a few minutes.\n",
    "\n",
    "You may optionally enable downloads with pydrive in order to authenticate and avoid drive download limits when fetching pre-trained ReStyle and StyleGAN2 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorflow_version 1.x\n",
    "\n",
    "import os\n",
    "\n",
    "restyle_dir = os.path.join(\"/content\", \"restyle\")\n",
    "stylegan_ada_dir = os.path.join(\"/content\", \"stylegan_ada\")\n",
    "stylegan_nada_dir = os.path.join(\"/content\", \"stylegan_nada\")\n",
    "\n",
    "output_dir = os.path.join(\"/content\", \"output\")\n",
    "\n",
    "output_model_dir = os.path.join(output_dir, \"models\")\n",
    "output_image_dir = os.path.join(output_dir, \"images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/yuval-alaluf/restyle-encoder.git $restyle_dir\n",
    "\n",
    "!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
    "!sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
    "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force\n",
    "\n",
    "!pip install ftfy regex tqdm wandb\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "\n",
    "!git clone https://github.com/NVlabs/stylegan2-ada/ $stylegan_ada_dir\n",
    "!git clone https://github.com/rinongal/stylegan-nada.git $stylegan_nada_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "sys.path.append(restyle_dir)\n",
    "sys.path.append(stylegan_nada_dir)\n",
    "sys.path.append(os.path.join(stylegan_nada_dir, \"ZSSGAN\"))\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Choose a model type.\n",
    "Model will be downloaded and converted to a pytorch compatible version.\n",
    "\n",
    "Re-runs of the cell with the same model will re-use the previously downloaded version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Configs for Selecting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = \"stylegan-nada\" #@param {\"type\": \"string\"}\n",
    "source_model_type = 'ffhq' #@param['ffhq', 'cat', 'dog', 'church', 'horse', 'car']\n",
    "\n",
    "artifact_adressed = {\n",
    "    \"car\": \"geekyrakshit/stylegan-nada/car:v0\",\n",
    "    \"horse\": \"geekyrakshit/stylegan-nada/horse:v0\",\n",
    "    \"church\": \"geekyrakshit/stylegan-nada/church:v0\",\n",
    "    \"dog\": \"geekyrakshit/stylegan-nada/dog:v0\",\n",
    "    \"cat\": \"geekyrakshit/stylegan-nada/cat:v0\",\n",
    "    \"ffhq\": \"geekyrakshit/stylegan-nada/ffhq:v0\"\n",
    "}\n",
    "\n",
    "model_names = {\n",
    "    \"ffhq\": \"ffhq.pt\",\n",
    "    \"cat\": \"afhqcat.pkl\",\n",
    "    \"dog\": \"afhqdog.pkl\",\n",
    "    \"church\": \"stylegan2-church-config-f.pkl\",\n",
    "    \"car\": \"stylegan2-car-config-f.pkl\",\n",
    "    \"horse\": \"stylegan2-horse-config-f.pkl\"\n",
    "}\n",
    "\n",
    "dataset_sizes = {\n",
    "    \"ffhq\": 1024,\n",
    "    \"cat\": 512,\n",
    "    \"dog\": 512,\n",
    "    \"church\": 256,\n",
    "    \"horse\": 256,\n",
    "    \"car\": 512,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=project, job_type=\"train\")\n",
    "config = wandb.config\n",
    "config.source_model_type = source_model_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching Models from [WandB Artifacts](https://docs.wandb.ai/guides/artifacts/artifacts-core-concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact = wandb.use_artifact(artifact_adressed[source_model_type])\n",
    "pretrained_model_dir = artifact.download()\n",
    "pt_file_name = model_names[source_model_type].split(\".\")[0] + \".pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Train the model.\n",
    "Describe your source and target class. These describe the direction of change you're trying to apply (e.g. \"photo\" to \"sketch\", \"dog\" to \"the joker\" or \"dog\" to \"avocado dog\").\n",
    "\n",
    "Alternatively, upload a directory with a small (~3) set of target style images (there is no need to preprocess them in any way) and set `style_image_dir` to point at them. This will use the images as a target rather than the source/class texts.\n",
    "\n",
    "We reccomend leaving the 'improve shape' button unticked at first, as it will lead to an increase in running times and is often not needed.\n",
    "For more drastic changes, turn it on and increase the number of iterations.\n",
    "\n",
    "As a rule of thumb:\n",
    "- Style and minor domain changes ('photo' -> 'sketch') require ~200-400 iterations.\n",
    "- Identity changes ('person' -> 'taylor swift') require ~150-200 iterations.\n",
    "- Simple in-domain changes ('face' -> 'smiling face') may require as few as 50.\n",
    "- The `style_image_dir` option often requires ~400-600 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from tqdm import notebook\n",
    "\n",
    "from ZSSGAN.model.ZSSGAN import ZSSGAN\n",
    "from ZSSGAN.utils.file_utils import save_images, get_dir_img_list\n",
    "from ZSSGAN.utils.training_utils import mixing_noise\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "source_class = \"Human\" #@param {\"type\": \"string\"}\n",
    "config.source_class = source_class\n",
    "\n",
    "target_class = \"The Joker\" #@param {\"type\": \"string\"}\n",
    "config.target_class = target_class\n",
    "\n",
    "style_image_dir = \"\" #@param {'type': 'string'}\n",
    "config.style_image_dir = style_image_dir\n",
    "\n",
    "seed = 3 #@param {\"type\": \"integer\"}\n",
    "config.seed = seed\n",
    "\n",
    "target_img_list = get_dir_img_list(style_image_dir) if style_image_dir else None\n",
    "\n",
    "improve_shape = False #@param{type:\"boolean\"}\n",
    "config.improve_shape = improve_shape\n",
    "\n",
    "model_choice = [\"ViT-B/32\", \"ViT-B/16\"]\n",
    "model_weights = [1.0, 0.0]\n",
    "\n",
    "if improve_shape or style_image_dir:\n",
    "    model_weights[1] = 1.0\n",
    "    \n",
    "mixing = 0.9 if improve_shape else 0.0\n",
    "\n",
    "auto_layers_k = int(2 * (2 * np.log2(dataset_sizes[source_model_type]) - 2) / 3) if improve_shape else 0\n",
    "auto_layer_iters = 1 if improve_shape else 0\n",
    "\n",
    "training_iterations = 251 #@param {type: \"integer\"}\n",
    "config.training_iterations = training_iterations\n",
    "\n",
    "output_interval     = 10 #@param {type: \"integer\"}\n",
    "config.output_interval = output_interval\n",
    "\n",
    "save_interval       = 10 #@param {type: \"integer\"}\n",
    "config.save_interval = save_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = {\n",
    "    \"size\": dataset_sizes[source_model_type],\n",
    "    \"batch\": 2,\n",
    "    \"n_sample\": 4,\n",
    "    \"output_dir\": output_dir,\n",
    "    \"lr\": 0.002,\n",
    "    \"frozen_gen_ckpt\": os.path.join(pretrained_model_dir, pt_file_name),\n",
    "    \"train_gen_ckpt\": os.path.join(pretrained_model_dir, pt_file_name),\n",
    "    \"iter\": training_iterations,\n",
    "    \"source_class\": source_class,\n",
    "    \"target_class\": target_class,\n",
    "    \"lambda_direction\": 1.0,\n",
    "    \"lambda_patch\": 0.0,\n",
    "    \"lambda_global\": 0.0,\n",
    "    \"lambda_texture\": 0.0,\n",
    "    \"lambda_manifold\": 0.0,\n",
    "    \"auto_layer_k\": auto_layers_k,\n",
    "    \"auto_layer_iters\": auto_layer_iters,\n",
    "    \"auto_layer_batch\": 8,\n",
    "    \"output_interval\": 50,\n",
    "    \"clip_models\": model_choice,\n",
    "    \"clip_model_weights\": model_weights,\n",
    "    \"mixing\": mixing,\n",
    "    \"phase\": None,\n",
    "    \"sample_truncation\": 0.7,\n",
    "    \"save_interval\": save_interval,\n",
    "    \"target_img_list\": target_img_list,\n",
    "    \"img2img_batch\": 16,\n",
    "    \"channel_multiplier\": 2,\n",
    "    \"sg3\": False,\n",
    "    \"sgxl\": False,\n",
    "}\n",
    "config.training_args = training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(**training_args)\n",
    "\n",
    "resume_training_from_artifact = False #@param{type:\"boolean\"}\n",
    "config.resume_training_from_artifact = resume_training_from_artifact\n",
    "\n",
    "checkpoint_artifact_address = \"geekyrakshit/stylegan-nada/model-winter-frost-8:v14\" #@param {'type': 'string'}\n",
    "config.checkpoint_artifact_address = checkpoint_artifact_address\n",
    "\n",
    "print(\"Loading base models...\")\n",
    "net = ZSSGAN(args)\n",
    "print(\"Done\")\n",
    "\n",
    "g_reg_ratio = 4 / 5\n",
    "\n",
    "g_optim = torch.optim.Adam(\n",
    "    net.generator_trainable.parameters(),\n",
    "    lr=args.lr * g_reg_ratio,\n",
    "    betas=(0 ** g_reg_ratio, 0.99 ** g_reg_ratio),\n",
    ")\n",
    "\n",
    "if resume_training_from_artifact and checkpoint_artifact_address is not None:\n",
    "    artifact = wandb.use_artifact(checkpoint_artifact_address)\n",
    "    artifact_dir = artifact.download()\n",
    "    checkpoint = torch.load(glob(os.path.join(artifact_dir, \"*.pt\"))[0])\n",
    "    net.generator_trainable.generator.load_state_dict(checkpoint['g_ema'])\n",
    "    g_optim.load_state_dict(checkpoint['g_optim'])\n",
    "\n",
    "# Set up output directories.\n",
    "sample_dir = os.path.join(args.output_dir, \"sample\")\n",
    "config.sample_dir = sample_dir\n",
    "\n",
    "ckpt_dir   = os.path.join(args.output_dir, \"checkpoint\")\n",
    "config.ckpt_dir = ckpt_dir\n",
    "\n",
    "os.makedirs(sample_dir, exist_ok=True)\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_z = torch.randn(args.n_sample, 512, device=device)\n",
    "\n",
    "for i in notebook.tqdm(range(args.iter)):\n",
    "    net.train()\n",
    "        \n",
    "    sample_z = mixing_noise(args.batch, 512, args.mixing, device)\n",
    "\n",
    "    [sampled_src, sampled_dst], clip_loss = net(sample_z)\n",
    "    wandb.log({\"CLIP-Loss\": clip_loss.item()}, step=i)\n",
    "\n",
    "\n",
    "    net.zero_grad()\n",
    "    clip_loss.backward()\n",
    "\n",
    "    g_optim.step()\n",
    "\n",
    "    if i % output_interval == 0:\n",
    "        net.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            [sampled_src, sampled_dst], loss = net([fixed_z], truncation=args.sample_truncation)\n",
    "\n",
    "            if source_model_type == 'car':\n",
    "                sampled_dst = sampled_dst[:, :, 64:448, :]\n",
    "\n",
    "            sampled_dst = torch.permute(sampled_dst, (0, 2, 3, 1)).cpu()\n",
    "            sampled_dst = [wandb.Image(dst.numpy()) for dst in sampled_dst]\n",
    "            wandb.log({\"Samples\": sampled_dst}, step=i)\n",
    "    \n",
    "    if (args.save_interval > 0) and (i > 0) and (i % args.save_interval == 0):\n",
    "        model_file = f\"{ckpt_dir}/{str(i).zfill(6)}.pt\"\n",
    "        torch.save(\n",
    "            {\n",
    "                \"g_ema\": net.generator_trainable.generator.state_dict(),\n",
    "                \"g_optim\": g_optim.state_dict(),\n",
    "            },\n",
    "            model_file,\n",
    "        )\n",
    "        artifact = wandb.Artifact(f\"model-{wandb.run.name}\", type=\"model\")\n",
    "        artifact.add_file(model_file)\n",
    "        wandb.log_artifact(artifact, aliases=[\"latest\", f\"step_{i}\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Generate samples with the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncation = 0.7 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
    "config.truncation = truncation\n",
    "\n",
    "samples = 9\n",
    "config.samples = samples\n",
    "\n",
    "artifact = wandb.use_artifact(f\"model-{wandb.run.name}:latest\")\n",
    "artifact_dir = artifact.download()\n",
    "checkpoint = torch.load(glob(os.path.join(artifact_dir, \"*.pt\"))[0])\n",
    "\n",
    "print(\"Loading models from checkpoint artifact...\")\n",
    "net = ZSSGAN(args)\n",
    "net.generator_trainable.generator.load_state_dict(checkpoint['g_ema'])\n",
    "print(\"Done\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    sample_z = torch.randn(samples, 512, device=device)\n",
    "\n",
    "    [sampled_src, sampled_dst], loss = net([sample_z], truncation=truncation)\n",
    "\n",
    "    if source_model_type == 'car':\n",
    "        sampled_dst = sampled_dst[:, :, 64:448, :]\n",
    "\n",
    "    sampled_dst = torch.permute(sampled_dst, (0, 2, 3, 1)).cpu()\n",
    "    sampled_dst = [wandb.Image(dst.numpy()) for dst in sampled_dst]\n",
    "    wandb.log({\"Generated Samples\": sampled_dst})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Editing a real image with Re-Style inversion (currently only FFHQ inversion is supported):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Fetch ReStyle Models from [WandB Artifacts](https://docs.wandb.ai/guides/artifacts/artifacts-core-concepts)\n",
    "\n",
    "This may take a few minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from restyle.utils.common import tensor2im\n",
    "from restyle.models.psp import pSp\n",
    "from restyle.models.e4e import e4e\n",
    "\n",
    "\n",
    "artifact = wandb.use_artifact(\"geekyrakshit/stylegan-nada/restyle:v0\")\n",
    "pretrained_model_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Choose a re-style model\n",
    "\n",
    "We reccomend choosing the e4e model as it performs better under domain translations. Choose pSp for better reconstructions on minor domain changes (typically those that require less than 150 training steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_type = 'e4e' #@param['psp', 'e4e']\n",
    "\n",
    "restyle_experiment_args = {\n",
    "    \"model_path\": os.path.join(pretrained_model_dir, f\"restyle_{encoder_type}_ffhq_encode.pt\"),\n",
    "    \"transform\": transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "}\n",
    "\n",
    "model_path = restyle_experiment_args['model_path']\n",
    "ckpt = torch.load(model_path, map_location='cpu')\n",
    "\n",
    "opts = ckpt['opts']\n",
    "\n",
    "opts['checkpoint_path'] = model_path\n",
    "opts = Namespace(**opts)\n",
    "\n",
    "restyle_net = (pSp if encoder_type == 'psp' else e4e)(opts)\n",
    "\n",
    "restyle_net.eval()\n",
    "restyle_net.cuda()\n",
    "print('Model successfully loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Align and invert an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_alignment(image_path):\n",
    "    import dlib\n",
    "    from scripts.align_faces_parallel import align_face\n",
    "    if not os.path.exists(\"shape_predictor_68_face_landmarks.dat\"):\n",
    "        print('Downloading files for aligning face image...')\n",
    "        os.system('wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2')\n",
    "        os.system('bzip2 -dk shape_predictor_68_face_landmarks.dat.bz2')\n",
    "        print('Done.')\n",
    "    predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "    aligned_image = align_face(filepath=image_path, predictor=predictor) \n",
    "    print(\"Aligned image has shape: {}\".format(aligned_image.size))\n",
    "    return aligned_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = \"https://engineering.nyu.edu/sites/default/files/styles/square_large_default_2x/public/2018-06/yann-lecun.jpg\" #@param {\"type\": \"string\"}\n",
    "file_name = \"yann-lecun.jpg\" #@param {\"type\": \"string\"}\n",
    "\n",
    "if not os.path.isfile(file_name):\n",
    "    !wget {image_url}\n",
    "\n",
    "image_path = os.path.join(\"/content\", file_name)\n",
    "original_image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "input_image = run_alignment(image_path)\n",
    "\n",
    "img_transforms = restyle_experiment_args['transform']\n",
    "transformed_image = img_transforms(input_image)\n",
    "\n",
    "def get_avg_image(net):\n",
    "    avg_image = net(\n",
    "        net.latent_avg.unsqueeze(0),\n",
    "        input_code=True,\n",
    "        randomize_noise=False,\n",
    "        return_latents=False,\n",
    "        average_code=True\n",
    "    )[0]\n",
    "    avg_image = avg_image.to('cuda').float().detach()\n",
    "    return avg_image\n",
    "\n",
    "opts.n_iters_per_batch = 5\n",
    "opts.resize_outputs = False  # generate outputs at full resolution\n",
    "\n",
    "from restyle.utils.inference_utils import run_on_batch\n",
    "\n",
    "with torch.no_grad():\n",
    "    avg_image = get_avg_image(restyle_net)\n",
    "    result_batch, result_latents = run_on_batch(\n",
    "        transformed_image.unsqueeze(0).cuda(), restyle_net, opts, avg_image\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_latent = torch.Tensor(result_latents[0][4]).cuda().unsqueeze(0).unsqueeze(1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    \n",
    "    [sampled_src, sampled_dst] = net(inverted_latent, input_is_latent=True)[0]\n",
    "    \n",
    "    sampled_src = torch.permute(sampled_src, (0, 2, 3, 1)).cpu().numpy()[0]\n",
    "    sampled_dst = torch.permute(sampled_dst, (0, 2, 3, 1)).cpu().numpy()[0]\n",
    "\n",
    "    table = wandb.Table(\n",
    "        columns=[\"Source-Class-Text\", \"Source-Image\", \"Target-Class-Text\", \"Translated-Image\"],\n",
    "        data=[[source_class, wandb.Image(sampled_src), target_class, wandb.Image(sampled_dst)]]\n",
    "    )\n",
    "\n",
    "    wandb.log({\"Restyle\": table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
