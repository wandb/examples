{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/mmdetection/Train_an_Object_Detection+Semantic_Segmentation_Model_with_MMDetection_and_W&B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "<!--- @wandbcode{mmdetection-wandb-colab} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\"/> <br>\n",
    "\n",
    "<!--- @wandbcode{mmdetection-wandb-colab, v=1} -->\n",
    "\n",
    "<img src=\"http://wandb.me/mini-diagram\" width=\"600\" alt=\"Weights & Biases\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üí° Train an Object Detection+Semantic Segmentation Model with MMDetection and Weights and Biases\n",
    "\n",
    "Weights and Biases\n",
    "\n",
    "In this colab, we will train a Mask RCNN model using [MMDetection](https://mmdetection.readthedocs.io/en/latest/1_exist_data_model.html) on a small [Balloon](https://github.com/matterport/Mask_RCNN/tree/master/samples/balloon) dataset.Through this colab you will learn to:\n",
    "\n",
    "* use MMDetection to train an object detection + Semantic Segmentation model on a custom dataset,\n",
    "* use [Weights and Biases](https://wandb.ai/site) to log training and validation metrics, visualize model predictions, visualize raw validation dataset, and more.\n",
    "\n",
    "This colab in particular, will showcase a dedicated [`MMDetWandbHook`](https://github.com/open-mmlab/mmdetection/pull/7459) for MMDetection that can be used to:\n",
    "\n",
    "‚úÖ Log training and evaluation metrics. <br>\n",
    "‚úÖ Log versioned model checkpoints. <br>\n",
    "‚úÖ Log versioned validation dataset with ground truth bounding boxes and segmentation masks. <br>\n",
    "‚úÖ Log and visualize model predictions.\n",
    "\n",
    "But before we continue, here's a quick summary of MMDetection and W&B if you are not familiar with them.\n",
    "\n",
    "### üì∏ MMDetection\n",
    "\n",
    "MMDetection is an open source object detection toolbox based on PyTorch. It provides composable components that are easy to customize and has out-of-box support for single and multi GPU training/inference. It also has hundreds of pretrained detection models in Model Zoo, and supports multiple standard datasets. Check out the GitHub repository [here](https://github.com/open-mmlab/mmdetection).\n",
    "\n",
    "### üì∏ Weights and Biases\n",
    "\n",
    "Consider **[Weights and Biases](https://wandb.ai/site)** (W&B) to be the GitHub for machine learning. Use W&B for machine learning experiment tracking, dataset and model versioning, project collaboration, hyperparameter optimization, dataset exploration, model evaluation and so much more. If you are new to W&B, check out this [intro colab](https://wandb.me/intro)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚öΩÔ∏è Imports and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1Ô∏è‚É£ Install MMDetection\n",
    "\n",
    "MMDetection is heavily dependent on the [MMCV](https://mmcv.readthedocs.io/en/latest/#installation) library. We will have to install the version of MMCV that is compatible with the given PyTorch version. Check out the [Installation documentation](https://mmdetection.readthedocs.io/en/latest/get_started.html#installation) for more details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies: (use cu111 because colab has CUDA 11.1)\n",
    "!pip install -qq torch==1.9.0+cu111 torchvision==0.10.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "# install mmcv-full thus we could use CUDA operators\n",
    "!pip install -qq mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu111/torch1.9.0/index.html\n",
    "\n",
    "# Install mmdetection\n",
    "!pip install git+git://github.com/open-mmlab/mmdetection.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2Ô∏è‚É£ Install Weights and Biases\n",
    "\n",
    "Install the latest version of W&B. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3Ô∏è‚É£ General Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "# MMDetection\n",
    "import mmdet\n",
    "print(mmdet.__version__)\n",
    "from mmdet.datasets import build_dataset\n",
    "from mmdet.models import build_detector\n",
    "from mmdet.apis import train_detector\n",
    "from mmdet.datasets.builder import DATASETS\n",
    "from mmdet.datasets.custom import CustomDataset\n",
    "from mmdet.apis import set_random_seed\n",
    "\n",
    "# MMCV\n",
    "import mmcv\n",
    "from mmcv import Config\n",
    "\n",
    "# Weights and Biases\n",
    "import wandb\n",
    "print(wandb.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4Ô∏è‚É£ Login with you W&B account\n",
    "\n",
    "Create a free W&B account (it's free for personal and academic usage). Visit [wandb.ai/authorize](https://wandb.ai/authorize) to get your unique authentication token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèÄ Dataset\n",
    "\n",
    "We will be using a small [Balloon](https://github.com/matterport/Mask_RCNN/tree/master/samples/balloon) dataset for this colab notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1Ô∏è‚É£ Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/matterport/Mask_RCNN/releases/download/v2.1/balloon_dataset.zip\n",
    "!unzip -q balloon_dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls balloon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: The `train` and `val` folders contain the images along with annotations as `.json` files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2Ô∏è‚É£ Convert annotations to COCO format\n",
    "\n",
    "To support a new data format, it's recommended to convert the annotations to COCO format or PASCAL VOC format.\n",
    "\n",
    "If you are converting annotations to COCO format, do so offline and use the `CocoDataset` class. If you are converting it to the PASCAL format, use the `VOCDataset` class. You will see the usage below.\n",
    "\n",
    "You can find more details about customizing the dataset [here](https://mmdetection.readthedocs.io/en/latest/tutorials/customize_dataset.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Utility function `convert_balloon_to_coco`\n",
    "# Credit: https://github.com/open-mmlab/mmdetection/blob/master/demo/MMDet_InstanceSeg_Tutorial.ipynb\n",
    "def convert_balloon_to_coco(ann_file, out_file, image_prefix):\n",
    "    data_infos = mmcv.load(ann_file)\n",
    "\n",
    "    annotations = []\n",
    "    images = []\n",
    "    obj_count = 0\n",
    "    for idx, v in enumerate(mmcv.track_iter_progress(data_infos.values())):\n",
    "        filename = v['filename']\n",
    "        img_path = osp.join(image_prefix, filename)\n",
    "        height, width = mmcv.imread(img_path).shape[:2]\n",
    "\n",
    "        images.append(dict(\n",
    "            id=idx,\n",
    "            file_name=filename,\n",
    "            height=height,\n",
    "            width=width))\n",
    "\n",
    "        bboxes = []\n",
    "        labels = []\n",
    "        masks = []\n",
    "        for _, obj in v['regions'].items():\n",
    "            assert not obj['region_attributes']\n",
    "            obj = obj['shape_attributes']\n",
    "            px = obj['all_points_x']\n",
    "            py = obj['all_points_y']\n",
    "            poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n",
    "            poly = [p for x in poly for p in x]\n",
    "\n",
    "            x_min, y_min, x_max, y_max = (\n",
    "                min(px), min(py), max(px), max(py))\n",
    "\n",
    "\n",
    "            data_anno = dict(\n",
    "                image_id=idx,\n",
    "                id=obj_count,\n",
    "                category_id=0,\n",
    "                bbox=[x_min, y_min, x_max - x_min, y_max - y_min],\n",
    "                area=(x_max - x_min) * (y_max - y_min),\n",
    "                segmentation=[poly],\n",
    "                iscrowd=0)\n",
    "            annotations.append(data_anno)\n",
    "            obj_count += 1\n",
    "\n",
    "    coco_format_json = dict(\n",
    "        images=images,\n",
    "        annotations=annotations,\n",
    "        categories=[{'id':0, 'name': 'balloon'}])\n",
    "    mmcv.dump(coco_format_json, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_balloon_to_coco(\n",
    "    'balloon/train/via_region_data.json',\n",
    "    'balloon/train/annotation_coco.json',\n",
    "    'balloon/train/')\n",
    "convert_balloon_to_coco(\n",
    "    'balloon/val/via_region_data.json',\n",
    "    'balloon/val/annotation_coco.json',\n",
    "    'balloon/val/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèà Model\n",
    "\n",
    "There are over hundred pre-trained object detectors provided by MMDetection via Model Zoo. Check out the Model Zoo [documentation](https://mmdetection.readthedocs.io/en/v2.21.0/model_zoo.html) page.\n",
    "\n",
    "You can also customize the model's backbone, neck, head, ROI, and loss. More on customizing the model [here](https://mmdetection.readthedocs.io/en/latest/tutorials/customize_models.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1Ô∏è‚É£ Download the model\n",
    "\n",
    "We will be using a pretrained model checkpoint to fine tune on our custom dataset. Let's download the model in the `checkpoints` directory.\n",
    "\n",
    "I am using the [this](https://download.openmmlab.com/mmdetection/v2.0/mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth) model with [this](https://github.com/open-mmlab/mmdetection/tree/master/configs/mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco.py) config from the Model Zoo. You can find similar Mask RCNN model [here](https://github.com/open-mmlab/mmdetection/tree/master/configs/mask_rcnn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir checkpoints\n",
    "!wget -c https://download.openmmlab.com/mmdetection/v2.0/mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth \\\n",
    "      -O checkpoints/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚öæÔ∏è Configuration\n",
    "\n",
    "MMDetection relies heavily on a config system. In the cell below, we will be loading a config file and modify few of the methods as per the need of this notebook.\n",
    "\n",
    "Note that both train and test dataloaders will use the same training samples. This is not a recommended practice but for the sake of a simplified notebook, let's use it. \n",
    "\n",
    "Learn more about the MMDetection Config system [here](https://mmdetection.readthedocs.io/en/latest/tutorials/config.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1Ô∏è‚É£ Load the config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = 'mmdetection/configs/mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_1x_coco.py'\n",
    "cfg = Config.fromfile(config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2Ô∏è‚É£ Modify data config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define type and path to the images.\n",
    "cfg.dataset_type = 'COCODataset'\n",
    "\n",
    "cfg.data.test.ann_file = 'balloon/val/annotation_coco.json'\n",
    "cfg.data.test.img_prefix = 'balloon/val/'\n",
    "cfg.data.test.classes = ('balloon',)\n",
    "\n",
    "cfg.data.train.ann_file = 'balloon/train/annotation_coco.json'\n",
    "cfg.data.train.img_prefix = 'balloon/train/'\n",
    "cfg.data.train.classes = ('balloon',)\n",
    "\n",
    "cfg.data.val.ann_file = 'balloon/val/annotation_coco.json'\n",
    "cfg.data.val.img_prefix = 'balloon/val/'\n",
    "cfg.data.val.classes = ('balloon',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3Ô∏è‚É£ Modify model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify num classes of the model in box head and mask head\n",
    "cfg.model.roi_head.bbox_head.num_classes = 1\n",
    "cfg.model.roi_head.mask_head.num_classes = 1\n",
    "\n",
    "# Use the pretrained model.\n",
    "cfg.load_from = 'checkpoints/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4Ô∏è‚É£ Modify training config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The original learning rate (LR) is set for 8-GPU training.\n",
    "# We divide it by 8 since we only use one GPU.\n",
    "cfg.optimizer.lr = 0.02 / 8\n",
    "cfg.lr_config.warmup = None\n",
    "cfg.log_config.interval = 10\n",
    "\n",
    "# Epochs\n",
    "cfg.runner.max_epochs = 12\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 0\n",
    "set_random_seed(0, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "# ‚≠êÔ∏è Set the checkpoint interval.\n",
    "cfg.checkpoint_config.interval = 4\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './tutorial_exps'\n",
    "cfg.device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5Ô∏è‚É£ Modify evaluation config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚≠êÔ∏è Set the evaluation interval.\n",
    "cfg.evaluation.interval = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that we are evaluating the model on the validation dataset after every 2 epochs while saving the checkpoints after every 4 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚≠êÔ∏è If you want to log the configuration to W&B for efficient experiment tracking, add the config filename to a dict with key `exp_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = dict()\n",
    "meta['exp_name'] = osp.basename(config_file)\n",
    "print(meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéæ Define Weights and Biases Hook\n",
    "\n",
    "MMDetection comes with a dedicated Weights and Biases Hook - `MMDetWandHook`.\n",
    "\n",
    "With this dedicated hook, you can:\n",
    "\n",
    "* log train and eval metrics along with system (CPU/GPU) metrics, \n",
    "* visualize the validation dataset as interactive [W&B Tables](https://docs.wandb.ai/guides/data-vis),\n",
    "* visualize the model prediction as interactive W&B Tables, and\n",
    "* save the model checkpoints as [W&B Artifacts](https://docs.wandb.ai/guides/artifacts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this hook, you can append a dict to `log_config.hooks`. The `log_config` wraps multiple logger hooks like  the `TextLoggerHook` used below.\n",
    "\n",
    "There are four important arguments in the `MMDetWandbHook` that can help you get the most out of MMDetection. \n",
    "\n",
    "- `init_kwargs`: Use this argument to in-turn pass arguments to `wandb.init`. You can use it to set the W&B project name, set the team name (entity) if you want to log the runs to a team account, pass the configuration, and more. Check out the arguments that you can pass to `wandb.init` [here](https://docs.wandb.ai/ref/python/init).\n",
    "\n",
    "- `log_checkpoint`: Save the checkpoint at every checkpoint interval\n",
    "as W&B Artifacts. Use this for model versioning where each version is a checkpoint. The checkpoint interval is set using `checkpoint_config.interval` (starred above). Note that this feature is dependent on MMCV's [`CheckpointHook`](https://mmcv.readthedocs.io/en/latest/api.html#mmcv.runner.CheckpointHook).\n",
    "\n",
    "- `log_checkpoint_metadata`: Log the evaluation metrics computed on the validation data with the checkpoint, along with current epoch as a metadata to that checkpoint.\n",
    "\n",
    "- `num_eval_images`: At every evaluation interval, the `MMDetWandbHook` logs the model prediction as interactive W&B Tables. The eval interval is determined by `evaluation.interval` (starred above). The number of samples logged is given by `num_eval_images`. Before training begins, the callback logs the validation data (images along with bounding box and masks). At every evaluation interval, the model predictions are logged. This Feature is dependent on MMCV's [`EvalHook`](https://mmcv.readthedocs.io/en/latest/api.html#mmcv.runner.EvalHook) or [`DistEvalHook`](https://mmcv.readthedocs.io/en/latest/api.html#mmcv.runner.DistEvalHook).\n",
    "\n",
    "- `bbox_score_thr`: Threshold for bounding box scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.log_config.hooks = [\n",
    "    dict(type='TextLoggerHook'),\n",
    "    dict(type='MMDetWandbHook',\n",
    "         init_kwargs={'project': 'MMDetection-tutorial'},\n",
    "         interval=10,\n",
    "         log_checkpoint=True,\n",
    "         log_checkpoint_metadata=True,\n",
    "         num_eval_images=10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèê Train\n",
    "\n",
    "Now that we have the dataset, pretrained model weight, and have defined the configs. Let's stitch them together to train an object detector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1Ô∏è‚É£ Build the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataset\n",
    "datasets = [build_dataset(cfg.data.train)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2Ô∏è‚É£ Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the detector\n",
    "model = build_detector(\n",
    "    cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# Add an attribute for visualization convenience\n",
    "model.CLASSES = datasets[0].CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3Ô∏è‚É£ Train with W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "\n",
    "# dump config\n",
    "cfg.dump(osp.join(cfg.work_dir, meta['exp_name']))\n",
    "\n",
    "# Train\n",
    "train_detector(model, datasets, cfg, distributed=False, validate=True, meta=meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4Ô∏è‚É£ Notes on using `MMDetWandbHook`. \n",
    "\n",
    "Using `MMDetWandbHook` is easy and in most cases it will throw friendly `UserWarning` if something is not quite right. However in the best interest, here are some of things and best practices you should keep in mind:\n",
    "\n",
    "* The `MMDetWandbHook` depends on `CheckpointHook` for logging the checkpoints as W&B Artifacts and `EvalHook`/`DistEvalHook` for logging validation data and model predictions. If anyone or both aren't available, this hook will give `UserWarning` and not cause any error. \n",
    "\n",
    "* The priority of both `CheckpointHook` and `EvalHook`/`DistEvalHook` should be more than `MMDetWandbHook`. \n",
    "\n",
    "* The validation data is logged once as `val_data` W&B Table. The evaluation tables, use reference to this data thus you will not be uploading the same data multiple times. \n",
    "\n",
    "* If you want to log the configuration to W&B, pass this key-value pair `'config': cfg._cfg_dict.to_dict()` to `init_kwargs`. However, it's recommended to use `meta['exp_name'] = config_filename` and pass `meta` to `train_detector`."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
