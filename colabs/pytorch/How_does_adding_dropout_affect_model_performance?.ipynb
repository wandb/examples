{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/gb6B4ig.png\" width=\"400\" alt=\"Weights & Biases\" />\n",
    "\n",
    "<!-- @wandbcode{pytorch-dropout} -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/How_does_adding_dropout_affect_model_performance%3F.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this colab, we'll see an example of adding dropout to a PyTorch model and observe the effect dropout has on the model's performance by tracking our models in [Weights & Biases](https://wandb.ai/wandb/getting-started/reports/Visualize-Debug-Machine-Learning-Models--VmlldzoyNzY5MDk).\n",
    "\n",
    "You can read more about using dropout in PyTorch [here](https://wandb.ai/authors/ayusht/reports/Dropout-in-PyTorch-An-Example--VmlldzoxNTgwOTE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install wandb -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download dataset and prepare `DataLoader`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root=\"./data\", train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root=\"./data\", train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "CLASS_NAMES = (\"plane\", \"car\", \"bird\", \"cat\",\n",
    "               \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(image_batch, label_batch):\n",
    "  plt.figure(figsize=(10,10))\n",
    "  for n in range(25):\n",
    "      ax = plt.subplot(5,5,n+1)\n",
    "      img = image_batch[n] / 2 + 0.5     # unnormalize\n",
    "      img = img.numpy()\n",
    "      plt.imshow(np.transpose(img, (1, 2, 0)))\n",
    "      plt.title(CLASS_NAMES[label_batch[n]])\n",
    "      plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_images, sample_labels = next(iter(trainloader))\n",
    "show_batch(sample_images, sample_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, criterion, epoch, steps_per_epoch=20):\n",
    "  # Switch model to training mode. This is necessary for layers like dropout, batchnorm etc which behave differently in training and evaluation mode\n",
    "  model.train()\n",
    "\n",
    "  train_loss = 0\n",
    "  train_total = 0\n",
    "  train_correct = 0\n",
    "\n",
    "  # We loop over the data iterator, and feed the inputs to the network and adjust the weights.\n",
    "  for batch_idx, (data, target) in enumerate(train_loader, start=0):\n",
    "    \n",
    "    # Load the input features and labels from the training dataset\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    \n",
    "    # Reset the gradients to 0 for all learnable weight parameters\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass: Pass image data from training dataset, make predictions about class image belongs to (0-9 in this case)\n",
    "    output = model(data)\n",
    "    \n",
    "    # Define our loss function, and compute the loss\n",
    "    loss = criterion(output, target)\n",
    "    train_loss += loss.item()\n",
    "\n",
    "    scores, predictions = torch.max(output.data, 1)\n",
    "    train_total += target.size(0)\n",
    "    train_correct += int(sum(predictions == target))\n",
    "            \n",
    "    # Reset the gradients to 0 for all learnable weight parameters\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute the gradients of the loss w.r.t. the model\"s parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the neural network weights\n",
    "    optimizer.step()\n",
    "\n",
    "  acc = round((train_correct / train_total) * 100, 2)\n",
    "  print(\"Epoch [{}], Loss: {}, Accuracy: {}\".format(epoch, train_loss/train_total, acc), end=\"\")\n",
    "  wandb.log({\"Train Loss\": train_loss/train_total, \"Train Accuracy\": acc, \"Epoch\": epoch})\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, criterion, classes):\n",
    "  # Switch model to evaluation mode. This is necessary for layers like dropout, batchnorm etc which behave differently in training and evaluation mode\n",
    "  model.eval()\n",
    "\n",
    "  test_loss = 0\n",
    "  test_total = 0\n",
    "  test_correct = 0\n",
    "\n",
    "  example_images = []\n",
    "  with torch.no_grad():\n",
    "      for data, target in test_loader:\n",
    "          # Load the input features and labels from the test dataset\n",
    "          data, target = data.to(device), target.to(device)\n",
    "          \n",
    "          # Make predictions: Pass image data from test dataset, make predictions about class image belongs to (0-9 in this case)\n",
    "          output = model(data)\n",
    "          \n",
    "          # Compute the loss sum up batch loss\n",
    "          test_loss += criterion(output, target).item()\n",
    "          \n",
    "          scores, predictions = torch.max(output.data, 1)\n",
    "          test_total += target.size(0)\n",
    "          test_correct += int(sum(predictions == target))\n",
    "\n",
    "  acc = round((test_correct / test_total) * 100, 2)\n",
    "  print(\" Test_loss: {}, Test_accuracy: {}\".format(test_loss/test_total, acc))\n",
    "  wandb.log({\"Test Loss\": test_loss/test_total, \"Test Accuracy\": acc})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the unregularized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self, input_shape=(3,32,32)):\n",
    "    super(Net, self).__init__()\n",
    "    \n",
    "    self.conv1 = nn.Conv2d(3, 32, 3)\n",
    "    self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "    self.conv3 = nn.Conv2d(64, 128, 3)\n",
    "    \n",
    "    self.pool = nn.MaxPool2d(2,2)\n",
    "\n",
    "    n_size = self._get_conv_output(input_shape)\n",
    "    \n",
    "    self.fc1 = nn.Linear(n_size, 512)\n",
    "    self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "  def _get_conv_output(self, shape):\n",
    "    batch_size = 1\n",
    "    input = torch.autograd.Variable(torch.rand(batch_size, *shape))\n",
    "    output_feat = self._forward_features(input)\n",
    "    n_size = output_feat.data.view(batch_size, -1).size(1)\n",
    "    return n_size\n",
    "\n",
    "  def _forward_features(self, x):\n",
    "    x = self.pool(F.relu(self.conv1(x)))\n",
    "    x = self.pool(F.relu(self.conv2(x)))\n",
    "    x = self.pool(F.relu(self.conv3(x)))\n",
    "    return x\n",
    "      \n",
    "  def forward(self, x):\n",
    "    x = self._forward_features(x)\n",
    "    x = x.view(x.size(0), -1)\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = self.fc2(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Model, Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net().to(device)\n",
    "print(net)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"dropout\")\n",
    "wandb.watch(net, log=\"all\")\n",
    "\n",
    "for epoch in range(8):\n",
    "  train(net, device, trainloader, optimizer, criterion, epoch)\n",
    "  test(net, device, testloader, criterion, CLASS_NAMES)\n",
    "\n",
    "print(\"Finished Training\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a model with dropout regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self, input_shape=(3,32,32)):\n",
    "    super(Net, self).__init__()\n",
    "    \n",
    "    self.conv1 = nn.Conv2d(3, 32, 3)\n",
    "    self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "    self.conv3 = nn.Conv2d(64, 128, 3)\n",
    "    \n",
    "    self.pool = nn.MaxPool2d(2,2)\n",
    "\n",
    "    n_size = self._get_conv_output(input_shape)\n",
    "    \n",
    "    self.fc1 = nn.Linear(n_size, 512)\n",
    "    self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "  def _get_conv_output(self, shape):\n",
    "    batch_size = 1\n",
    "    input = torch.autograd.Variable(torch.rand(batch_size, *shape))\n",
    "    output_feat = self._forward_features(input)\n",
    "    n_size = output_feat.data.view(batch_size, -1).size(1)\n",
    "    return n_size\n",
    "\n",
    "  def _forward_features(self, x):\n",
    "    x = self.pool(F.relu(self.conv1(x)))\n",
    "    x = self.pool(F.relu(self.conv2(x)))\n",
    "    x = self.pool(F.relu(self.conv3(x)))\n",
    "    return x\n",
    "      \n",
    "  def forward(self, x):\n",
    "    x = self._forward_features(x)\n",
    "    x = x.view(x.size(0), -1)\n",
    "    x = self.dropout(x)\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = self.dropout(x)\n",
    "    x = self.fc2(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Model, Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net().to(device)\n",
    "print(net)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"dropout\", anonymous=\"must\")\n",
    "wandb.watch(net, log=\"all\")\n",
    "\n",
    "for epoch in range(8):\n",
    "  train(net, device, trainloader, optimizer, criterion, epoch)\n",
    "  test(net, device, testloader, criterion, CLASS_NAMES)\n",
    "\n",
    "print(\"Finished Training\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/gb6B4ig.png\" width=\"400\" alt=\"Weights & Biases\" />\n",
    "\n",
    "# Visualize and debug model pipelines with W&B\n",
    "Think of [W&B](https://www.wandb.com/) like GitHub for machine learning models — **save everything you need to debug, compare and reproduce your models** — architecture, hyperparameters, weights, model predictions, GPU usage, git commits, and even datasets — with a few lines of code.\n",
    "\n",
    "W&B lightweight integrations work with any Python script, and all you need to do is sign up for a free W&B account to start tracking and visualizing your models.\n",
    "\n",
    "Used by the likes of OpenAI, Lyft, Github and researchers at top machine learning labs across the world, W&B is part of the new standard of best practices for machine learning.\n",
    "\n",
    "How W&B can help you optimize your machine learning workflows:\n",
    "\n",
    "- [Debug](https://wandb.ai/wandb/getting-started/reports/Visualize-Debug-Machine-Learning-Models--VmlldzoyNzY5MDk#Free-2) model performance in real time\n",
    "- Automatically tracked [GPU, CPU usage](https://wandb.ai/wandb/getting-started/reports/Visualize-Debug-Machine-Learning-Models--VmlldzoyNzY5MDk#System-4) and other system metrics\n",
    "- Powerful [custom charts](https://wandb.ai/wandb/customizable-charts/reports/Powerful-Custom-Charts-To-Debug-Model-Peformance--VmlldzoyNzY4ODI)\n",
    "- [Share model insights](https://wandb.ai/wandb/getting-started/reports/Visualize-Debug-Machine-Learning-Models--VmlldzoyNzY5MDk#Share-8) interactively\n",
    "- Efficient [hyperparameter optimization](https://docs.wandb.com/sweeps)\n",
    "- Dataset and model [pipeline tracking](https://docs.wandb.com/artifacts) and production model management\n",
    "\n",
    "**W&B is free for individuals, academics and open source projects.**\n",
    "\n",
    "![W&B Dashboard](https://api.wandb.ai/files/wandb/images/projects/26571/5efd7117.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
