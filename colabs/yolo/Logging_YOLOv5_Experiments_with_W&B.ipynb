{
 "accelerator": "GPU",
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76b762fd",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/yolo/Logging_YOLOv5_Experiments_with_W&B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "\n",
    "<!--- @wandbcode{yolov5-log} -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://user-images.githubusercontent.com/26833433/82952157-51b7db00-9f5d-11ea-8f4b-dda1ffecf992.jpg\">\n",
    "\n",
    "<!--- @wandbcode{yolov5-log} -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/gb6B4ig.png\" width=\"400\" alt=\"Weights & Biases\" />\n",
    "\n",
    "# You Always Log Everything (YALE)\n",
    "\n",
    "### Logging YOLOv5 Experiments with W&B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[YOLO](https://github.com/ultralytics/yolov5) (\"You Only Look Once\") provides tools for real-time object detection with convolutional neural networks.\n",
    "\n",
    "YOLO now works with [Weights & Biases](http://wandb.com),\n",
    "an experiment tracking toolkit, so you can\n",
    "keep track of all the hyperparameters you've tried,\n",
    "view real-time updates on system and model metrics,\n",
    "version and store datasets and models,\n",
    "[and more](http://github.com/wandb/examples)!\n",
    "\n",
    "In this colab, we'll show you how to use YOLO and W&B together.\n",
    "**It's as easy as running a single `pip install` before you run your YOLO experiments!**\n",
    "\n",
    "<h3> Follow along with a <a href=\"http://wandb.me/yolov5-video\"> video tutorial</a> on YouTube. </h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "First, let's get ourselves organized: clone the repo, install our dependencies, and confirm we've got PyTorch and a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
    "%cd yolov5\n",
    "%pip install -qr requirements.txt  # install dependencies\n",
    "\n",
    "import torch\n",
    "from IPython.display import Image, clear_output  # to display images\n",
    "\n",
    "clear_output()\n",
    "print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Inference\n",
    "\n",
    "Now, let's apply a pre-trained, already existing object detection network.\n",
    "\n",
    "`detect.py` runs inference on a variety of sources, downloading models automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases).\n",
    "\n",
    "Here, we'll just run a sample image through that network to make sure everything is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python detect.py --weights yolov5s.pt --img 640 --conf 0.25 --source data/images/\n",
    "Image(filename='runs/detect/exp/bus.jpg', width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are saved to `runs/detect`. A full list of available inference sources:\n",
    "<img src=\"https://user-images.githubusercontent.com/26833433/98274798-2b7a7a80-1f94-11eb-91a4-70c73593e26b.jpg\" width=\"900\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training to Fine-Tune\n",
    "\n",
    "For applications, it's often important to take a pre-trained model\n",
    "and fine-tune it to work on a specific dataset --\n",
    "[for example, in a construction safety application](https://wandb.ai/authors/artifact-workplace-safety/reports/Organize-Your-Machine-Learning-Pipelines-with-Artifacts--VmlldzoxODQwNTY), we might use fine-tuning to specialize our network in detecting the presence/absence of protective equipment.\n",
    "\n",
    "We'll mimic this process on the\n",
    "[COCO128](https://www.kaggle.com/ultralytics/coco128) image tutorial dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2# Download COCO128\n",
    "torch.hub.download_url_to_file('https://github.com/ultralytics/yolov5/releases/download/v1.0/coco128.zip', 'tmp.zip')\n",
    "!unzip -q tmp.zip -d ../ && rm tmp.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights & Biases Logging (ðŸš€ NEW)\n",
    "\n",
    "[Weights & Biases](https://www.wandb.com/) (W&B) is now integrated with YOLOv5 for real-time visualization and cloud logging of training runs. This allows for better run comparison and introspection, as well improved visibility and collaboration among team members. To enable W&B logging install `wandb`, and then train normally (you will be guided through setting up `wandb` account during your first use).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install \"wandb==0.12.10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! So long as W&B is installed, you'll get rich, detailed metrics in a live dashboard accessible from a browser on any device.\n",
    "\n",
    "Just click the link that appears below next to `wandb` and the ðŸš€ emoji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train YOLOv5s on COCO128 for 5 epochs\n",
    "!python train.py --img 640 --batch 64 --epochs 5 --data coco128.yaml --weights yolov5s.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With W&B, during training you will see live updates on the dashboard at [wandb.ai](https://www.wandb.ai/), including interactive bounding box visualizations (look for a panel called \"Images\" in the Media panel section), and you can create and share detailed [Reports](https://wandb.ai/glenn-jocher/yolov5_tutorial/reports/YOLOv5-COCO128-Tutorial-Results--VmlldzozMDI5OTY) of your results. For more information see the [YOLOv5 Weights & Biases Tutorial](https://github.com/ultralytics/yolov5/issues/1289)\n",
    "or check out the [video tutorial for this notebook](http://wandb.me/yolov5-video).\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/26833433/98184457-bd3da580-1f0a-11eb-8461-95d908a71893.jpg\" width=\"800\">"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
