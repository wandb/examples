{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brain tumor 3D segmentation with MONAI and Weights & Biases\n",
    "\n",
    "This tutorial shows how to construct a training workflow of multi-labels segmentation task using [MONAI](https://github.com/Project-MONAI/MONAI) and use experiment tracking and data visualization features of [Weights & Biases](https://wandb.ai/site). The tutorial contains the following features:\n",
    "\n",
    "1. Initialize a Weights & Biases run and synchrozize all configs associated with the run for reproducibility.\n",
    "2. MONAI transform API:\n",
    "    1. MONAI Transforms for dictionary format data.\n",
    "    2. How to define a new transform according to MONAI transform API.\n",
    "    3. How to randomly adjust intensity for data augmentation.\n",
    "3. Data Loading and Visualization:\n",
    "    1. Load Nifti image with metadata, load a list of images and stack them.\n",
    "    2. Cache IO and transforms to accelerate training and validation.\n",
    "    3. Visualize the data using `wandb.Table` and interactive segmentation overlay on Weights & Biases.\n",
    "4. Training a 3D `SegResNet` model\n",
    "    1. Using the `networks`, `losses`, and `metrics` APIs from MONAI.\n",
    "    2. Training the 3D `SegResNet` model using a PyTorch training loop.\n",
    "    3. Track the training experiment using Weights & Biases.\n",
    "    4. Log and version model checkpoints as model artifacts on Weights & Biases.\n",
    "5. Visualize and compare the predictions on the validation dataset using `wandb.Table` and interactive segmentation overlay on Weights & Biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, let us install the latest version of both MONAI and Weights and Biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import monai\" || pip install -q -U \"monai[nibabel, tqdm]\"\n",
    "!python -c \"import wandb\" || pip install -q -U wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "\n",
    "from monai.apps import DecathlonDataset\n",
    "from monai.data import DataLoader, decollate_batch\n",
    "from monai.losses import DiceLoss\n",
    "from monai.config import print_config\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.nets import SegResNet\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    MapTransform,\n",
    "    NormalizeIntensityd,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandScaleIntensityd,\n",
    "    RandShiftIntensityd,\n",
    "    RandSpatialCropd,\n",
    "    Spacingd,\n",
    "    EnsureTyped,\n",
    "    EnsureChannelFirstd,\n",
    ")\n",
    "from monai.utils import set_determinism\n",
    "\n",
    "import torch\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then authenticate this colab instance to use W&B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"monai-brain-tumor-segmentation\", job_type=\"test\")\n",
    "\n",
    "config = wandb.config\n",
    "config.seed = 0\n",
    "config.roi_size = [224, 224, 144]\n",
    "config.batch_size = 1\n",
    "config.num_workers = 4\n",
    "config.max_train_images_visualized = 20\n",
    "config.max_val_images_visualized = 20\n",
    "config.dice_loss_smoothen_numerator = 0\n",
    "config.dice_loss_smoothen_denominator = 1e-5\n",
    "config.dice_loss_squared_prediction = True\n",
    "config.dice_loss_target_onehot = False\n",
    "config.dice_loss_apply_sigmoid = True\n",
    "config.initial_learning_rate = 1e-4\n",
    "config.weight_decay = 1e-5\n",
    "config.max_train_epochs = 50\n",
    "config.validation_intervals = 1\n",
    "config.dataset_dir = \"./dataset/\"\n",
    "config.checkpoint_dir = \"./checkpoints\"\n",
    "config.inference_roi_size = (128, 128, 64)\n",
    "config.max_prediction_images_visualized = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(config.dataset_dir, exist_ok=True)\n",
    "os.makedirs(config.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "set_determinism(seed=config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvertToMultiChannelBasedOnBratsClassesd(MapTransform):\n",
    "    \"\"\"\n",
    "    Convert labels to multi channels based on brats classes:\n",
    "    label 1 is the peritumoral edema\n",
    "    label 2 is the GD-enhancing tumor\n",
    "    label 3 is the necrotic and non-enhancing tumor core\n",
    "    The possible classes are TC (Tumor core), WT (Whole tumor)\n",
    "    and ET (Enhancing tumor).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            result = []\n",
    "            # merge label 2 and label 3 to construct TC\n",
    "            result.append(torch.logical_or(d[key] == 2, d[key] == 3))\n",
    "            # merge labels 1, 2 and 3 to construct WT\n",
    "            result.append(\n",
    "                torch.logical_or(\n",
    "                    torch.logical_or(d[key] == 2, d[key] == 3), d[key] == 1\n",
    "                )\n",
    "            )\n",
    "            # label 2 is ET\n",
    "            result.append(d[key] == 2)\n",
    "            d[key] = torch.stack(result, axis=0).float()\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = Compose(\n",
    "    [\n",
    "        # load 4 Nifti images and stack them together\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=\"image\"),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "        ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            pixdim=(1.0, 1.0, 1.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        RandSpatialCropd(\n",
    "            keys=[\"image\", \"label\"], roi_size=config.roi_size, random_size=False\n",
    "        ),\n",
    "        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n",
    "        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=1),\n",
    "        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=2),\n",
    "        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "        RandScaleIntensityd(keys=\"image\", factors=0.1, prob=1.0),\n",
    "        RandShiftIntensityd(keys=\"image\", offsets=0.1, prob=1.0),\n",
    "    ]\n",
    ")\n",
    "val_transform = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=\"image\"),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "        ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            pixdim=(1.0, 1.0, 1.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DecathlonDataset(\n",
    "    root_dir=config.dataset_dir,\n",
    "    task=\"Task01_BrainTumour\",\n",
    "    transform=val_transform,\n",
    "    section=\"training\",\n",
    "    download=True,\n",
    "    cache_rate=0.0,\n",
    "    num_workers=4,\n",
    ")\n",
    "val_dataset = DecathlonDataset(\n",
    "    root_dir=config.dataset_dir,\n",
    "    task=\"Task01_BrainTumour\",\n",
    "    transform=val_transform,\n",
    "    section=\"validation\",\n",
    "    download=False,\n",
    "    cache_rate=0.0,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_data_samples_into_tables(\n",
    "    sample_image: np.array,\n",
    "    sample_label: np.array,\n",
    "    split: str = None,\n",
    "    data_idx: int = None,\n",
    "    table: wandb.Table = None,\n",
    "):\n",
    "    num_channels, _, _, num_slices = sample_image.shape\n",
    "    with tqdm(total=num_slices, leave=False) as progress_bar:\n",
    "        for slice_idx in range(num_slices):\n",
    "            ground_truth_wandb_images = []\n",
    "            for channel_idx in range(num_channels):\n",
    "                ground_truth_wandb_images.append(\n",
    "                    wandb.Image(\n",
    "                        sample_image[channel_idx, :, :, slice_idx],\n",
    "                        masks={\n",
    "                            \"ground-truth/Tumor-Core\": {\n",
    "                                \"mask_data\": sample_label[0, :, :, slice_idx],\n",
    "                                \"class_labels\": {0: \"background\", 1: \"Tumor Core\"},\n",
    "                            },\n",
    "                            \"ground-truth/Whole-Tumor\": {\n",
    "                                \"mask_data\": sample_label[1, :, :, slice_idx] * 2,\n",
    "                                \"class_labels\": {0: \"background\", 2: \"Whole Tumor\"},\n",
    "                            },\n",
    "                            \"ground-truth/Enhancing-Tumor\": {\n",
    "                                \"mask_data\": sample_label[2, :, :, slice_idx] * 3,\n",
    "                                \"class_labels\": {0: \"background\", 3: \"Enhancing Tumor\"},\n",
    "                            },\n",
    "                        },\n",
    "                    )\n",
    "                )\n",
    "            table.add_data(split, data_idx, slice_idx, *ground_truth_wandb_images)\n",
    "            progress_bar.update(1)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = wandb.Table(\n",
    "    columns=[\n",
    "        \"Split\",\n",
    "        \"Data Index\",\n",
    "        \"Slice Index\",\n",
    "        \"Image-Channel-0\",\n",
    "        \"Image-Channel-1\",\n",
    "        \"Image-Channel-2\",\n",
    "        \"Image-Channel-3\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "max_samples = (\n",
    "    min(config.max_train_images_visualized, len(train_dataset))\n",
    "    if config.max_train_images_visualized > 0\n",
    "    else len(train_dataset)\n",
    ")\n",
    "progress_bar = tqdm(\n",
    "    enumerate(train_dataset[:max_samples]),\n",
    "    total=max_samples,\n",
    "    desc=\"Generating Train Dataset Visualizations:\",\n",
    ")\n",
    "for data_idx, sample in progress_bar:\n",
    "    sample_image = sample[\"image\"].detach().cpu().numpy()\n",
    "    sample_label = sample[\"label\"].detach().cpu().numpy()\n",
    "    table = log_data_samples_into_tables(\n",
    "        sample_image,\n",
    "        sample_label,\n",
    "        split=\"train\",\n",
    "        data_idx=data_idx,\n",
    "        table=table,\n",
    "    )\n",
    "\n",
    "max_samples = (\n",
    "    min(config.max_val_images_visualized, len(val_dataset))\n",
    "    if config.max_val_images_visualized > 0\n",
    "    else len(val_dataset)\n",
    ")\n",
    "progress_bar = tqdm(\n",
    "    enumerate(val_dataset[:max_samples]),\n",
    "    total=max_samples,\n",
    "    desc=\"Generating Validation Dataset Visualizations:\",\n",
    ")\n",
    "for data_idx, sample in progress_bar:\n",
    "    sample_image = sample[\"image\"].detach().cpu().numpy()\n",
    "    sample_label = sample[\"label\"].detach().cpu().numpy()\n",
    "    table = log_data_samples_into_tables(\n",
    "        sample_image,\n",
    "        sample_label,\n",
    "        split=\"val\",\n",
    "        data_idx=data_idx,\n",
    "        table=table,\n",
    "    )\n",
    "\n",
    "wandb.log({\"Tumor-Segmentation-Data\": table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.transform = train_transform\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=config.num_workers,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=config.num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "model = SegResNet(\n",
    "    blocks_down=[1, 2, 2, 4],\n",
    "    blocks_up=[1, 1, 1],\n",
    "    init_filters=16,\n",
    "    in_channels=4,\n",
    "    out_channels=3,\n",
    "    dropout_prob=0.2,\n",
    ").to(device)\n",
    "loss_function = DiceLoss(\n",
    "    smooth_nr=config.dice_loss_smoothen_numerator,\n",
    "    smooth_dr=config.dice_loss_smoothen_denominator,\n",
    "    squared_pred=config.dice_loss_squared_prediction,\n",
    "    to_onehot_y=config.dice_loss_target_onehot,\n",
    "    sigmoid=config.dice_loss_apply_sigmoid,\n",
    ")\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    config.initial_learning_rate,\n",
    "    weight_decay=config.weight_decay,\n",
    ")\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=config.max_train_epochs\n",
    ")\n",
    "\n",
    "dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "dice_metric_batch = DiceMetric(include_background=True, reduction=\"mean_batch\")\n",
    "post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, input):\n",
    "    def _compute(input):\n",
    "        return sliding_window_inference(\n",
    "            inputs=input,\n",
    "            roi_size=(240, 240, 160),\n",
    "            sw_batch_size=1,\n",
    "            predictor=model,\n",
    "            overlap=0.5,\n",
    "        )\n",
    "\n",
    "    with torch.cuda.amp.autocast():\n",
    "        return _compute(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_step = 0\n",
    "validation_step = 0\n",
    "metric_values = []\n",
    "metric_values_tumor_core = []\n",
    "metric_values_whole_tumor = []\n",
    "metric_values_enhanced_tumor = []\n",
    "wandb.define_metric(\"epoch/epoch_step\")\n",
    "wandb.define_metric(\"epoch/*\", step_metric=\"epoch/epoch_step\")\n",
    "wandb.define_metric(\"batch/batch_step\")\n",
    "wandb.define_metric(\"batch/*\", step_metric=\"batch/batch_step\")\n",
    "wandb.define_metric(\"validation/validation_step\")\n",
    "wandb.define_metric(\"validation/*\", step_metric=\"validation/validation_step\")\n",
    "\n",
    "epoch_progress_bar = tqdm(range(config.max_train_epochs), desc=\"Training:\")\n",
    "for epoch in epoch_progress_bar:\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    total_batch_steps = len(train_dataset) // train_loader.batch_size\n",
    "    batch_progress_bar = tqdm(train_loader, total=total_batch_steps, leave=False)\n",
    "    for batch_data in batch_progress_bar:\n",
    "        inputs, labels = (\n",
    "            batch_data[\"image\"].to(device),\n",
    "            batch_data[\"label\"].to(device),\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        epoch_loss += loss.item()\n",
    "        batch_progress_bar.set_description(f\"train_loss: {loss.item():.4f}:\")\n",
    "        wandb.log({\"batch/batch_step\": batch_step, \"batch/train_loss\": loss.item()})\n",
    "        batch_step += 1\n",
    "\n",
    "    lr_scheduler.step()\n",
    "    epoch_loss /= total_batch_steps\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"epoch/epoch_step\": epoch,\n",
    "            \"epoch/mean_train_loss\": epoch_loss,\n",
    "            \"epoch/learning_rate\": lr_scheduler.get_last_lr()[0],\n",
    "        }\n",
    "    )\n",
    "    epoch_progress_bar.set_description(f\"Training: train_loss: {epoch_loss:.4f}:\")\n",
    "\n",
    "    if (epoch + 1) % config.validation_intervals == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_loader:\n",
    "                val_inputs, val_labels = (\n",
    "                    val_data[\"image\"].to(device),\n",
    "                    val_data[\"label\"].to(device),\n",
    "                )\n",
    "                val_outputs = inference(model, val_inputs)\n",
    "                val_outputs = [post_trans(i) for i in decollate_batch(val_outputs)]\n",
    "                dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "                dice_metric_batch(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "            metric_values.append(dice_metric.aggregate().item())\n",
    "            metric_batch = dice_metric_batch.aggregate()\n",
    "            metric_values_tumor_core.append(metric_batch[0].item())\n",
    "            metric_values_whole_tumor.append(metric_batch[1].item())\n",
    "            metric_values_enhanced_tumor.append(metric_batch[2].item())\n",
    "            dice_metric.reset()\n",
    "            dice_metric_batch.reset()\n",
    "\n",
    "            checkpoint_path = os.path.join(config.checkpoint_dir, \"model.pth\")\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            artifact = wandb.Artifact(\n",
    "                name=f\"{wandb.run.id}-checkpoint\", type=\"model\"\n",
    "            )\n",
    "            artifact.add_file(local_path=checkpoint_path)\n",
    "            wandb.log_artifact(artifact, aliases=[f\"epoch_{epoch}\"])\n",
    "\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"validation/validation_step\": validation_step,\n",
    "                    \"validation/mean_dice\": metric_values[-1],\n",
    "                    \"validation/mean_dice_tumor_core\": metric_values_tumor_core[-1],\n",
    "                    \"validation/mean_dice_whole_tumor\": metric_values_whole_tumor[\n",
    "                        -1\n",
    "                    ],\n",
    "                    \"validation/mean_dice_enhanced_tumor\": metric_values_enhanced_tumor[\n",
    "                        -1\n",
    "                    ],\n",
    "                }\n",
    "            )\n",
    "            validation_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_predictions_into_tables(\n",
    "    sample_image: np.array,\n",
    "    sample_label: np.array,\n",
    "    predicted_label: np.array,\n",
    "    split: str = None,\n",
    "    data_idx: int = None,\n",
    "    table: wandb.Table = None,\n",
    "):\n",
    "    num_channels, _, _, num_slices = sample_image.shape\n",
    "    with tqdm(total=num_slices, leave=False) as progress_bar:\n",
    "        for slice_idx in range(num_slices):\n",
    "            wandb_images = []\n",
    "            for channel_idx in range(num_channels):\n",
    "                wandb_images += [\n",
    "                    wandb.Image(\n",
    "                        sample_image[channel_idx, :, :, slice_idx],\n",
    "                        masks={\n",
    "                            \"ground-truth/Tumor-Core\": {\n",
    "                                \"mask_data\": sample_label[0, :, :, slice_idx],\n",
    "                                \"class_labels\": {0: \"background\", 1: \"Tumor Core\"},\n",
    "                            },\n",
    "                            \"prediction/Tumor-Core\": {\n",
    "                                \"mask_data\": predicted_label[0, :, :, slice_idx] * 2,\n",
    "                                \"class_labels\": {0: \"background\", 2: \"Tumor Core\"},\n",
    "                            },\n",
    "                        },\n",
    "                    ),\n",
    "                    wandb.Image(\n",
    "                        sample_image[channel_idx, :, :, slice_idx],\n",
    "                        masks={\n",
    "                            \"ground-truth/Whole-Tumor\": {\n",
    "                                \"mask_data\": sample_label[1, :, :, slice_idx],\n",
    "                                \"class_labels\": {0: \"background\", 1: \"Whole Tumor\"},\n",
    "                            },\n",
    "                            \"prediction/Whole-Tumor\": {\n",
    "                                \"mask_data\": predicted_label[1, :, :, slice_idx] * 2,\n",
    "                                \"class_labels\": {0: \"background\", 2: \"Whole Tumor\"},\n",
    "                            },\n",
    "                        },\n",
    "                    ),\n",
    "                    wandb.Image(\n",
    "                        sample_image[channel_idx, :, :, slice_idx],\n",
    "                        masks={\n",
    "                            \"ground-truth/Enhancing-Tumor\": {\n",
    "                                \"mask_data\": sample_label[2, :, :, slice_idx],\n",
    "                                \"class_labels\": {0: \"background\", 1: \"Enhancing Tumor\"},\n",
    "                            },\n",
    "                            \"prediction/Enhancing-Tumor\": {\n",
    "                                \"mask_data\": predicted_label[2, :, :, slice_idx] * 2,\n",
    "                                \"class_labels\": {0: \"background\", 2: \"Enhancing Tumor\"},\n",
    "                            },\n",
    "                        },\n",
    "                    ),\n",
    "                ]\n",
    "            table.add_data(split, data_idx, slice_idx, *wandb_images)\n",
    "            progress_bar.update(1)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_artifact = wandb.use_artifact(\n",
    "    \"geekyrakshit/monai-brain-tumor-segmentation/d5ex6n4a-checkpoint:v49\",\n",
    "    type=\"model\",\n",
    ")\n",
    "model_artifact_dir = model_artifact.download()\n",
    "model.load_state_dict(torch.load(os.path.join(model_artifact_dir, \"model.pth\")))\n",
    "model.eval()\n",
    "\n",
    "prediction_table = wandb.Table(\n",
    "    columns=[\n",
    "        \"Split\",\n",
    "        \"Data Index\",\n",
    "        \"Slice Index\",\n",
    "        \"Image-Channel-0/Tumor-Core\",\n",
    "        \"Image-Channel-1/Tumor-Core\",\n",
    "        \"Image-Channel-2/Tumor-Core\",\n",
    "        \"Image-Channel-3/Tumor-Core\",\n",
    "        \"Image-Channel-0/Whole-Tumor\",\n",
    "        \"Image-Channel-1/Whole-Tumor\",\n",
    "        \"Image-Channel-2/Whole-Tumor\",\n",
    "        \"Image-Channel-3/Whole-Tumor\",\n",
    "        \"Image-Channel-0/Enhancing-Tumor\",\n",
    "        \"Image-Channel-1/Enhancing-Tumor\",\n",
    "        \"Image-Channel-2/Enhancing-Tumor\",\n",
    "        \"Image-Channel-3/Enhancing-Tumor\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    config.max_prediction_images_visualized\n",
    "    max_samples = (\n",
    "        min(config.max_prediction_images_visualized, len(val_dataset))\n",
    "        if config.max_prediction_images_visualized > 0\n",
    "        else len(val_dataset)\n",
    "    )\n",
    "    progress_bar = tqdm(\n",
    "        enumerate(val_dataset[:max_samples]),\n",
    "        total=max_samples,\n",
    "        desc=\"Generating Predictions:\",\n",
    "    )\n",
    "    for data_idx, sample in progress_bar:\n",
    "        val_input = sample[\"image\"].unsqueeze(0).to(device)\n",
    "        val_output = inference(model, val_input)\n",
    "        val_output = post_trans(val_output[0])\n",
    "        prediction_table = log_predictions_into_tables(\n",
    "            sample_image=sample[\"image\"].cpu().numpy(),\n",
    "            sample_label=sample[\"label\"].cpu().numpy(),\n",
    "            predicted_label=val_output.cpu().numpy(),\n",
    "            data_idx=data_idx,\n",
    "            split=\"validation\",\n",
    "            table=prediction_table,\n",
    "        )\n",
    "\n",
    "    wandb.log({\"Predictions/Tumor-Segmentation-Data\": prediction_table})"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
