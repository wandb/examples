{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e205baf0",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/paella/Outpainting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "<!--- @wandbcode{paella-image-outpainting} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def788f4",
   "metadata": {},
   "source": [
    "# üî•üî• Image Outpainting with Paella + WandB Playground ü™Ñüêù\n",
    "\n",
    "<!--- @wandbcode{paella-image-outpainting} -->\n",
    "\n",
    "A demo of Image Outpainting using [Paella](https://github.com/dome272/Paella) and [Weights & Biases](https://wandb.ai/site)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19414087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import wandb\n",
    "import requests\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "\n",
    "import open_clip\n",
    "from rudalle import get_vae\n",
    "from einops import rearrange\n",
    "from open_clip import tokenizer\n",
    "\n",
    "from Paella.modules import DenoiseUNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef82556",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_project = \"paella\" #@param {\"type\": \"string\"}\n",
    "wandb_entity = \"geekyrakshit\" #@param {\"type\": \"string\"}\n",
    "\n",
    "wandb.init(project=wandb_project, entity=wandb_entity, job_type=\"outpainting\")\n",
    "\n",
    "\n",
    "config = wandb.config\n",
    "config.model_artifact = \"geekyrakshit/paella/text-model:v0\"\n",
    "config.seed = 42\n",
    "config.batch_size = 3\n",
    "config.latent_shape = (32, 32)\n",
    "config.image_url = \"https://media.istockphoto.com/id/1193591781/photo/obedient-dog-breed-welsh-corgi-pembroke-sitting-and-smiles-on-a-white-background-not-isolate.jpg?s=612x612&w=0&k=20&c=ZDKTgSFQFG9QvuDziGsnt55kvQoqJtIhrmVRkpYqxtQ=\"\n",
    "config.prompt = \"black & white photograph of a rocket from the bottom.\"\n",
    "config.target_size = 256\n",
    "config.batch_size = 5\n",
    "config.canvas_size = (40, 64)\n",
    "config.top_left = (0, 16)\n",
    "\n",
    "# Seed Everything\n",
    "torch.manual_seed(config.seed)\n",
    "torch.random.manual_seed(config.seed)\n",
    "torch.cuda.manual_seed(config.seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Download Model from Weights & Biases Artifacts\n",
    "text_model_path = os.path.join(wandb.use_artifact(config.model_artifact, type='model').download(), \"model_600000.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7274ce33",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd2036d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_inoutpainting_results(input_image, generated_images, encoded_tokens, mask):\n",
    "    generated_images = [\n",
    "        wandb.Image(image)\n",
    "        for image in (generated_images.cpu().numpy() * 255.0).astype(np.uint8)\n",
    "    ]\n",
    "    encoded_tokens = [wandb.Image(\n",
    "        token,\n",
    "        masks={\n",
    "            \"mask\": {\n",
    "                \"mask_data\": mask,\n",
    "                \"class_labels\": {\n",
    "                    0: \"non-masked\",\n",
    "                    1: \"region-of-interest\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ) for token in encoded_tokens]\n",
    "    table = wandb.Table(\n",
    "        columns=[\"Seed\", \"URL\", \"Input-Image\", \"Prompt\", \"Job-Type\", \"Encoded-Tokens\", \"Generated-Image\"]\n",
    "    )\n",
    "    table.add_data(\n",
    "        config.seed,\n",
    "        config.image_url,\n",
    "        wandb.Image(input_image),\n",
    "        config.prompt,\n",
    "        wandb.run.job_type,\n",
    "        encoded_tokens,\n",
    "        generated_images\n",
    "    )\n",
    "    wandb.log({\"Inpainting-Outpainting-Results\": table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1057acf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(t, eps=1e-20):\n",
    "    return torch.log(t + eps)\n",
    "\n",
    "def gumbel_noise(t):\n",
    "    noise = torch.zeros_like(t).uniform_(0, 1)\n",
    "    return -log(-log(noise))\n",
    "\n",
    "def gumbel_sample(t, temperature=1., dim=-1):\n",
    "    return ((t / max(temperature, 1e-10)) + gumbel_noise(t)).argmax(dim=dim)\n",
    "\n",
    "def sample(\n",
    "    model, c, x=None, mask=None, T=12, size=(32, 32),\n",
    "    starting_t=0, temp_range=[1.0, 1.0], typical_filtering=True,\n",
    "    typical_mass=0.2, typical_min_tokens=1, classifier_free_scale=-1,\n",
    "    renoise_steps=11, renoise_mode='start'\n",
    "):\n",
    "    with torch.inference_mode():\n",
    "        r_range = torch.linspace(0, 1, T+1)[:-1][:, None].expand(-1, c.size(0)).to(c.device)\n",
    "        temperatures = torch.linspace(temp_range[0], temp_range[1], T)\n",
    "        preds = []\n",
    "        if x is None:\n",
    "            x = torch.randint(0, model.num_labels, size=(c.size(0), *size), device=c.device)\n",
    "        elif mask is not None:\n",
    "            noise = torch.randint(0, model.num_labels, size=(c.size(0), *size), device=c.device)\n",
    "            x = noise * mask + (1-mask) * x\n",
    "        init_x = x.clone()\n",
    "        for i in range(starting_t, T):\n",
    "            if renoise_mode == 'prev':\n",
    "                prev_x = x.clone()\n",
    "            r, temp = r_range[i], temperatures[i]\n",
    "            logits = model(x, c, r)\n",
    "            if classifier_free_scale >= 0:\n",
    "                logits_uncond = model(x, torch.zeros_like(c), r)\n",
    "                logits = torch.lerp(logits_uncond, logits, classifier_free_scale)\n",
    "            x = logits\n",
    "            x_flat = x.permute(0, 2, 3, 1).reshape(-1, x.size(1))\n",
    "            if typical_filtering:\n",
    "                x_flat_norm = torch.nn.functional.log_softmax(x_flat, dim=-1)\n",
    "                x_flat_norm_p = torch.exp(x_flat_norm)\n",
    "                entropy = -(x_flat_norm * x_flat_norm_p).nansum(-1, keepdim=True)\n",
    "\n",
    "                c_flat_shifted = torch.abs((-x_flat_norm) - entropy)\n",
    "                c_flat_sorted, x_flat_indices = torch.sort(c_flat_shifted, descending=False)\n",
    "                x_flat_cumsum = x_flat.gather(-1, x_flat_indices).softmax(dim=-1).cumsum(dim=-1)\n",
    "\n",
    "                last_ind = (x_flat_cumsum < typical_mass).sum(dim=-1)\n",
    "                sorted_indices_to_remove = c_flat_sorted > c_flat_sorted.gather(1, last_ind.view(-1, 1))\n",
    "                if typical_min_tokens > 1:\n",
    "                    sorted_indices_to_remove[..., :typical_min_tokens] = 0\n",
    "                indices_to_remove = sorted_indices_to_remove.scatter(1, x_flat_indices, sorted_indices_to_remove)\n",
    "                x_flat = x_flat.masked_fill(indices_to_remove, -float(\"Inf\"))\n",
    "            x_flat = gumbel_sample(x_flat, temperature=temp)\n",
    "            x = x_flat.view(x.size(0), *x.shape[2:])\n",
    "            if mask is not None:\n",
    "                x = x * mask + (1-mask) * init_x\n",
    "            if i < renoise_steps:\n",
    "                if renoise_mode == 'start':\n",
    "                    x, _ = model.add_noise(x, r_range[i+1], random_x=init_x)\n",
    "                elif renoise_mode == 'prev':\n",
    "                    x, _ = model.add_noise(x, r_range[i+1], random_x=prev_x)\n",
    "                else: # 'rand'\n",
    "                    x, _ = model.add_noise(x, r_range[i+1])\n",
    "            preds.append(x.detach())\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a7beac",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqmodel = get_vae().to(device)\n",
    "vqmodel.eval().requires_grad_(False)\n",
    "\n",
    "clip_model, _, _ = open_clip.create_model_and_transforms('ViT-g-14', pretrained='laion2b_s12b_b42k')\n",
    "clip_model = clip_model.to(device).eval().requires_grad_(False)\n",
    "\n",
    "\n",
    "def encode(x):\n",
    "    return vqmodel.model.encode((2 * x - 1))[-1][-1]\n",
    "    \n",
    "def decode(img_seq, shape=(32,32)):\n",
    "        img_seq = img_seq.view(img_seq.shape[0], -1)\n",
    "        b, n = img_seq.shape\n",
    "        one_hot_indices = torch.nn.functional.one_hot(img_seq, num_classes=vqmodel.num_tokens).float()\n",
    "        z = (one_hot_indices @ vqmodel.model.quantize.embed.weight)\n",
    "        z = rearrange(z, 'b (h w) c -> b c h w', h=shape[0], w=shape[1])\n",
    "        img = vqmodel.model.decode(z)\n",
    "        img = (img.clamp(-1., 1.) + 1) * 0.5\n",
    "        return img\n",
    "    \n",
    "state_dict = torch.load(text_model_path, map_location=device)\n",
    "model = DenoiseUNet(num_labels=8192).to(device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval().requires_grad_()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757f8a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(config.image_url)\n",
    "original_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "\n",
    "preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(config.target_size),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "images = preprocess(original_image).unsqueeze(0).expand(config.batch_size, -1, -1, -1).to(device)[:, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2574a92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text_and_image = tokenizer.tokenize([config.prompt] * images.shape[0]).to(device)\n",
    "with torch.inference_mode():\n",
    "    with torch.autocast(device_type=\"cuda\"):\n",
    "        clip_embeddings = clip_model.encode_text(tokenized_text_and_image).float()\n",
    "        encoded_tokens = encode(images)\n",
    "        canvas = torch.zeros((images.shape[0], *config.canvas_size), dtype=torch.long).to(device)\n",
    "        canvas[\n",
    "            :,\n",
    "            config.top_left[0]: config.top_left[0] + encoded_tokens.shape[1],\n",
    "            config.top_left[1]: config.top_left[1] + encoded_tokens.shape[2]\n",
    "        ] = encoded_tokens\n",
    "        mask = torch.ones_like(canvas)\n",
    "        mask[\n",
    "            :,\n",
    "            config.top_left[0]: config.top_left[0] + encoded_tokens.shape[1],\n",
    "            config.top_left[1]: config.top_left[1] + encoded_tokens.shape[2]\n",
    "        ] = 0\n",
    "        s = time.time()\n",
    "        sampled = sample(\n",
    "            model, clip_embeddings, x=canvas, mask=mask, T=12,\n",
    "            size=config.canvas_size, starting_t=0, temp_range=[1.0, 1.0],\n",
    "            typical_filtering=True, typical_mass=0.2, typical_min_tokens=1,\n",
    "            classifier_free_scale=4, renoise_steps=11\n",
    "        )\n",
    "        wandb.log({\"Sampling-Time\": time.time() - s})\n",
    "    sampled = decode(sampled[-1], config.canvas_size).permute(0, 2, 3, 1)\n",
    "    mask = mask[0:1].cpu().numpy()[0]\n",
    "    encoded_tokens = encoded_tokens.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afff6dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_inoutpainting_results(original_image, sampled, encoded_tokens, mask)\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
