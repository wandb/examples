{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/diffusers/diffusers-image-generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "<!--- @wandbcode{unconditional-diffusers-colab} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üî•üî• Unconditional Image Generation using ü§ó Diffusers + Weights & Biases ü™Ñüêù\n",
    "\n",
    "<!--- @wandbcode{unconditional-diffusers-colab} -->\n",
    "\n",
    "**Reference:** [Official Diffusers Example for Unconditional Image Generation](https://github.com/huggingface/diffusers/tree/main/examples/unconditional_image_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -qq accelerate diffusers datasets wandb ml_collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "!accelerate config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "!accelerate env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    InterpolationMode,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate import notebook_launcher\n",
    "from accelerate.logging import get_logger\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from diffusers import UNet2DModel\n",
    "from diffusers import DDPMPipeline, DDPMScheduler\n",
    "from diffusers import DDIMPipeline, DDIMScheduler\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.training_utils import EMAModel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "from ml_collections import ConfigDict\n",
    "from tqdm.auto import tqdm\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "config = ConfigDict()\n",
    "\n",
    "##################### Dataset Configs #####################\n",
    "\n",
    "# The name of the Dataset (from the HuggingFace hub or WandB Artifacts) to train on (could be your own,\n",
    "# possibly private, dataset). It can also be a path pointing to a local copy of a dataset in your filesystem,\n",
    "# or to a folder containing files that HF Datasets can understand.\n",
    "config.dataset_name = \"geekyrakshit/diffusers-image-generation/anime-faces:v0\" #@param {type:\"string\"}\n",
    "\n",
    "# Is the config.dataset_name a Weights & Biase artifact or not\n",
    "config.is_dataset_wandb_artifact = True  #@param {type:\"boolean\"}\n",
    "\n",
    "# The config of the Dataset, leave as None if there's only one config.\n",
    "config.dataset_config_name = None #@param {type:\"raw\"}\n",
    "\n",
    "# A folder containing the training data. Folder contents must follow the structure described in\n",
    "# https://huggingface.co/docs/datasets/image_dataset#imagefolder. In particular, a `metadata.jsonl` file\n",
    "# must exist to provide the captions for the images. Ignored if `dataset_name` is specified.\n",
    "config.train_data_dir = None #@param {type:\"raw\"}\n",
    "\n",
    "# The output directory where the model predictions and checkpoints will be written.\n",
    "config.output_dir = \"ddpm-model-64\" #@param {type:\"string\"}\n",
    "\n",
    "# The directory where the downloaded models and datasets will be stored.\n",
    "config.cache_dir = None #@param {type:\"raw\"}\n",
    "\n",
    "\n",
    "##################### Training Configs #####################\n",
    "\n",
    "# Type of Diffusion pipeline\n",
    "config.diffusion_pipeline = \"ddim\" #@param [\"ddpm\", \"ddim\"] {type:\"string\"}\n",
    "\n",
    "# The resolution for input images, all the images in the train/validation dataset will be resized to\n",
    "# this resolution.\n",
    "config.resolution = 64 #@param {type:\"slider\", min:64, max:1024, step:4}\n",
    "\n",
    "# Batch size (per device) for the training dataloader.\n",
    "config.train_batch_size = 64 #@param {type:\"slider\", min:16, max:256, step:16}\n",
    "\n",
    "# The number of images to generate for evaluation.\n",
    "config.eval_batch_size = 64 #@param {type:\"slider\", min:16, max:256, step:16}\n",
    "\n",
    "# The number of subprocesses to use for data loading. 0 means that the data will be loaded in the\n",
    "# main process.\n",
    "config.dataloader_num_workers = 0 #@param {type:\"slider\", min:0, max:16, step:1}\n",
    "\n",
    "# Number of diffusion steps used to train the model.\n",
    "config.num_train_timesteps = 1000 #@param {type:\"slider\", min:0, max:5000, step:100}\n",
    "\n",
    "# Number of training epochs\n",
    "config.num_epochs = 100 #@param {type:\"slider\", min:0, max:500, step:1}\n",
    "\n",
    "# How often to save images during training.\n",
    "config.save_images_epochs = 10 #@param {type:\"slider\", min:0, max:100, step:5}\n",
    "\n",
    "# How often to save the model during training.\n",
    "config.save_model_epochs = 10 #@param {type:\"slider\", min:0, max:100, step:5}\n",
    "\n",
    "# Number of updates steps to accumulate before performing a backward/update pass.\n",
    "config.gradient_accumulation_steps = 1 #@param {type:\"slider\", min:0, max:10, step:1}\n",
    "\n",
    "# Initial learning rate (after the potential warmup period) to use.\n",
    "config.learning_rate = 1e-4 #@param {type:\"number\"}\n",
    "\n",
    "# The scheduler type to use. Choose between\n",
    "# [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"]\n",
    "config.lr_scheduler = \"cosine\" #@param [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"] {type:\"string\"}\n",
    "\n",
    "# Number of steps for the warmup in the learning rate scheduler.\n",
    "config.lr_warmup_steps = 500 #@param {type:\"slider\", min:0, max:1000, step:50}\n",
    "\n",
    "# The exponential decay rate for the 1st moment estimates (the beta1 parameter for the Adam optimizer).\n",
    "config.adam_beta1 = 0.95 #@param {type:\"number\"}\n",
    "\n",
    "# The exponential decay rate for the 2nd moment estimates (the beta2 parameter for the Adam optimizer).\n",
    "config.adam_beta2 = 0.999 #@param {type:\"number\"}\n",
    "\n",
    "# Weight decay magnitude for the Adam optimizer.\n",
    "config.adam_weight_decay = 1e-6 #@param {type:\"number\"}\n",
    "\n",
    "# Epsilon value for the Adam optimizer.\n",
    "config.adam_epsilon = 1e-08 #@param {type:\"number\"}\n",
    "\n",
    "# Whether to use Exponential Moving Average for the final model weights.\n",
    "config.use_ema = True #@param {type:\"boolean\"}\n",
    "\n",
    "# The inverse gamma value for the EMA decay.\n",
    "config.ema_inv_gamma = 1.0 #@param {type:\"number\"}\n",
    "\n",
    "# The power value for the EMA decay.\n",
    "config.ema_power = 3 / 4 #@param {type:\"raw\"}\n",
    "\n",
    "# The maximum decay magnitude for EMA.\n",
    "config.ema_max_decay = 0.9999 #@param {type:\"number\"}\n",
    "\n",
    "# For distributed training: local_rank\n",
    "config.local_rank = -1 #@param {type:\"number\"}\n",
    "\n",
    "# Number of processes\n",
    "config.num_processes = 1 #@param {type:\"number\"}\n",
    "\n",
    "# Whether to use mixed precision.\n",
    "# Choose between \"no\", \"fp16\" and \"bf16\" (bfloat16).\n",
    "# Note that Bf16 requires PyTorch >= 1.10. and an Nvidia Ampere GPU.\n",
    "config.mixed_precision = \"no\" #@param [\"no\", \"fp16\", \"bf16\"] {type:\"raw\"}\n",
    "\n",
    "\n",
    "##################### Weights & Biases Configs #####################\n",
    "\n",
    "# Weights & Biases Project\n",
    "config.wandb_project = \"diffusers-image-generation\" #@param {type:\"string\"}\n",
    "\n",
    "# Weights & Biases Entity\n",
    "config.wandb_entity = \"geekyrakshit\" #@param {type:\"string\"}\n",
    "\n",
    "# Number of images to be visualized in a table\n",
    "config.num_images_in_table = 6 #@param {type:\"slider\", min:1, max:50, step:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def build_unet_model():\n",
    "    return UNet2DModel(\n",
    "        sample_size=config.resolution,\n",
    "        in_channels=3,\n",
    "        out_channels=3,\n",
    "        layers_per_block=2,\n",
    "        block_out_channels=(128, 128, 256, 256, 512, 512),\n",
    "        down_block_types=(\n",
    "            \"DownBlock2D\",\n",
    "            \"DownBlock2D\",\n",
    "            \"DownBlock2D\",\n",
    "            \"DownBlock2D\",\n",
    "            \"AttnDownBlock2D\",\n",
    "            \"DownBlock2D\",\n",
    "        ),\n",
    "        up_block_types=(\n",
    "            \"UpBlock2D\",\n",
    "            \"AttnUpBlock2D\",\n",
    "            \"UpBlock2D\",\n",
    "            \"UpBlock2D\",\n",
    "            \"UpBlock2D\",\n",
    "            \"UpBlock2D\",\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def transforms(examples):\n",
    "    augmentations = Compose(\n",
    "        [\n",
    "            Resize(\n",
    "                config.resolution,\n",
    "                interpolation=InterpolationMode.BILINEAR\n",
    "            ),\n",
    "            CenterCrop(config.resolution),\n",
    "            RandomHorizontalFlip(),\n",
    "            ToTensor(),\n",
    "            Normalize([0.5], [0.5]),\n",
    "        ]\n",
    "    )\n",
    "    images = [\n",
    "        augmentations(image.convert(\"RGB\"))\n",
    "        for image in examples[\"image\"]\n",
    "    ]\n",
    "    return {\"input\": images}\n",
    "\n",
    "\n",
    "def build_dataloader():\n",
    "    if not config.is_dataset_wandb_artifact:\n",
    "        dataset = (\n",
    "            load_dataset(\n",
    "                config.dataset_name,\n",
    "                config.dataset_config_name,\n",
    "                cache_dir=config.cache_dir,\n",
    "                split=\"train\",\n",
    "            )\n",
    "            if config.dataset_name is not None else\n",
    "            load_dataset(\n",
    "                \"imagefolder\",\n",
    "                data_dir=config.train_data_dir,\n",
    "                cache_dir=config.cache_dir,\n",
    "                split=\"train\"\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        artifact = wandb.use_artifact(config.dataset_name, type='dataset')\n",
    "        artifact_dir = artifact.download()\n",
    "        config.train_data_dir = artifact_dir\n",
    "        dataset = load_dataset(\n",
    "            \"imagefolder\",\n",
    "            data_dir=config.train_data_dir,\n",
    "            cache_dir=config.cache_dir,\n",
    "            split=\"train\"\n",
    "        )\n",
    "\n",
    "    dataset.set_transform(transforms)\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=config.train_batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=config.dataloader_num_workers\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def training_loop():\n",
    "    # Initialize Accelerator\n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        mixed_precision=config.mixed_precision,\n",
    "        log_with=\"wandb\"\n",
    "    )\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        accelerator.init_trackers(\n",
    "            project_name=config.wandb_project, \n",
    "            init_kwargs={\n",
    "                \"wandb\": {\n",
    "                    'entity': config.wandb_entity,\n",
    "                    'config': config.to_dict()\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        wandb_table = wandb.Table(\n",
    "            columns=['Epoch', 'Step', 'Generated-Images']\n",
    "        )\n",
    "    \n",
    "    # Initialize Train Dataloader\n",
    "    train_dataloader = build_dataloader()\n",
    "    \n",
    "    # Initialize Model\n",
    "    model = build_unet_model()\n",
    "    \n",
    "    # Initialize Diffusion Pipeline\n",
    "    noise_scheduler = DDPMScheduler(\n",
    "        num_train_timesteps=config.num_train_timesteps\n",
    "    ) if config.diffusion_pipeline == \"ddpm\" else DDIMScheduler(\n",
    "        num_train_timesteps=config.num_train_timesteps\n",
    "    )\n",
    "    \n",
    "    # Initialize AdamW optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config.learning_rate,\n",
    "        betas=(config.adam_beta1, config.adam_beta2),\n",
    "        weight_decay=config.adam_weight_decay,\n",
    "        eps=config.adam_epsilon,\n",
    "    )\n",
    "    \n",
    "    # Initialize Learning Rate Scheduler\n",
    "    lr_scheduler = get_scheduler(\n",
    "        config.lr_scheduler,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=config.lr_warmup_steps,\n",
    "        num_training_steps=(\n",
    "            len(train_dataloader) * config.num_epochs\n",
    "        ) // config.gradient_accumulation_steps,\n",
    "    )\n",
    "\n",
    "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, lr_scheduler\n",
    "    )\n",
    "\n",
    "    num_update_steps_per_epoch = math.ceil(\n",
    "        len(train_dataloader) / config.gradient_accumulation_steps\n",
    "    )\n",
    "\n",
    "    ema_model = EMAModel(\n",
    "        model,\n",
    "        inv_gamma=config.ema_inv_gamma,\n",
    "        power=config.ema_power,\n",
    "        max_value=config.ema_max_decay\n",
    "    )\n",
    "    \n",
    "    global_step = 0\n",
    "    for epoch in range(config.num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        progress_bar = tqdm(\n",
    "            total=num_update_steps_per_epoch,\n",
    "            disable=not accelerator.is_local_main_process\n",
    "        )\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            clean_images = batch[\"input\"]\n",
    "            # Sample noise that we'll add to the images\n",
    "            noise = torch.randn(clean_images.shape).to(clean_images.device)\n",
    "            bsz = clean_images.shape[0]\n",
    "            # Sample a random timestep for each image\n",
    "            timesteps = torch.randint(\n",
    "                0,\n",
    "                noise_scheduler.config.num_train_timesteps,\n",
    "                (bsz,),\n",
    "                device=clean_images.device\n",
    "            ).long()\n",
    "\n",
    "            # Add noise to the clean images according to the noise magnitude\n",
    "            # at each timestep (this is the forward diffusion process)\n",
    "            noisy_images = noise_scheduler.add_noise(\n",
    "                clean_images, noise, timesteps\n",
    "            )\n",
    "\n",
    "            with accelerator.accumulate(model):\n",
    "                # Predict the noise residual\n",
    "                noise_pred = model(noisy_images, timesteps).sample\n",
    "                loss = F.mse_loss(noise_pred, noise)\n",
    "                accelerator.backward(loss)\n",
    "\n",
    "                if accelerator.sync_gradients:\n",
    "                    accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                if config.use_ema:\n",
    "                    ema_model.step(model)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # Checks if the accelerator has performed an optimization step\n",
    "            # behind the scenes\n",
    "            if accelerator.sync_gradients:\n",
    "                progress_bar.update(1)\n",
    "                global_step += 1\n",
    "\n",
    "            logs = {\n",
    "                \"loss\": loss.detach().item(),\n",
    "                \"lr\": lr_scheduler.get_last_lr()[0],\n",
    "                \"step\": global_step\n",
    "            }\n",
    "            if config.use_ema:\n",
    "                logs[\"ema_decay\"] = ema_model.decay\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            accelerator.log(logs, step=global_step)\n",
    "\n",
    "        accelerator.log({'epoch':epoch}, step=global_step)\n",
    "        progress_bar.close()\n",
    "\n",
    "        accelerator.wait_for_everyone()\n",
    "\n",
    "        # Generate sample images for visual inspection\n",
    "        if accelerator.is_main_process:\n",
    "            if epoch % config.save_images_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                pipeline = DDPMPipeline(\n",
    "                    unet=accelerator.unwrap_model(\n",
    "                        ema_model.averaged_model if config.use_ema else model\n",
    "                    ),\n",
    "                    scheduler=noise_scheduler,\n",
    "                ) if config.diffusion_pipeline == \"ddpm\" else DDIMPipeline(\n",
    "                    unet=accelerator.unwrap_model(\n",
    "                        ema_model.averaged_model if config.use_ema else model\n",
    "                    ),\n",
    "                    scheduler=noise_scheduler,\n",
    "                )\n",
    "\n",
    "                generator = torch.manual_seed(0)\n",
    "                # run pipeline in inference (sample random noise and denoise)\n",
    "                images = pipeline(\n",
    "                    generator=generator,\n",
    "                    batch_size=config.eval_batch_size,\n",
    "                    output_type=\"numpy\"\n",
    "                ).images\n",
    "\n",
    "                # denormalize the images and save to wandb\n",
    "                images_processed = (images * 255).round().astype(\"uint8\")\n",
    "                wandb_images = [wandb.Image(i) for i in images_processed]\n",
    "\n",
    "                \n",
    "                wandb_table.add_data(\n",
    "                    epoch,\n",
    "                    global_step,\n",
    "                    wandb_images[:config.num_images_in_table]\n",
    "                )\n",
    "\n",
    "                wandb.log({'generated_images':wandb_images,}, step=global_step)\n",
    "\n",
    "            if epoch % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                # save the model\n",
    "                pipeline.save_pretrained(config.output_dir)\n",
    "\n",
    "                # log wandb artifact\n",
    "                model_artifact = wandb.Artifact(\n",
    "                    f'{wandb.run.id}-{config.output_dir}', \n",
    "                    type='model'\n",
    "                    )\n",
    "                model_artifact.add_dir(config.output_dir)\n",
    "                wandb.log_artifact(\n",
    "                    model_artifact,\n",
    "                    aliases=[f'step_{global_step}', f'epoch_{epoch}']\n",
    "                )\n",
    "\n",
    "        accelerator.wait_for_everyone()\n",
    "\n",
    "    wandb.log({'Generated-Images-Table': wandb_table})\n",
    "    accelerator.end_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "notebook_launcher(training_loop, num_processes=config.num_processes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tensorflow')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
