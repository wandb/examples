{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/diffusers/sdxl-text-to-image.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable-Diffusion XL 1.0 using ðŸ¤— Diffusers\n",
    "\n",
    "This notebook demonstrates the following:\n",
    "- Performing text-conditional image-generations using [ðŸ¤— Diffusers](https://huggingface.co/docs/diffusers).\n",
    "- Using the Stable Diffusion XL Refiner pipeline to further refine the outputs of the base model.\n",
    "- Manage image generation experiments using [Weights & Biases](http://wandb.ai/geekyrakshit).\n",
    "- Log the prompts and generated images to [Weigts & Biases](http://wandb.ai/geekyrakshit) for visalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing the Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq diffusers[\"torch\"] transformers wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "from diffusers import (\n",
    "    StableDiffusionXLPipeline,\n",
    "    StableDiffusionXLImg2ImgPipeline,\n",
    "    EulerDiscreteScheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Management using Weights & Biases\n",
    "\n",
    "Managing our image generation experiments is crucial for the sake of reproducibility. Hence we sync all the configs of our experiments with our Weights & Biases run. This stores all the configs of the experiments, right from the prompts to the refinement technque and the configuration of the scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"stable-diffusion-xl\" # @param {type:\"string\"}\n",
    "\n",
    "# initialize a wandb run\n",
    "wandb.init(project=project_name, job_type=\"text-to-image\")\n",
    "\n",
    "# define experiment configs\n",
    "config = wandb.config\n",
    "config.stable_diffusion_checkpoint = \"stabilityai/stable-diffusion-xl-base-1.0\" # @param [\"stabilityai/stable-diffusion-xl-base-1.0\", \"stabilityai/stable-diffusion-xl-base-0.9\"] {allow-input: true}\n",
    "config.refiner_checkpoint = \"stabilityai/stable-diffusion-xl-refiner-1.0\" # @param [\"stabilityai/stable-diffusion-xl-refiner-1.0\", \"stabilityai/stable-diffusion-xl-refiner-0.9\"] {allow-input: true}\n",
    "config.compile_model = False\n",
    "config.prompt_1 = \"a photograph of an evil and vile looking demon in Bengali attire eating fish. The demon has large and bloody teeth. The demon is sitting on the branches of a giant Banyan tree, dimly lit, bluish and dark color palette, realistic, 8k\" # @param {type:\"string\"}\n",
    "config.prompt_2 = \"\" # @param {type:\"string\"}\n",
    "config.negative_prompt_1 = \"static, frame, painting, illustration, sd character, low quality, low resolution, greyscale, monochrome, nose, cropped, lowres, jpeg artifacts, deformed iris, deformed pupils, bad eyes, semi-realistic worst quality, bad lips, deformed mouth, deformed face, deformed fingers, deformed toes standing still, posing\" # @param {type:\"string\"}\n",
    "config.negative_prompt_2 = \"static, frame, painting, illustration, sd character, low quality, low resolution, greyscale, monochrome, nose, cropped, lowres, jpeg artifacts, deformed iris, deformed pupils, bad eyes, semi-realistic worst quality, bad lips, deformed mouth, deformed face, deformed fingers, deformed toes standing still, posing\" # @param {type:\"string\"}\n",
    "config.base_guidance_scale = 5.0 # @param {type:\"slider\", min:1, max:10, step:0.1}\n",
    "config.seed = 0 # @param {type:\"raw\"}\n",
    "config.num_inference_steps = 100 # @param {type:\"slider\", min:1, max:500, step:1}\n",
    "\n",
    "config.enable_cpu_offload_base = True  # @param {type:\"boolean\"}\n",
    "config.enable_cpu_offload_refiner = True  # @param {type:\"boolean\"}\n",
    "\n",
    "config.compile_base_model = False # @param {type:\"boolean\"}\n",
    "\n",
    "# Enable refinement only if high-ram instance\n",
    "config.enable_refinement = False  # @param {type:\"boolean\"}\n",
    "config.compile_refinement_model = False # @param {type:\"boolean\"}\n",
    "config.refiner_guidance_scale = 5.0 # @param {type:\"slider\", min:1, max:10, step:0.1}\n",
    "config.num_refinement_steps = 150 # @param {type:\"slider\", min:1, max:500, step:1}\n",
    "\n",
    "# Set explicitly only if config.use_ensemble_of_experts is True\n",
    "config.high_noise_fraction = 0.8 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
    "\n",
    "beta_schedule = \"scaled_linear\" # @param [\"linear\", \"scaled_linear\"]\n",
    "interpolation_type =  \"linear\" # @param [\"linear\", \"log_linear\"] {allow-input: true}\n",
    "prediction_type = \"epsilon\" # @param [\"epsilon\", \"sample\", \"v_prediction\"]\n",
    "timestep_spacing =  \"leading\" # @param [\"linspace\", \"leading\"] {allow-input: true}\n",
    "\n",
    "# configs for diffusers.EulerDiscreteScheduler\n",
    "scheduler_kwargs = {\n",
    "    \"beta_end\": 0.012,\n",
    "    \"beta_schedule\": beta_schedule,\n",
    "    \"beta_start\": 0.00085,\n",
    "    \"interpolation_type\": interpolation_type,\n",
    "    \"num_train_timesteps\": 1000,\n",
    "    \"prediction_type\": prediction_type,\n",
    "    \"steps_offset\": 1,\n",
    "    \"timestep_spacing\": timestep_spacing,\n",
    "    \"trained_betas\": None,\n",
    "    \"use_karras_sigmas\": False,\n",
    "}\n",
    "\n",
    "config.scheduler_kwargs = scheduler_kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make the experiment deterministic based on the seed specified in the experiment configs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = [torch.Generator(device=\"cuda\")]\n",
    "if config.seed:\n",
    "    generator = [g.manual_seed(config.seed) for g in generator]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Base Diffusion Pipelines\n",
    "\n",
    "For performing text-conditional image generation, we use the `diffusers` library to define the diffusion pipelines corresponding to the base SDXL model and the SDXL refinement model.\n",
    "\n",
    "1. We define the base diffusion pipeline using `diffusers.DiffusionPipeline` and load the pre-trained weights for SDXL 1.0 by calling the `from_pretrained` function on it. We also pass the scheduler as `diffusers.EulerDiscreteScheduler` in this  step.\n",
    "\n",
    "2. In case we don't have a GPU with large enough GPU, it's recommended to enable CPU offloading. Otherwise, we load the model on the GPU. In case you're curious how HiggingFace manages CPU offloading in the most optimized manner, we recommend you read this port by [Sylvain Gugger](https://huggingface.co/sgugger): [How ðŸ¤— Accelerate runs very large models thanks to PyTorch](https://huggingface.co/blog/accelerate-large-models).\n",
    "\n",
    "3. We can compile model using `torch.compile`, this might give a significant speedup.\n",
    "\n",
    "4. We generate the image from the prompts and negative prompts using the base pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Base Pipeline\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    config.stable_diffusion_checkpoint,\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    "    use_safetensors=True,\n",
    "    scheduler=EulerDiscreteScheduler(**config.scheduler_kwargs),\n",
    ")\n",
    "\n",
    "if config.enable_cpu_offload_base:\n",
    "    # Offload base pipeline to CPU\n",
    "    pipe.enable_model_cpu_offload()\n",
    "else:\n",
    "    # Load base pipeline to GPU\n",
    "    pipe.to(\"cuda\")\n",
    "\n",
    "# Compile model using `torch.compile`, this might give a significant speedup\n",
    "if config.compile_base_model:\n",
    "    pipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\n",
    "\n",
    "# Generate image from the prompts and negative prompts using the base pipeline\n",
    "generated_image = pipe(\n",
    "    prompt=config.prompt_1,\n",
    "    prompt_2=config.prompt_2,\n",
    "    negative_prompt=config.negative_prompt_1,\n",
    "    negative_prompt_2=config.negative_prompt_2,\n",
    "    guidance_scale=config.base_guidance_scale,\n",
    "    output_type=\"latent\" if config.enable_refinement else \"pil\",\n",
    "    num_inference_steps=config.num_inference_steps,\n",
    "    denoising_end=config.high_noise_fraction if config.enable_refinement else None,\n",
    "    generator=generator,\n",
    ").images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refining the Generated Image\n",
    "\n",
    "For refining the image generated by the base pipeline, we using the SDXL Refiner pipeline using the base and refiner model as an ensemble of expert of denoisers. In this case, the base model should serve as the expert for the high-noise diffusion stage and the refiner serves as the expert for the low-noise diffusion stage.\n",
    "\n",
    "1. We define the diffusion pipeline for the refiner using `diffusers.DiffusionPipeline` and load the pre-trained weights for SDXL 1.0 refiner by calling the `from_pretrained` function on it. We also pass the scheduler as `diffusers.EulerDiscreteScheduler` in this  step.\n",
    "\n",
    "2. In case we don't have a GPU with large enough GPU, it's recommended to enable CPU offloading. Otherwise, we load the model on the GPU. In case you're curious how HiggingFace manages CPU offloading in the most optimized manner, we recommend you read this port by [Sylvain Gugger](https://huggingface.co/sgugger): [How ðŸ¤— Accelerate runs very large models thanks to PyTorch](https://huggingface.co/blog/accelerate-large-models).\n",
    "\n",
    "3. We can compile model using `torch.compile`, this might give a significant speedup.\n",
    "\n",
    "4. We refine the latents generated by the base model from the same set of prompts and negative prompts using the refiner pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.enable_refinement:\n",
    "    refiner = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n",
    "        config.refiner_checkpoint,\n",
    "        text_encoder_2=pipe.text_encoder_2,\n",
    "        vae=pipe.vae,\n",
    "        torch_dtype=torch.float16,\n",
    "        use_safetensors=True,\n",
    "        variant=\"fp16\",\n",
    "        scheduler=EulerDiscreteScheduler(**config.scheduler_kwargs),\n",
    "    )\n",
    "\n",
    "    if config.enable_cpu_offload_refiner:\n",
    "        refiner.enable_model_cpu_offload()\n",
    "    else:\n",
    "        refiner.to(\"cuda\")\n",
    "    \n",
    "    # Compile model using `torch.compile`, this might give a significant speedup\n",
    "    if config.compile_refinement_model:\n",
    "        refiner.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\n",
    "\n",
    "    generated_image = refiner(\n",
    "        prompt=config.prompt_1,\n",
    "        prompt_2=config.prompt_2,\n",
    "        negative_prompt=config.negative_prompt_1,\n",
    "        negative_prompt_2=config.negative_prompt_2,\n",
    "        guidance_scale=config.refiner_guidance_scale,\n",
    "        image=generated_image,\n",
    "        num_inference_steps=config.num_refinement_steps,\n",
    "        denoising_start=config.high_noise_fraction,\n",
    "        generator=generator,\n",
    "    ).images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging the Images to Weights & Biases\n",
    "\n",
    "Now, we log the images to Weights & Biases. This enables us to:\n",
    "\n",
    "- Visualize our generations\n",
    "- Examine the generated images across different images\n",
    "- Ensure reproducibility of the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a [wandb table](https://docs.wandb.ai/guides/tables)\n",
    "table = wandb.Table(columns=[\n",
    "    \"Prompt-1\",\n",
    "    \"Prompt-2\",\n",
    "    \"Negative-Prompt-1\",\n",
    "    \"Negative-Prompt-2\",\n",
    "    \"Generated-Image\",\n",
    "])\n",
    "\n",
    "generated_image = wandb.Image(generated_image[0])\n",
    "\n",
    "# Add the images to the table\n",
    "table.add_data(\n",
    "    config.prompt_1,\n",
    "    config.prompt_2,\n",
    "    config.negative_prompt_1,\n",
    "    config.negative_prompt_2,\n",
    "    generated_image,\n",
    ")\n",
    "\n",
    "# Log the images and table to wandb\n",
    "wandb.log({\n",
    "    \"Generated-Image\": generated_image,\n",
    "    \"Text-to-Image\": table\n",
    "})\n",
    "\n",
    "# finish the experiment\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how you can examine your generations across multiple experiments ðŸ‘‡\n",
    "\n",
    "![](https://i.imgur.com/zNynGye.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how you can manage your prompts and your generations across experiments ðŸ‘‡\n",
    "\n",
    "![](https://i.imgur.com/JVEXkx0.png)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
