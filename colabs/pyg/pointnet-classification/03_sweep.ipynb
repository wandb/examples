{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15297b92",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pyg/pointnet-classification/03_sweep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "<!--- @wandbcode{pyg-pointnet2-sweep} -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f529ef25",
   "metadata": {},
   "source": [
    "# üî•üî• Run a Hyperparamter Sweep on PointNet++ ü™Ñüêù\n",
    "\n",
    "<!--- @wandbcode{pyg-pointnet2-sweep} -->\n",
    "\n",
    "This notebook demonstrates the process of running a [Hyperparameter Sweep using Weights & Biases](https://docs.wandb.ai/guides/sweeps) on our point cloud classification training workflow in order to maximize the performance of our model.\n",
    "\n",
    "If you wish to know how to implement the PointNet++ architecture and train it you can check out the following [notebook](http://wandb.me/pyg-sampling)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f036a8ae",
   "metadata": {},
   "source": [
    "## Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae1ec82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages.\n",
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b7d66c7",
   "metadata": {},
   "source": [
    "We now install PyTorch Geometric according to our PyTorch Version. We also install Weights & Biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b5b8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q torch-cluster -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
    "!pip install -q wandb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22b0c260",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f71cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from glob import glob\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import ModelNet\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import MLP, PointConv, fps, global_max_pool, radius"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df67a2eb",
   "metadata": {},
   "source": [
    "## Function to Build Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a450c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_and_loaders(sample_points, batch_size, num_workers):\n",
    "    pre_transform = T.NormalizeScale()\n",
    "    transform = T.SamplePoints(sample_points)\n",
    "\n",
    "    train_dataset = ModelNet(\n",
    "        root=\"ModelNet10\",\n",
    "        name='10',\n",
    "        train=True,\n",
    "        transform=transform,\n",
    "        pre_transform=pre_transform\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    val_dataset = ModelNet(\n",
    "        root=\"ModelNet10\",\n",
    "        name='10',\n",
    "        train=False,\n",
    "        transform=transform,\n",
    "        pre_transform=pre_transform\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    return train_dataset, train_loader, val_dataset, val_loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd89a7b6",
   "metadata": {},
   "source": [
    "## Implementing the PointNet++ Model using PyTorch Geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efd3a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetAbstraction(torch.nn.Module):\n",
    "    def __init__(self, ratio, r, nn):\n",
    "        super().__init__()\n",
    "        self.ratio = ratio\n",
    "        self.r = r\n",
    "        self.conv = PointConv(nn, add_self_loops=False)\n",
    "\n",
    "    def forward(self, x, pos, batch):\n",
    "        idx = fps(pos, batch, ratio=self.ratio)\n",
    "        row, col = radius(pos, pos[idx], self.r, batch, batch[idx], max_num_neighbors=64)\n",
    "        edge_index = torch.stack([col, row], dim=0)\n",
    "        x_dst = None if x is None else x[idx]\n",
    "        x = self.conv((x, x_dst), (pos, pos[idx]), edge_index)\n",
    "        pos, batch = pos[idx], batch[idx]\n",
    "        return x, pos, batch\n",
    "\n",
    "\n",
    "class GlobalSetAbstraction(torch.nn.Module):\n",
    "    def __init__(self, nn):\n",
    "        super().__init__()\n",
    "        self.nn = nn\n",
    "\n",
    "    def forward(self, x, pos, batch):\n",
    "        x = self.nn(torch.cat([x, pos], dim=1))\n",
    "        x = global_max_pool(x, batch)\n",
    "        pos = pos.new_zeros((x.size(0), 3))\n",
    "        batch = torch.arange(x.size(0), device=batch.device)\n",
    "        return x, pos, batch\n",
    "\n",
    "\n",
    "class PointNet2(torch.nn.Module):\n",
    "    def __init__(self, set_abstraction_ratio_1, set_abstraction_ratio_2, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        # Input channels account for both `pos` and node features.\n",
    "        self.sa1_module = SetAbstraction(\n",
    "            set_abstraction_ratio_1, 0.2, MLP([3, 64, 64, 128])\n",
    "        )\n",
    "        self.sa2_module = SetAbstraction(\n",
    "            set_abstraction_ratio_2, 0.4, MLP([128 + 3, 128, 128, 256])\n",
    "        )\n",
    "        self.sa3_module = GlobalSetAbstraction(MLP([256 + 3, 256, 512, 1024]))\n",
    "\n",
    "        self.mlp = MLP([1024, 512, 256, 10], dropout=dropout, norm=None)\n",
    "\n",
    "    def forward(self, data):\n",
    "        sa0_out = (data.x, data.pos, data.batch)\n",
    "        sa1_out = self.sa1_module(*sa0_out)\n",
    "        sa2_out = self.sa2_module(*sa1_out)\n",
    "        sa3_out = self.sa3_module(*sa2_out)\n",
    "        x, pos, batch = sa3_out\n",
    "\n",
    "        return self.mlp(x).log_softmax(dim=-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b990617a",
   "metadata": {},
   "source": [
    "## Define a Training Function Instrumented with WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d790cb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    wandb.init(project=\"pyg-point-cloud\", entity=\"geekyrakshit\")\n",
    "    \n",
    "    # Set Default Configs\n",
    "    config = wandb.config\n",
    "    config.categories = sorted([\n",
    "        x.split(os.sep)[-2]\n",
    "        for x in glob(os.path.join(\"ModelNet10\", \"raw\", '*', ''))\n",
    "    ])\n",
    "    config.num_workers = 6\n",
    "    config.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    device = torch.device(config.device)\n",
    "    config.learning_rate = 1e-4\n",
    "    config.epochs = 5\n",
    "    \n",
    "    # Get tuned configs from sweep\n",
    "    batch_size = config.batch_size\n",
    "    sample_points = config.sample_points\n",
    "    set_abstraction_ratio_1 = config.set_abstraction_ratio_1\n",
    "    set_abstraction_ratio_2 = config.set_abstraction_ratio_2\n",
    "    dropout = config.dropout\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    (\n",
    "        train_dataset, train_loader, val_dataset, val_loader\n",
    "    ) = get_dataset_and_loaders(\n",
    "        sample_points, batch_size, config.num_workers\n",
    "    )\n",
    "    \n",
    "    model = PointNet2(\n",
    "        set_abstraction_ratio_1, set_abstraction_ratio_2, dropout\n",
    "    ).to(device)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=config.learning_rate\n",
    "    )\n",
    "    \n",
    "    for epoch in range(1, config.epochs + 1):\n",
    "        \n",
    "        # Training Step\n",
    "        model.train()\n",
    "        epoch_loss, correct = 0, 0\n",
    "        num_train_examples = len(train_loader)\n",
    "\n",
    "        progress_bar = tqdm(\n",
    "            range(num_train_examples),\n",
    "            desc=f\"Training Epoch {epoch}/{config.epochs}\"\n",
    "        )\n",
    "        for batch_idx in progress_bar:\n",
    "            data = next(iter(train_loader)).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(data)\n",
    "            loss = F.nll_loss(prediction, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            correct += prediction.max(1)[1].eq(data.y).sum().item()\n",
    "\n",
    "        epoch_loss = epoch_loss / num_train_examples\n",
    "        epoch_accuracy = correct / len(train_loader.dataset)\n",
    "\n",
    "        wandb.log({\n",
    "            \"Train/Loss\": epoch_loss,\n",
    "            \"Train/Accuracy\": epoch_accuracy\n",
    "        })\n",
    "        \n",
    "        # Validation Step\n",
    "        model.eval()\n",
    "        epoch_loss, correct = 0, 0\n",
    "        num_val_examples = len(val_loader)\n",
    "\n",
    "        progress_bar = tqdm(\n",
    "            range(num_val_examples),\n",
    "            desc=f\"Validation Epoch {epoch}/{config.epochs}\"\n",
    "        )\n",
    "        for batch_idx in progress_bar:\n",
    "            data = next(iter(val_loader)).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                prediction = model(data)\n",
    "\n",
    "            loss = F.nll_loss(prediction, data.y)\n",
    "            epoch_loss += loss.item()\n",
    "            correct += prediction.max(1)[1].eq(data.y).sum().item()\n",
    "\n",
    "        epoch_loss = epoch_loss / num_val_examples\n",
    "        epoch_accuracy = correct / len(val_loader.dataset)\n",
    "\n",
    "        wandb.log({\n",
    "            \"Validation/Loss\": epoch_loss,\n",
    "            \"Validation/Accuracy\": epoch_accuracy\n",
    "        })\n",
    "        \n",
    "        # Save Checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()\n",
    "        }, \"checkpoint.pt\")\n",
    "\n",
    "        artifact_name = wandb.util.make_artifact_name_safe(\n",
    "            f\"{wandb.run.name}-{wandb.run.id}-checkpoint\"\n",
    "        )\n",
    "\n",
    "        checkpoint_artifact = wandb.Artifact(artifact_name, type=\"checkpoint\")\n",
    "        checkpoint_artifact.add_file(\"checkpoint.pt\")\n",
    "        wandb.log_artifact(\n",
    "            checkpoint_artifact, aliases=[\"latest\", f\"epoch-{epoch}\"]\n",
    "        )\n",
    "    \n",
    "    model = model.cpu()\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6671e92",
   "metadata": {},
   "source": [
    "## Start the Hyperparameter Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31702ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sweep configuration\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {'goal': 'maximize', 'name': 'Validation/Accuracy'},\n",
    "    'parameters': \n",
    "    {\n",
    "        'batch_size': {'values': [8, 16, 32, 64]},\n",
    "        'sample_points': {'values': [512, 1024, 2048]},\n",
    "        'set_abstraction_ratio_1': {'min': 0.1, 'max': 0.9},\n",
    "        'set_abstraction_ratio_2': {'min': 0.1, 'max': 0.9},\n",
    "        'dropout': {'min': 0.1, 'max': 0.7},\n",
    "     }\n",
    "}\n",
    "\n",
    "# Get Sweep ID\n",
    "sweep_id = wandb.sweep(\n",
    "    sweep=sweep_configuration, project='pyg-point-cloud', entity=\"geekyrakshit\"\n",
    ")\n",
    "\n",
    "# Run Sweep\n",
    "wandb.agent(sweep_id, function=train, count=30)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
