{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pyg/pointnet-classification/02_pointnet_plus_plus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "<!--- @wandbcode{pyg-pointnet2-train} -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a089e90",
   "metadata": {},
   "source": [
    "# üî•üî• Train PointNet++ Model using PyTorch Geometric and Weights & Biases ü™Ñüêù\n",
    "\n",
    "<!--- @wandbcode{pyg-pointnet2-train} -->\n",
    "\n",
    "This notebook demonstrates an implementation of the [PointeNet++](https://arxiv.org/pdf/1706.02413.pdf) architecture implemented using PyTorch Geometric and experiment tracked and visualized using [Weights & Biases](https://wandb.ai/site).\n",
    "\n",
    "If you wish to know how to compare and visualize the different sampling strategies used in the PointNet++ implementation, you can check out the following notebook:\n",
    "\n",
    "[![](https://colab.research.google.com/assets/colab-badge.svg)](http://wandb.me/pyg-pointnet2-train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "74844944",
   "metadata": {},
   "source": [
    "## Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316dcba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages.\n",
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4380ba4f",
   "metadata": {},
   "source": [
    "We now install PyTorch Geometric according to our PyTorch Version. We also install Weights & Biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fda5066",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q torch-cluster -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
    "!pip install -q wandb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6616543",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc13bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from glob import glob\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import ModelNet\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import MLP, PointConv, fps, global_max_pool, radius"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46e5b154",
   "metadata": {},
   "source": [
    "## Initialize Weights & Biases\n",
    "\n",
    "We need to call [`wandb.init()`](https://docs.wandb.ai/ref/python/init) once at the beginning of our program to initialize a new job. This creates a new run in W&B and launches a background process to sync data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783f6a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_project = \"pyg-point-cloud\" #@param {\"type\": \"string\"}\n",
    "wandb_run_name = \"final-experiment/modelnet10/2\" #@param {\"type\": \"string\"}\n",
    "\n",
    "wandb.init(project=wandb_project, name=wandb_run_name, job_type=\"baseline-train\")\n",
    "\n",
    "# Set experiment configs to be synced with wandb\n",
    "config = wandb.config\n",
    "config.modelnet_dataset_alias = \"ModelNet10\" #@param [\"ModelNet10\", \"ModelNet40\"] {type:\"raw\"}\n",
    "\n",
    "config.seed = 4242 #@param {type:\"number\"}\n",
    "random.seed(config.seed)\n",
    "torch.manual_seed(config.seed)\n",
    "\n",
    "config.sample_points = 2048 #@param {type:\"slider\", min:256, max:4096, step:16}\n",
    "\n",
    "config.categories = sorted([\n",
    "    x.split(os.sep)[-2]\n",
    "    for x in glob(os.path.join(\n",
    "        config.modelnet_dataset_alias, \"raw\", '*', ''\n",
    "    ))\n",
    "])\n",
    "\n",
    "config.batch_size = 16 #@param {type:\"slider\", min:4, max:128, step:4}\n",
    "config.num_workers = 6 #@param {type:\"slider\", min:1, max:10, step:1}\n",
    "\n",
    "config.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(config.device)\n",
    "\n",
    "config.set_abstraction_ratio_1 = 0.748 #@param {type:\"slider\", min:0.1, max:1.0, step:0.01}\n",
    "config.set_abstraction_radius_1 = 0.4817 #@param {type:\"slider\", min:0.1, max:1.0, step:0.01}\n",
    "config.set_abstraction_ratio_2 = 0.3316 #@param {type:\"slider\", min:0.1, max:1.0, step:0.01}\n",
    "config.set_abstraction_radius_2 = 0.2447 #@param {type:\"slider\", min:0.1, max:1.0, step:0.01}\n",
    "config.dropout = 0.1 #@param {type:\"slider\", min:0.1, max:1.0, step:0.1}\n",
    "\n",
    "config.learning_rate = 1e-4 #@param {type:\"number\"}\n",
    "config.epochs = 10 #@param {type:\"slider\", min:1, max:100, step:1}\n",
    "config.num_visualization_samples = 20 #@param {type:\"slider\", min:1, max:100, step:1}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "524732b9",
   "metadata": {},
   "source": [
    "## Load ModelNet Dataset using PyTorch Geometric\n",
    "\n",
    "We now load, preprocess and batch the ModelNet dataset for training, validation/testing and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a74c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_transform = T.NormalizeScale()\n",
    "transform = T.SamplePoints(config.sample_points)\n",
    "\n",
    "\n",
    "train_dataset = ModelNet(\n",
    "    root=config.modelnet_dataset_alias,\n",
    "    name=config.modelnet_dataset_alias[-2:],\n",
    "    train=True,\n",
    "    transform=transform,\n",
    "    pre_transform=pre_transform\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=config.num_workers\n",
    ")\n",
    "\n",
    "val_dataset = ModelNet(\n",
    "    root=config.modelnet_dataset_alias,\n",
    "    name=config.modelnet_dataset_alias[-2:],\n",
    "    train=False,\n",
    "    transform=transform,\n",
    "    pre_transform=pre_transform\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=config.num_workers\n",
    ")\n",
    "\n",
    "random_indices = random.sample(\n",
    "    list(range(len(val_dataset))),\n",
    "    config.num_visualization_samples\n",
    ")\n",
    "vizualization_loader = DataLoader(\n",
    "    [val_dataset[idx] for idx in random_indices],\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=config.num_workers\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4960beac",
   "metadata": {},
   "source": [
    "## Implementing the PointNet++ Model using PyTorch Geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4fe7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetAbstraction(torch.nn.Module):\n",
    "    def __init__(self, ratio, r, nn):\n",
    "        super().__init__()\n",
    "        self.ratio = ratio\n",
    "        self.r = r\n",
    "        self.conv = PointConv(nn, add_self_loops=False)\n",
    "\n",
    "    def forward(self, x, pos, batch):\n",
    "        idx = fps(pos, batch, ratio=self.ratio)\n",
    "        row, col = radius(pos, pos[idx], self.r, batch, batch[idx],\n",
    "                          max_num_neighbors=64)\n",
    "        edge_index = torch.stack([col, row], dim=0)\n",
    "        x_dst = None if x is None else x[idx]\n",
    "        x = self.conv((x, x_dst), (pos, pos[idx]), edge_index)\n",
    "        pos, batch = pos[idx], batch[idx]\n",
    "        return x, pos, batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e266ddad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalSetAbstraction(torch.nn.Module):\n",
    "    def __init__(self, nn):\n",
    "        super().__init__()\n",
    "        self.nn = nn\n",
    "\n",
    "    def forward(self, x, pos, batch):\n",
    "        x = self.nn(torch.cat([x, pos], dim=1))\n",
    "        x = global_max_pool(x, batch)\n",
    "        pos = pos.new_zeros((x.size(0), 3))\n",
    "        batch = torch.arange(x.size(0), device=batch.device)\n",
    "        return x, pos, batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e838fc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNet2(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        set_abstraction_ratio_1, set_abstraction_ratio_2,\n",
    "        set_abstraction_radius_1, set_abstraction_radius_2, dropout\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Input channels account for both `pos` and node features.\n",
    "        self.sa1_module = SetAbstraction(\n",
    "            set_abstraction_ratio_1,\n",
    "            set_abstraction_radius_1,\n",
    "            MLP([3, 64, 64, 128])\n",
    "        )\n",
    "        self.sa2_module = SetAbstraction(\n",
    "            set_abstraction_ratio_2,\n",
    "            set_abstraction_radius_2,\n",
    "            MLP([128 + 3, 128, 128, 256])\n",
    "        )\n",
    "        self.sa3_module = GlobalSetAbstraction(MLP([256 + 3, 256, 512, 1024]))\n",
    "\n",
    "        self.mlp = MLP([1024, 512, 256, 10], dropout=dropout, norm=None)\n",
    "\n",
    "    def forward(self, data):\n",
    "        sa0_out = (data.x, data.pos, data.batch)\n",
    "        sa1_out = self.sa1_module(*sa0_out)\n",
    "        sa2_out = self.sa2_module(*sa1_out)\n",
    "        sa3_out = self.sa3_module(*sa2_out)\n",
    "        x, pos, batch = sa3_out\n",
    "\n",
    "        return self.mlp(x).log_softmax(dim=-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "973729a2",
   "metadata": {},
   "source": [
    "## Training PointNet++ and Logging Metrics on Weights & Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c524e338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PointNet++ model.\n",
    "model = PointNet2(\n",
    "    config.set_abstraction_ratio_1,\n",
    "    config.set_abstraction_ratio_2,\n",
    "    config.set_abstraction_radius_1,\n",
    "    config.set_abstraction_radius_2,\n",
    "    config.dropout\n",
    ").to(device)\n",
    "\n",
    "# Define Optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=config.learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b753e2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(epoch):\n",
    "    \"\"\"Training Step\"\"\"\n",
    "    model.train()\n",
    "    epoch_loss, correct = 0, 0\n",
    "    num_train_examples = len(train_loader)\n",
    "    \n",
    "    progress_bar = tqdm(\n",
    "        range(num_train_examples),\n",
    "        desc=f\"Training Epoch {epoch}/{config.epochs}\"\n",
    "    )\n",
    "    data_iter = iter(train_loader)\n",
    "    for batch_idx in progress_bar:\n",
    "        data = next(data_iter).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(data)\n",
    "        loss = F.nll_loss(prediction, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        correct += prediction.max(1)[1].eq(data.y).sum().item()\n",
    "    \n",
    "    epoch_loss = epoch_loss / num_train_examples\n",
    "    epoch_accuracy = correct / len(train_loader.dataset)\n",
    "    \n",
    "    wandb.log({\n",
    "        \"Train/Loss\": epoch_loss,\n",
    "        \"Train/Accuracy\": epoch_accuracy\n",
    "    })\n",
    "\n",
    "\n",
    "def val_step(epoch):\n",
    "    \"\"\"Validation Step\"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss, correct = 0, 0\n",
    "    num_val_examples = len(val_loader)\n",
    "    \n",
    "    progress_bar = tqdm(\n",
    "        range(num_val_examples),\n",
    "        desc=f\"Validation Epoch {epoch}/{config.epochs}\"\n",
    "    )\n",
    "    data_iter = iter(val_loader)\n",
    "    for batch_idx in progress_bar:\n",
    "        data = next(data_iter).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            prediction = model(data)\n",
    "        \n",
    "        loss = F.nll_loss(prediction, data.y)\n",
    "        epoch_loss += loss.item()\n",
    "        correct += prediction.max(1)[1].eq(data.y).sum().item()\n",
    "    \n",
    "    epoch_loss = epoch_loss / num_val_examples\n",
    "    epoch_accuracy = correct / len(val_loader.dataset)\n",
    "    \n",
    "    wandb.log({\n",
    "        \"Validation/Loss\": epoch_loss,\n",
    "        \"Validation/Accuracy\": epoch_accuracy\n",
    "    })\n",
    "\n",
    "\n",
    "def visualize_evaluation(table, epoch):\n",
    "    \"\"\"Visualize validation result in a Weights & Biases Table\"\"\"\n",
    "    point_clouds, losses, predictions, ground_truths, is_correct = [], [], [], [], []\n",
    "    progress_bar = tqdm(\n",
    "        range(config.num_visualization_samples),\n",
    "        desc=f\"Generating Visualizations for Epoch {epoch}/{config.epochs}\"\n",
    "    )\n",
    "    \n",
    "    for idx in progress_bar:\n",
    "        data = next(iter(vizualization_loader)).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            prediction = model(data)\n",
    "        \n",
    "        point_clouds.append(\n",
    "            wandb.Object3D(torch.squeeze(data.pos, dim=0).cpu().numpy())\n",
    "        )\n",
    "        losses.append(F.nll_loss(prediction, data.y).item())\n",
    "        predictions.append(config.categories[int(prediction.max(1)[1].item())])\n",
    "        ground_truths.append(config.categories[int(data.y.item())])\n",
    "        is_correct.append(prediction.max(1)[1].eq(data.y).sum().item())\n",
    "    \n",
    "    table.add_data(\n",
    "        epoch, point_clouds, losses, predictions, ground_truths, is_correct\n",
    "    )\n",
    "    return table\n",
    "\n",
    "\n",
    "def save_checkpoint(epoch):\n",
    "    \"\"\"Save model checkpoints as Weights & Biases artifacts\"\"\"\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, \"checkpoint.pt\")\n",
    "    \n",
    "    artifact_name = wandb.util.make_artifact_name_safe(\n",
    "        f\"{wandb.run.name}-{wandb.run.id}-checkpoint\"\n",
    "    )\n",
    "    \n",
    "    checkpoint_artifact = wandb.Artifact(artifact_name, type=\"checkpoint\")\n",
    "    checkpoint_artifact.add_file(\"checkpoint.pt\")\n",
    "    wandb.log_artifact(\n",
    "        checkpoint_artifact, aliases=[\"latest\", f\"epoch-{epoch}\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b37a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = wandb.Table(\n",
    "    columns=[\n",
    "        \"Epoch\",\n",
    "        \"Point-Clouds\",\n",
    "        \"Losses\",\n",
    "        \"Predicted-Classes\",\n",
    "        \"Ground-Truth\",\n",
    "        \"Is-Correct\"\n",
    "    ]\n",
    ")\n",
    "for epoch in range(1, config.epochs + 1):\n",
    "    train_step(epoch)\n",
    "    val_step(epoch)\n",
    "    visualize_evaluation(table, epoch)\n",
    "    save_checkpoint(epoch)\n",
    "wandb.log({\"Evaluation\": table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bbb06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e236382d",
   "metadata": {},
   "source": [
    "Next, you can check out the following notebook to learn how to run a hyperparameter sweep on our PointNet++ trainig loop using Weights & Biases:\n",
    "\n",
    "|Tune Hyperparameters using Weights & Biases Sweep|[![](https://colab.research.google.com/assets/colab-badge.svg)](http://wandb.me/pyg-pointnet2-sweep)|"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
