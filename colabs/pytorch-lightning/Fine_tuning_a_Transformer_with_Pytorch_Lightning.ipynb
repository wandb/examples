{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
    "\n",
    "<!--- @wandbcode{lightning_hf} -->\n",
    "\n",
    "# Train a Model to Check Your Grammar Using W&B, PyTorch Lightning ‚ö°, and ü§ó\n",
    "\n",
    "\n",
    "*Based on Ayush Chaurasia's awesome [W&B report](https://wandb.ai/cayush/bert-finetuning/reports/Sentence-Classification-With-Huggingface-BERT-and-W-B--Vmlldzo4MDMwNA) and [colab](https://colab.research.google.com/drive/1SQ-FOgji8AiyrQ08sIVfDiA8OUw4bC12?usp=sharing) which performs the same task using BERT, vanilla PyTorch, and W&B.*\n",
    "\n",
    "<img src=\"https://wandb.me/mini-diagram\" width=\"650\" alt=\"Weights & Biases\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch-lightning/Fine_tuning_a_Transformer_with_Pytorch_Lightning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are going to train a model to detect ungrammatical sentences from the CoLA dataset. To perform the classification, we will be using Pytorch Lightning ‚ö° to fine tune [DistilBERT](https://arxiv.org/abs/1910.01108), a transformer model from huggingface ü§ó.\n",
    "\n",
    "We'll use Weights & Biases to:\n",
    "- Version our model inputs and outputs using [W&B Artifacts](https://docs.wandb.ai/guides/artifacts), including preprocessing steps, train/validation splits, and model checkpoints\n",
    "- Log and visualize training and validation performance using [W&B's Pytorch Lightning integration](https://docs.wandb.ai/guides/integrations/lightning)\n",
    "- Visualize and explore the raw dataset using [W&B Tables](https://docs.wandb.ai/guides/data-vis)\n",
    "- Orchestrate a hyperparameter search using [W&B Sweeps](https://docs.wandb.ai/guides/sweeps)\n",
    "\n",
    "Be sure to follow the links that each run outputs to your W&B workspace, where you will be able to see...\n",
    "\n",
    "**Your model's performance metrics updating in real time**\n",
    "\n",
    "![](https://i.imgur.com/8yejscO.png)\n",
    "\n",
    "**The raw data as a W&B Table, which you can sort, group, and filter**\n",
    "\n",
    "![](https://imgur.com/oiQ8RE4.png)\n",
    "\n",
    "**An awesome artifact graph showing our full pipeline**\n",
    "\n",
    "![](https://imgur.com/vMJqKw7.png)\n",
    "\n",
    "**Interactive visualizations of how our hyperparameter choices effect model performance**\n",
    "\n",
    "![](https://imgur.com/Twq7V6c.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install some dependencies\n",
    "!pip install pandas torch pytorch-lightning transformers==4.1.1 -q\n",
    "!pip install --upgrade wandb -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bulk import cell\n",
    "import wandb\n",
    "import random\n",
    "import torch\n",
    "import transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derandomizing cell\n",
    "pl.seed_everything(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Note that if you are using W&B local you will need to pass the url of your W&B \n",
    "deployment to wandb.login through the host keyword argument.\n",
    "\n",
    "For example:\n",
    "wandb.login(host=\"api.wandb.ai\")\n",
    "\"\"\"\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = \"grammar-checker\"  # W&B project name here\n",
    "entity = None  # your W&B username or teamname here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The CoLA Dataset ü•§\n",
    "\n",
    "We‚Äôll fine tune the model on The Corpus of Linguistic Acceptability (CoLA) dataset for single sentence classification. It‚Äôs a set of sentences labeled as grammatically correct or incorrect. It was first published in May of 2018, and is one of the tests included in the ‚ÄúGLUE Benchmark‚Äù on which models like DistilBERT are competing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a [reference artifact](https://docs.wandb.ai/guides/artifacts/references) to store a pointer to the source data. The advantages of doing this are:\n",
    "* Any runs that use this artifact reference will be able to trace their lineage back to the true source\n",
    "* We can use W&B to download the raw data in our code.\n",
    "\n",
    "The cell below starts a run with job type `register-data`. In the context of this run, we:\n",
    " 1. Create an artifact called `cola-raw`\n",
    " 2. Add a reference to the CoLA dataset to our `cola-raw` artifact\n",
    " 3. Log the `cola-raw` artifact to Weights & Biases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the context of a W&B Run object, referenceable with the 'run' variable\n",
    "with wandb.init(entity=entity, project=project, job_type=\"register-data\") as run:\n",
    "\n",
    "  # Construct a wandb.Artifact object\n",
    "  data_source = wandb.Artifact(\"cola-raw\", type=\"dataset\")\n",
    "\n",
    "  # Store a reference to the download URL of the CoLA dataset\n",
    "  data_source.add_reference(\"https://nyu-mll.github.io/CoLA/cola_public_1.1.zip\", name=\"zipfile\")\n",
    "  \n",
    "  # Log the artifact to W&B\n",
    "  run.log_artifact(data_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization ü™ô\n",
    "\n",
    "The cell below defines the function `tokenize_data`, which transforms a list of sentences and a list of labels into a tuple of `torch.tensor` objects which can be consumed by the transormer model we'll be using. The 3 tensors returned are the tokenized form of the sentences, the attention masks indicating which tokens in each sentence correspond to actual words, and a tensor containing the original labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(sentences, labels):\n",
    "\n",
    "  # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "  input_ids = []\n",
    "  attention_masks = []\n",
    "\n",
    "  # Get BertTokenizer from transformers\n",
    "  tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "  # For every sentence...\n",
    "  for sent in sentences:\n",
    "    \n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                      sent,                      # Sentence to encode.\n",
    "                      add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                      max_length = 64,           # Pad & truncate all sentences.\n",
    "                      pad_to_max_length = True,\n",
    "                      return_attention_mask = True,   # Construct attn. masks.\n",
    "                      return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "  # Convert the lists into tensors.\n",
    "  input_ids = torch.cat(input_ids, dim=0)\n",
    "  attention_masks = torch.cat(attention_masks, dim=0)\n",
    "  labels = torch.tensor(labels)\n",
    "  return input_ids, attention_masks, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below executes a run of type `preprocess-data`, which will\n",
    "1. Download the CoLA dataset using the reference artifact we logged previously\n",
    "2. Log the entire dataset to W&B as a Table\n",
    "3. Use the function `tokenize_data` to transform each sentence into a sequence of tokens and an attention mask\n",
    "4. Log the preprocessed data as an artifact to W&B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init(entity=entity, project=project, job_type=\"preprocess-data\") as run:\n",
    "  \n",
    "  # Download the raw cola data from the 'zipfile' reference we added to the cola-raw artifact.\n",
    "  raw_data_artifact = run.use_artifact(\"cola-raw:latest\")\n",
    "  zip_path = raw_data_artifact.get_path(\"zipfile\").download()\n",
    "  !unzip -o $zip_path  # jupyter hack to unzip data :P\n",
    "  \n",
    "  # Read in the raw data, log it to W&B as a wandb.Table\n",
    "  df = pd.read_csv(\n",
    "    \"./cola_public/raw/in_domain_train.tsv\", \n",
    "    delimiter='\\t', \n",
    "    header=None, \n",
    "    names=['sentence_source', 'label', 'label_notes', 'sentence']\n",
    "  )\n",
    "  run.log({\"raw-data\": wandb.Table(dataframe=df)})\n",
    "  \n",
    "  # Perform tokenization and store as a TensorDataset\n",
    "  input_ids, attention_masks, labels = tokenize_data(df.sentence.values, df.label.values)\n",
    "  preprocessed_data = torch.utils.data.TensorDataset(input_ids, attention_masks, labels)\n",
    "  \n",
    "  # 1. Create an artifact called preprocessed-data\n",
    "  # 2. Save the dataset to a local fil called preprocessed-data.pt\n",
    "  # 3. Add that file to the preprocessed-data artifact\n",
    "  # 4. Log the artifact to W&B\n",
    "  data_artifact = wandb.Artifact(\"preprocessed-data\", type=\"dataset\")\n",
    "  with open(\"preprocessed-data.pt\", \"wb\") as f:\n",
    "    torch.save(preprocessed_data, f)\n",
    "  data_artifact.add_file(\"preprocessed-data.pt\", name=\"dataset\")\n",
    "  run.log_artifact(data_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Our Data ü™ì\n",
    "\n",
    "For our training process, we want to split the data into a train and validation set. The train set is the data we will use to update the model parameters, while the validation set will be a smaller segment of data that we use to test whether our model is generalizing to examples that it hasn't been trained on.\n",
    "\n",
    "The cell below executes a `wandb.Run` with `job_type=\"split-data\"`. In the context of this run we will:\n",
    "\n",
    "1. Download the `preprocessed-data` artifact logged by our previous run\n",
    "2. Use the `random_split` function from `torch` to perform a randomn 90/10 test/valiation split on the preprocessed data\n",
    "3. Store the split datasets in a new artifact called `split-dataset`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init(entity=entity, project=project, job_type=\"split-data\") as run:\n",
    "\n",
    "  # Download the preprocessed data\n",
    "  pp_data_artifact = run.use_artifact(\"preprocessed-data:latest\")\n",
    "  data_path = pp_data_artifact.get_path(\"dataset\").download()\n",
    "  dataset = torch.load(data_path)\n",
    "\n",
    "  # Calculate the number of samples to include in each set.\n",
    "  train_size = int(0.9 * len(dataset))\n",
    "  val_size = len(dataset) - train_size\n",
    "\n",
    "  # Divide the dataset by randomly selecting samples.\n",
    "  train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "  # Construct a new artifact\n",
    "  split_data_artifact = wandb.Artifact(\"split-dataset\", type=\"dataset\")\n",
    "  \n",
    "  # Save the dataset splits to disk\n",
    "  torch.save(train_dataset, \"train.pt\")\n",
    "  torch.save(val_dataset, \"validation.pt\")\n",
    "  \n",
    "  # Add the data splits to the artifact\n",
    "  split_data_artifact.add_file(\"train.pt\", name=\"train-data\")\n",
    "  split_data_artifact.add_file(\"validation.pt\", name=\"validation-data\")\n",
    "  \n",
    "  # Log the artifact to W&B\n",
    "  run.log_artifact(split_data_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Our Model ‚ö°\n",
    "\n",
    "We define our model and the associated training + validation procedures in the `LightningModule` below. The model itself is a pre-trained `DistilBertForSequenceClassification` with two labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceClassifier(pl.LightningModule):\n",
    "  \n",
    "  def __init__(self, learning_rate=5e-5):\n",
    "    super(SentenceClassifier, self).__init__()\n",
    "    \n",
    "    # Load pretrained distilbert-base-uncased configured for classification with 2 labels\n",
    "    self.model = transformers.DistilBertForSequenceClassification.from_pretrained(\n",
    "      \"distilbert-base-uncased\", \n",
    "      num_labels = 2, \n",
    "      output_attentions = False, # Whether the model returns attentions weights.\n",
    "      output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "    )\n",
    "    self.learning_rate = learning_rate\n",
    "\n",
    "  def training_step(self, batch, batch_no):\n",
    "    \"\"\"\n",
    "    This function overrides the pl.LightningModule class. \n",
    "    \n",
    "    When trainer.fit is called, each batch from the provided data loader is fed \n",
    "    to this function successively. \n",
    "    \"\"\"\n",
    "    ids, masks, labels = batch\n",
    "    outputs = self.model(ids, attention_mask=masks, labels=labels)\n",
    "    preds = torch.argmax(outputs[\"logits\"], axis=1)\n",
    "    correct = sum(preds.flatten() == labels.flatten())\n",
    "    self.log(\"train/loss\", outputs[\"loss\"], on_step=True, on_epoch=True)\n",
    "    self.log(\"train/acc\", correct/len(ids), on_step=True, on_epoch=True)\n",
    "    return outputs[\"loss\"]\n",
    "\n",
    "  def validation_step(self, batch, batch_no):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    ids, masks, labels = batch\n",
    "    outputs = self.model(ids, attention_mask=masks, labels=labels)\n",
    "    preds = torch.argmax(outputs[\"logits\"], axis=1)\n",
    "    correct = sum(preds.flatten() == labels.flatten())\n",
    "    self.log(\"val/loss\", outputs[\"loss\"], on_step=False, on_epoch=True)\n",
    "    self.log(\"val/acc\", correct/len(ids), on_step=False, on_epoch=True)\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    \"\"\"\n",
    "    This is overriding a LightningModule method that is called to return the\n",
    "    optimizer used for training.\n",
    "    \"\"\"\n",
    "    return transformers.AdamW(\n",
    "        self.model.parameters(),\n",
    "        lr = self.learning_rate, \n",
    "        eps = 1e-8 \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Tracking Our Model üìâ\n",
    "\n",
    "In the cell below, we define a function `train` which sets up and performs training in the context of a W&B run. The train function takes a configuration dictionary as input then passes it to `wandb.init` via the `config` keyword argument. We use the values saved in the `wandb.config` object associated with the run to set the parameters of our trainer and data loaders. This is a crucial best practice to ensure that the values logged in the `config` object (and displayed in the run table of the W&B app) represent the actual parameters of the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config={\"learning_rate\": 5e-5, \"batch_size\": 16, \"epochs\": 2}):\n",
    "  \n",
    "  with wandb.init(project=project, entity=entity, job_type=\"train\", config=config) as run:  \n",
    "\n",
    "    # Load the datasets from the split-dataset artifact\n",
    "    data = run.use_artifact(\"split-dataset:latest\")\n",
    "    train_dataset = torch.load(data.get_path(\"train-data\").download())\n",
    "    val_dataset = torch.load(data.get_path(\"validation-data\").download())\n",
    "\n",
    "    # Extract the config object associated with the run\n",
    "    config = run.config\n",
    "    \n",
    "    # Construct our LightningModule with the learning rate from the config object\n",
    "    model = SentenceClassifier(learning_rate=config.learning_rate)\n",
    "\n",
    "    # This logger is used when we call self.log inside the LightningModule\n",
    "    logger = pl.loggers.WandbLogger(experiment=run, log_model=True)\n",
    "    \n",
    "    # Use as many GPUs as are available\n",
    "    gpus = -1 if torch.cuda.is_available() else 0\n",
    "    \n",
    "    # Construct a Trainer object with the W&B logger we created and epoch set by the config object\n",
    "    trainer = pl.Trainer(max_epochs=config.epochs, gpus=gpus, logger=logger)\n",
    "    \n",
    "    # Build data loaders for our datasets, using the batch_size from our config object\n",
    "    train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=config.batch_size)\n",
    "    val_data_loader = torch.utils.data.DataLoader(val_dataset, batch_size=config.batch_size)\n",
    "    \n",
    "    # Execute training\n",
    "    trainer.fit(model, train_data_loader, val_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()  # Run training with default parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a Hyperparameter Sweep üßπ\n",
    "\n",
    "W&B sweeps allow you to optimize your model hyperparameters with minimal effort. In general, the workflow of sweeps is:\n",
    "1. Construct a dictionary or YAML file that defines the hyperparameter space \n",
    "2. Call `wandb.sweep(<sweep-dict>)` from the python library or `wandb sweep <yaml-file>` from the command line to initialize the sweep in W&B\n",
    "3. Run `wandb.agent(<sweep-id>)` (python lib) or `wandb agent <sweep-id>` (cli) to start a sweep agent to continuously:\n",
    "  - pull hyperparameter combinations from W&B\n",
    "  - run training with the given hyperparameters \n",
    "  - log training metrics back to W&B\n",
    "\n",
    "<img src=\"https://i.imgur.com/zlbw3vQ.png\" alt=\"sweeps-diagram\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement the sweeps workflow laid out above by:\n",
    "1. Creating a `sweep_config` dictionary describing our hyperparameter space and objective\n",
    "  - The hyperparameters we will sweep over are `learning_rate`, `batch_size`, and `epochs`\n",
    "  - Our objective in this sweep is to maximize the `validation/epoch_acc` metric logged to W&B\n",
    "  - We will use the `random` strategy, which means we will sample uniformly from the parameter space indefinitely\n",
    "2. Calling `wandb.sweep(sweep_config)` to create the sweep in our W&B project\n",
    "  - `wandb.sweep` will return a unique id for the sweep, saved as `sweep_id`\n",
    "3. Calling `wandb.agent(sweep_id, function=train)` to create an agent that will execute training with different hyperparameter combinations\n",
    "  - The agent will repeatedly query W&B for hyperparameter combinations\n",
    "  - When `wandb.init` is called within an agent, the `config` dictionary of the returned `run` will be populated with the next hyperparameter combination in the sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random',  # Randomly sample the hyperparameter space (alternatives: grid, bayes)\n",
    "    'metric': {  # This is the metric we are interested in maximizing\n",
    "      'name': 'validation/epoch_acc',\n",
    "      'goal': 'maximize'   \n",
    "    },\n",
    "    # Paramters and parameter values we are sweeping across\n",
    "    'parameters': {\n",
    "        'learning_rate': {\n",
    "            'values': [5e-5, 3e-5, 2e-5]\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [16, 32]\n",
    "        },\n",
    "        'epochs':{\n",
    "            'values':[2, 3, 4]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=project, entity=entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run an agent üïµÔ∏è to try out 20 hyperparameter combinations\n",
    "wandb.agent(sweep_id, function=train, count=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('wandb': pyenv)",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
