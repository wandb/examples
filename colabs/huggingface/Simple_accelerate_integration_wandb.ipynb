{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "205e453e",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/huggingface/Simple_accelerate_integration_wandb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "<!--- @wandbcode{simple-accelerate} -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe7c4702",
   "metadata": {},
   "source": [
    "# Using Huggingface Accelerate with Weights and Biases\n",
    "<!--- @wandbcode{simple-accelerate} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b2cd3c",
   "metadata": {},
   "source": [
    "[Accelerate](https://github.com/huggingface/accelerate) is this amazing little framework that simplifies your PyTorch training scripts enabling you to train with all the tricks out there!\n",
    "- Quickly convert your code to support multiple hardward (GPUS, TPUs, Metal,...)\n",
    "- One code to support mixed precision, bfloat16 and even 8 bit Adam.\n",
    "\n",
    "Minimal code and no boilerplate. Weights and Biases integration out of the box!\n",
    "\n",
    "```diff\n",
    "  import torch\n",
    "  import torch.nn.functional as F\n",
    "  from datasets import load_dataset\n",
    "+ from accelerate import Accelerator\n",
    "\n",
    "+ accelerator = Accelerator(log_with=\"wandb\")\n",
    "+ accelerator.init_trackers(\"my_wandb_project\", config=cfg)\n",
    "- device = 'cpu'\n",
    "+ device = accelerator.device\n",
    "\n",
    "  model = torch.nn.Transformer().to(device)\n",
    "  optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "  dataset = load_dataset('my_dataset')\n",
    "  data = torch.utils.data.DataLoader(dataset, shuffle=True)\n",
    "\n",
    "+ model, optimizer, data = accelerator.prepare(model, optimizer, data)\n",
    "\n",
    "  model.train()\n",
    "  for epoch in range(10):\n",
    "      for source, targets in data:\n",
    "          source = source.to(device)\n",
    "          targets = targets.to(device)\n",
    "\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          output = model(source)\n",
    "          loss = F.cross_entropy(output, targets)\n",
    "\n",
    "-         loss.backward()\n",
    "+         accelerator.backward(loss)\n",
    "\n",
    "          optimizer.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179d31c1",
   "metadata": {},
   "source": [
    "## Training and Image Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a080fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate wandb torcheval timm fastprogress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62d276f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torchvision.datasets import FashionMNIST\n",
    "import torchvision.transforms as T\n",
    "from torcheval.metrics.toolkit import sync_and_compute\n",
    "from fastprogress import progress_bar\n",
    "\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2923f8",
   "metadata": {},
   "source": [
    "Store your configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e5118b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = SimpleNamespace(\n",
    "    path=\".\",\n",
    "    bs=256,\n",
    "    epochs=5,\n",
    "    size=28,\n",
    "    num_workers=8,\n",
    ")\n",
    "\n",
    "WANDB_PROJECT = \"accelerate_fmnist\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e038818",
   "metadata": {},
   "source": [
    "setup transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8424eb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = T.Compose([\n",
    "    T.RandomCrop(cfg.size, padding=1),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a359aa",
   "metadata": {},
   "source": [
    "Create a simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e903bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(in_ch, out_ch, ks=3): return nn.Sequential(nn.BatchNorm2d(in_ch),\n",
    "                                                    nn.Conv2d(in_ch, out_ch, ks, stride=2, padding=0), \n",
    "                                                    nn.ReLU())\n",
    "\n",
    "def create_cnn():\n",
    "    return nn.Sequential(nn.Conv2d(1, 16, 5, stride=1, padding=\"same\"),\n",
    "                         conv_block(16, 32),\n",
    "                         conv_block(32, 64),\n",
    "                         conv_block(64, 128),\n",
    "                         conv_block(128, 256, 1),\n",
    "                         nn.Sequential(nn.Flatten(), nn.Linear(256,10), nn.BatchNorm1d(10)),\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07a7cf2",
   "metadata": {},
   "source": [
    "Wrap everything into a training functions (this is necessary to run on multiple GPUS, if it is only one, you can skip the wrapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78346075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cfg):\n",
    "\n",
    "    # data\n",
    "    ds = FashionMNIST(cfg.path, transform=tfms, download=True) \n",
    "    dl = DataLoader(ds, batch_size=cfg.bs, num_workers=cfg.num_workers)\n",
    "    \n",
    "    # model\n",
    "    model = create_cnn()\n",
    "    \n",
    "    # training setup\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    \n",
    "    # accelerate\n",
    "    accelerator = Accelerator(log_with=\"wandb\")\n",
    "    \n",
    "    # this will call wandb.init(...)\n",
    "    accelerator.init_trackers(WANDB_PROJECT, config=cfg)\n",
    "    \n",
    "    # prepare\n",
    "    model, optimizer, dl = accelerator.prepare(model, optimizer, dl)\n",
    "    \n",
    "    # train\n",
    "    model.train()\n",
    "    for epoch in progress_bar(range(cfg.epochs)):\n",
    "        accurate, num_elems = 0., 0\n",
    "        for source, targets in dl:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(source)\n",
    "            loss = F.cross_entropy(output, targets)\n",
    "            accelerator.backward(loss)\n",
    "            \n",
    "            # under the hood this calls wandb.log(...) on the main process\n",
    "            accelerator.log({\"train_loss\": loss})\n",
    "            \n",
    "            accurate_preds = output.argmax(dim=1) == targets\n",
    "            num_elems += accurate_preds.shape[0]\n",
    "            accurate += accurate_preds.long().sum()\n",
    "            optimizer.step()\n",
    "        accuracy = accurate.item() / num_elems\n",
    "        accelerator.log({\"epoch\":epoch, \"accuracy\":accuracy}, log_kwargs={\"wandb\": {\"commit\": False}})\n",
    "        print(f\"epoch: {epoch:3} || loss: {loss:5.3f} || accuracy: {accuracy:5.3f}\")\n",
    "    \n",
    "    # this will call wandb.finish()\n",
    "    accelerator.end_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e162db1",
   "metadata": {},
   "source": [
    "Let's train on 2 GPUs! This is really nice, as accelerate will take care of only calling `log` on the main process, so only one run get's created, so no need to manually check the rank of the process when using multiple GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027b2d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_GPUSs = 2\n",
    "\n",
    "from accelerate import notebook_launcher\n",
    "\n",
    "notebook_launcher(train, (cfg,), num_processes=num_GPUSs)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
