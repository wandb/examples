{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/gb6B4ig.png\" width=\"400\" alt=\"Weights & Biases\" />\n",
    "\n",
    "_W&B Datasets & Predictions is currently in the early-access phase. You can use it in our production service at [wandb.ai](https://wandb.ai), with [some limitations](https://docs.wandb.com/datasets-and-predictions#current-limitations). APIs are subject to change. We'd love to hear questions, comments, and ideas! Drop us a line at feedback@wandb.com._\n",
    "\n",
    "# WandB Dataset Visualization Demo\n",
    "\n",
    "This notebook demonstrates WandB's dataset visualization features. In particular we will show how WandB [Artifacts](https://docs.wandb.com/artifacts) can be used to visualize datasets and predictions, with a focus on image data. We will track model and data lineage as well as perform interactive model analysis on the resulting datasets. The overall flow will be:\n",
    "\n",
    "1. Create a dataset\n",
    "2. Split the dataset into train and test\n",
    "3. Train a model to make predictions on the transformed dataet\n",
    "4. Log predications from the model against training and evaluation sets\n",
    "5. Analyze the model in WandB's UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/datasets-predictions/W%26B_Dataset_Visualization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Setup\n",
    "\n",
    "## Install requirements & utils\n",
    "\n",
    "For brevity, we put utility functions for working with the dataset in `util.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install the python dependencies\n",
    "!pip install matplotlib numpy Pillow wandb\n",
    "\n",
    "# Download a util file of helper methods for this notebook\n",
    "!curl https://raw.githubusercontent.com/wandb/dsviz-demo/master/util.py --output util.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab sometimes has problems with Pillow. If you are facing this issue, \n",
    "# uncomment the `exit()` line and run this cell. Then rerun the `!pip install`\n",
    "# cell above.\n",
    "\n",
    "# exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import wandb\n",
    "print(wandb.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Login to wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default project name where results will be logged\n",
    "WANDB_PROJECT = \"dsviz-demo-colab\"\n",
    "NUM_EXAMPLES = 50\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data\n",
    "\n",
    "Before we get started, we will download an example dataset to our local machine. This is a big dataset, so please be patient if you are on a slow connection. For brevity, we put utility functions for working with the dataset in `util.py`. After the download is complete, we will show an example of the data.\n",
    "\n",
    "**Note:** if you see the error \"``AttributeError: module 'PIL.TiffTags' has no attribute 'IFD'``\", this is likely a [Colab issue](https://github.com/facebookresearch/detectron2/issues/2231) which can be solved by restarting your runtime (header menu > Runtime > Restart runtime)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data if not already present\n",
    "util.download_data()\n",
    "# Show an example training image\n",
    "util.show_image(util.get_train_image_path(0))\n",
    "# Show an example of color mask\n",
    "util.show_image(util.get_color_label_image_path(0))\n",
    "\n",
    "# Print the label types:\n",
    "print(\"Class Mapping:\")\n",
    "print(list(zip(util.BDD_IDS, util.BDD_CLASSES)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Build the dataset\n",
    "\n",
    "First, let's build a dataset for use in the rest of this project. We will do this in the context of a `wandb.Run`. A `Run` is an isolated process which can optionally depend on upstream artifacts as well as optionally produce artifacts for later consumption. In this step, we will create a `wandb.Table` during our run and output it in an artifact. This table will contain all of our raw data for later use. Moreover, W&B offers rich tools to analyze and visualize such Tables in the interactive UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the run\n",
    "with wandb.init(\n",
    "    project=WANDB_PROJECT,       # The project to register this Run to\n",
    "    job_type=\"create_dataset\",   # The type of this Run. Runs of the same type can be grouped together in the UI\n",
    "    config={                     # Custom configuration parameters which you might want to tune or adjust for the Run\n",
    "        \"num_examples\": NUM_EXAMPLES,      # The number of raw samples to include.\n",
    "        \"scale_factor\": 2        # The scaling factor for the images\n",
    "    }) as run:\n",
    "\n",
    "    # Setup a WandB Classes object. This will give additional metadata for visuals\n",
    "    class_set = wandb.Classes([\n",
    "        {'name': name, 'id': id} \n",
    "        for name, id in zip(util.BDD_CLASSES, util.BDD_IDS)\n",
    "    ])\n",
    "    \n",
    "    # Setup a WandB Table object to hold our dataset\n",
    "    table = wandb.Table(\n",
    "        columns=[\"id\", \"train_image\", \"colored_image\", \"label_mask\", \"dominant_class\"]\n",
    "    )\n",
    "    \n",
    "    # Fill up the table\n",
    "    for ndx in range(run.config[\"num_examples\"]):\n",
    "        \n",
    "        # First, we will build a wandb.Image to act as our raw example object\n",
    "        #    classes: the classes which map to masks and/or box metadata\n",
    "        #    masks: the mask metadata. In this case, we use a 2d array where each cell corresponds to the label (this comes directly from the dataset)\n",
    "        #    boxes: the bounding box metadata. For example sake, we create bounding boxes by looking at the mask data and creating boxes which fully enclose each class.\n",
    "        #           The data is an array of objects like:\n",
    "        #                 \"position\": {\n",
    "        #                             \"minX\": minX,\n",
    "        #                             \"maxX\": maxX,\n",
    "        #                             \"minY\": minY,\n",
    "        #                             \"maxY\": maxY,\n",
    "        #                         },\n",
    "        #                         \"class_id\" : id_num,          \n",
    "        #                     }\n",
    "        example = wandb.Image(\n",
    "            util.get_scaled_train_image(ndx, run.config.scale_factor),\n",
    "            classes = class_set, \n",
    "            masks = {\n",
    "                \"ground_truth\": {\n",
    "                    \"mask_data\": util.get_scaled_mask_label(ndx, run.config.scale_factor)\n",
    "                },\n",
    "            }, \n",
    "            boxes={\n",
    "                \"ground_truth\": {\n",
    "                    \"box_data\": util.get_scaled_bounding_boxes(ndx, run.config.scale_factor)\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Next, we create two additional images which may be helpful during analysis. Notice that the additional metadata is optional.\n",
    "        color_label = wandb.Image(util.get_scaled_color_mask(ndx, run.config.scale_factor))\n",
    "        label_mask = wandb.Image(util.get_scaled_mask_label(ndx, run.config.scale_factor))\n",
    "        \n",
    "        # Finally, we add a row of our newly constructed data.\n",
    "        table.add_data(util.train_ids[ndx], example, color_label, label_mask, util.get_dominant_class(label_mask))\n",
    "    \n",
    "    # Create an Artifact (versioned folder)\n",
    "    artifact = wandb.Artifact(name=\"raw_data\", type=\"dataset\")\n",
    "    \n",
    "    # .add the table to the artifact\n",
    "    artifact.add(table, \"raw_examples\")\n",
    "    \n",
    "    # Finally, log the artifact\n",
    "    run.log_artifact(artifact)\n",
    "                       \n",
    "    print(\"Saving data to WandB...\")\n",
    "print(\"... Run Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review the dataset in the Dashboard\n",
    "\n",
    "Great, now if you click on the URL above, you should land on a run page. Since we did not log any metrics, there are no charts. Click the database icon (it looks like a stack of hockey pucks) on the left panel to see this run's artifacts. You should see something similar to the following:\n",
    "\n",
    "![Raw Data](https://raw.githubusercontent.com/wandb/dsviz-demo/master/notebook_images/raw_data.png)\n",
    "\n",
    "Click on the \"`raw_data`\" row and navigate to the \"Files\" table. It should look like this:\n",
    "\n",
    "![Raw Data](https://raw.githubusercontent.com/wandb/dsviz-demo/master/notebook_images/raw_data_files.png)\n",
    "\n",
    "Clicking on the \"`raw_examples.table.json`\" entry will launch an interactive data explorer to review the table we just built:\n",
    "\n",
    "![Raw Data](https://raw.githubusercontent.com/wandb/dsviz-demo/master/notebook_images/raw_data_explore.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Splitting the data into train and test\n",
    "\n",
    "Next, we will split the data into a train and a test dataset. Similar to before, we will launch a `Run` to perform this operation. Remember, this new execution could happen on a different machine as we will dynamically load the needed resources. In particular, we will lood in the raw dataset from the last run, and output 2 new datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step should look familiar by now:\n",
    "with wandb.init(\n",
    "    project=WANDB_PROJECT, \n",
    "    job_type=\"split_dataset\",\n",
    "    config = {\n",
    "        \"train_pct\": 0.7,\n",
    "    }\n",
    ") as run:\n",
    "    \n",
    "    # Get the latest version of the artifact. Notice the name alias follows this convention: \"<ARTIFACT_NAME>:<VERSION>\"\n",
    "    # When version is set to \"latest\", then the latest version will always be used.\n",
    "    # However, you can pin to a version by using an alias such as \"raw_data:v0\"\n",
    "    dataset_artifact = run.use_artifact(\"raw_data:latest\")\n",
    "    \n",
    "    # Next, we .get the table by the same name that we saved it in the last run.\n",
    "    data_table = dataset_artifact.get(\"raw_examples\")\n",
    "    \n",
    "    # Print a row\n",
    "    print(\"\\nExample Data row\\n\", data_table.data[0])\n",
    "    \n",
    "    # Show an example image\n",
    "    print(\"\\nExample Image\\n\")\n",
    "    plt.imshow(data_table.data[0][1]._image)\n",
    "    plt.show()\n",
    "    \n",
    "    # Notice that a new directory was made: artifacts which is managed by wandb\n",
    "    print(\"\\nArtifact Directory Contents: \\n\", os.listdir(\"artifacts\"))\n",
    "    \n",
    "    # Now we can build two separate artifacts for later use. We will first split the raw table into two parts,\n",
    "    # then create two different artifacts, each of which will hold our new tables. We create two artifacts so that\n",
    "    # in future runs, we can selectively decide which subsets of data to download.\n",
    "    \n",
    "    # Create the tables\n",
    "    train_count = int(len(data_table.data) * run.config.train_pct)\n",
    "    train_table = wandb.Table(columns=data_table.columns, data=data_table.data[:train_count])\n",
    "    test_table = wandb.Table(columns=data_table.columns, data=data_table.data[train_count:])\n",
    "    \n",
    "    # Create the artifacts\n",
    "    train_artifact = wandb.Artifact(\"train_data\", \"dataset\")\n",
    "    test_artifact = wandb.Artifact(\"test_data\", \"dataset\")\n",
    "    \n",
    "    # Save the tables to the artifacts with .add\n",
    "    train_artifact.add(train_table, \"train_table\")\n",
    "    test_artifact.add(test_table, \"test_table\")\n",
    "    \n",
    "    # Log the artifacts out as outputs of the run\n",
    "    run.log_artifact(train_artifact)\n",
    "    run.log_artifact(test_artifact)\n",
    "    \n",
    "    print(\"Saving data to WandB...\")\n",
    "print(\"... Run Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review the splits in the Dashboard\n",
    "Notice, in this step, the raw_data `wandb.Table` was reinstatiated and the data, images, etc... came along for the ride. This makes it easy for ML practitioners on a team to share data and assets easily. To manage this, you can see that we created an artifacts directory to save local data.\n",
    "\n",
    "Now we have two new datasets. Feel free to browse them similar to our last step. However, this time, click \"Graph View\" rather than \"Files\" to see the lineage of the artifact:\n",
    "\n",
    "![](https://raw.githubusercontent.com/wandb/dsviz-demo/master/notebook_images/split_data.png)\n",
    "\n",
    "![](https://raw.githubusercontent.com/wandb/dsviz-demo/master/notebook_images/split_graph.png)\n",
    "\n",
    "We will come back to this graph view later on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Model Training\n",
    "\n",
    "Now we will train a model to predict bounding boxes. For the sake of simplicity, we will \"train\" a model which splits the image into it's grayscale quantiles and assigns labels to each patch. As you can imagine, the model performance can be improved dramatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, create a run.\n",
    "with wandb.init(project=WANDB_PROJECT, job_type=\"model_train\") as run:\n",
    "    \n",
    "    # Similar to before, we will load in the artifact and asset we need. In this case, the training data\n",
    "    train_artifact = run.use_artifact(\"train_data:latest\")\n",
    "    train_table = train_artifact.get(\"train_table\")\n",
    "    \n",
    "    # Next, we split out the labels and train the model\n",
    "    train_data, mask_data = util.make_datasets(train_table, util.n_classes)\n",
    "    model = util.ExampleSegmentationModel(util.n_classes)\n",
    "    model.train(train_data, mask_data)\n",
    "    \n",
    "    # Finally we score the model. Behind the scenes, we score each mask on its IOU score.\n",
    "    scores, results = util.score_model(model, train_data, mask_data, util.n_classes)\n",
    "    \n",
    "    # Let's create a new table. Notice that we create many columns - an evaluation score for each class type.\n",
    "    results_table = wandb.Table(\n",
    "        columns = [\"id\", \"pred_mask\", \"dominant_pred\"] + util.BDD_CLASSES,\n",
    "        \n",
    "        # Data construction is similar to before, but we now use the predicted masks and bound boxes.\n",
    "        data=[\n",
    "        [train_table.data[ndx][0],\n",
    "         wandb.Image(train_table.data[ndx][1], masks={\n",
    "            \"train_predicted_truth\": {\n",
    "                \"mask_data\": results[ndx],\n",
    "            },\n",
    "        }, boxes={\n",
    "            \"ground_truth\": {\n",
    "                \"box_data\": util.mask_to_bounding(results[ndx])\n",
    "            }\n",
    "        }),\n",
    "        util.BDD_CLASSES[util.get_dominant_id_ndx(results[ndx])],\n",
    "        ] + list(row)\n",
    "        for ndx, row in enumerate(scores)\n",
    "    ])\n",
    "    \n",
    "    # We create an artifact, add the table, and log it as part of the run.\n",
    "    results_artifact = wandb.Artifact(\"train_results\", \"dataset\")\n",
    "    results_artifact.add(results_table, \"train_iou_score_table\")\n",
    "    run.log_artifact(results_artifact)\n",
    "    \n",
    "    # Finally, let's save the model as a flat file and add that to its own artifact.\n",
    "    model.save(\"model.pkl\")\n",
    "    model_artifact = wandb.Artifact(\"trained_model\", \"model\")\n",
    "    model_artifact.add_file(\"model.pkl\")\n",
    "    run.log_artifact(model_artifact)\n",
    "    \n",
    "    print(\"Saving data to WandB...\")\n",
    "print(\"... Run Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Model Evaluation\n",
    "\n",
    "Now that we have a trained model, we want to score it on the test data which was held out in step 2. This code is very similar to the training step, with the execption of slightly different naming. The important difference is that we load the saved model from the artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init(project=WANDB_PROJECT, job_type=\"model_eval\") as run:\n",
    "    \n",
    "    # Retrieve the test data\n",
    "    test_artifact = run.use_artifact(\"test_data:latest\")\n",
    "    test_table = test_artifact.get(\"test_table\")\n",
    "    test_data, mask_data = util.make_datasets(test_table, util.n_classes)\n",
    "    \n",
    "    # Download the saved model file.\n",
    "    model_artifact = run.use_artifact(\"trained_model:latest\")\n",
    "    path = model_artifact.get_path(\"model.pkl\").download()\n",
    "    \n",
    "    # Load the model from the file and score it\n",
    "    model = util.ExampleSegmentationModel.load(path)\n",
    "    scores, results = util.score_model(model, test_data, mask_data, util.n_classes)\n",
    "    \n",
    "    # Create a predicted score table similar to step 3.\n",
    "    results_artifact = wandb.Artifact(\"test_results\", \"dataset\")\n",
    "    data = [\n",
    "        [test_table.data[ndx][0], \n",
    "         wandb.Image(test_table.data[ndx][1], masks={\n",
    "            \"test_predicted_truth\": {\n",
    "                \"mask_data\": results[ndx],\n",
    "            },\n",
    "        }, boxes={\n",
    "            \"ground_truth\": {\n",
    "                \"box_data\": util.mask_to_bounding(results[ndx])\n",
    "            }\n",
    "        }),\n",
    "        util.BDD_CLASSES[util.get_dominant_id_ndx(results[ndx])],\n",
    "        ] + list(row)\n",
    "        for ndx, row in enumerate(scores)\n",
    "    ]\n",
    "    \n",
    "    # And log out the results.\n",
    "    results_artifact.add(wandb.Table([\"id\", \"pred_mask_test\", \"dominant_pred_test\"] + util.BDD_CLASSES, data=data), \"test_iou_score_table\")\n",
    "    run.log_artifact(results_artifact)\n",
    "    \n",
    "    print(\"Saving data to WandB...\")\n",
    "print(\"... Run Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Model Analysis\n",
    "\n",
    "This is where it all comes together. In this step, we join the train and test scoring results with the original dataset and output corresponding artifacts. The new idea introduced here is a `wandb.JoinedTable` which allows you to join two `Table`s for further analysis in the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init(project=WANDB_PROJECT, job_type=\"model_result_analysis\") as run:\n",
    "    \n",
    "    # Retrieve the original raw dataset\n",
    "    dataset_artifact = run.use_artifact(\"raw_data:latest\")\n",
    "    data_table = dataset_artifact.get(\"raw_examples\")\n",
    "    \n",
    "    # Retrieve the train and test score tables\n",
    "    train_artifact = run.use_artifact(\"train_results:latest\")\n",
    "    train_table = train_artifact.get(\"train_iou_score_table\")\n",
    "    \n",
    "    test_artifact = run.use_artifact(\"test_results:latest\")\n",
    "    test_table = test_artifact.get(\"test_iou_score_table\")\n",
    "    \n",
    "    # Join the tables on ID column and log them as outputs.\n",
    "    train_results = wandb.JoinedTable(train_table, data_table, \"id\")\n",
    "    test_results = wandb.JoinedTable(test_table, data_table, \"id\")\n",
    "    artifact = wandb.Artifact(\"summary_results\", \"dataset\")\n",
    "    artifact.add(train_results, \"train_results\")\n",
    "    artifact.add(test_results, \"test_results\")\n",
    "    run.log_artifact(artifact)\n",
    "    \n",
    "    print(\"Saving data to WandB...\")\n",
    "print(\"... Run Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review the model analysis in the Dashboard\n",
    "Now, click on the above **Project** page (second link). This will look like the following:\n",
    "\n",
    "![](https://raw.githubusercontent.com/wandb/dsviz-demo/master/notebook_images/project_page.png)\n",
    "\n",
    "Click on the database icon, as previously, to see the artifacts. This time, you are seeing the artifacts for the entire project, with counts of their versions:\n",
    "\n",
    "![](https://raw.githubusercontent.com/wandb/dsviz-demo/master/notebook_images/project_artifacts.png)\n",
    "\n",
    "Go ahead and click the \"`model`\" artifact type, \"Files\", and \"`model.pkl`\". The viewer will provide different renderings based on the file type. For a pickled class, you get the following image. For deep networks saved as `.h5` files, you can see all the layers and their attributes.\n",
    "\n",
    "![](https://raw.githubusercontent.com/wandb/dsviz-demo/master/notebook_images/model_view.png)\n",
    "\n",
    "Next, head back to the artifact page, click Database type, expand `summary_results`, and select your most recent version. Click \"Files\" and select one of the join tables:\n",
    "\n",
    "![](https://raw.githubusercontent.com/wandb/dsviz-demo/master/notebook_images/join_view.png)\n",
    "\n",
    "Exploring a bit, you can toggle the bounding boxes, masks, group, filter, and sort the data:\n",
    "\n",
    "![](https://raw.githubusercontent.com/wandb/dsviz-demo/master/notebook_images/summary_join.png)\n",
    "\n",
    "![](https://raw.githubusercontent.com/wandb/dsviz-demo/master/notebook_images/grouped.png)\n",
    "\n",
    "Finally, click graph view, and \"explode\". Now, you can visualize the entire process end-to-end:\n",
    "\n",
    "![](https://raw.githubusercontent.com/wandb/dsviz-demo/master/notebook_images/summary_graph.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
