{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/gb6B4ig.png\" width=\"400\" alt=\"Weights & Biases\" />\n",
    "\n",
    "<!--- @wandbcode{tables_whalesong} -->\n",
    "\n",
    "# Log timbre transfer audio experiments to W&B\n",
    "\n",
    "Given some input audio (a microphone recording or a file upload), resynthesize the melody of the audio as if it were played on a violin, flute, trumpet, or tenor sax. Log all your experiments to an interactive W&B Table for easy exploration and tuning.\n",
    "\n",
    "### Source Colab\n",
    "\n",
    "This notebook is a Weights & Biases integration and wrapper around the amazing Timbre Transfer Demo with DDSP (Differentiable Digital Signal Processing) from Tensorflow Magenta\n",
    "<a href=\"https://colab.research.google.com/github/magenta/ddsp/blob/master/ddsp/colab/demos/timbre_transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Timbre Transfer with Interactive Visualization\n",
    "\n",
    "The notebook processes audio input with timbre transfer, resynthesizing the melody using a model pretrained for various instruments (violin, flute, trumpet, etc). \n",
    "\n",
    "### [Explore an example with whale songs on W&B](https://wandb.ai/stacey/cshanty/reports/Whale2Song-W-B-Tables-for-Audio--Vmlldzo4NDI3NzM)\n",
    "\n",
    "<img src=\"https://i.imgur.com/T3vVzWZ.png\" height=400 alt=\"interactive audio wandb Table\"/></a>\n",
    "\n",
    "This notebook extracts features from input audio:\n",
    "* uploaded files\n",
    "* microphone recordings (to use this option, make sure to allow microphone access in your browser)\n",
    "* URLs to sound files (hardcoded for this demo, feel free to edit the variable SONG_URL)\n",
    "\n",
    "The available models are trained to generate audio conditioned on a time series of fundamental frequency and loudness. The input audio, synthesized song, and visualizations of the signal will be uploaded to an interactive W&B Table. You can experiment with different recordings, instruments, and various audio settings (using sliders) in this notebook. All of this configuraion will be organized alongside the song versions in one W&B project.\n",
    "\n",
    "<img src=\"https://i.imgur.com/Jo3vrGm.png\" height=400 alt=\"interactive audio wandb Table\"/></a>\n",
    "\n",
    "## Additional Resources\n",
    "* Full W&B Example: [Visualizing Audio Data with W&B Tables](https://wandb.ai/stacey/cshanty/reports/Whale2Song-W-B-Tables-for-Audio--Vmlldzo4NDI3NzM)\n",
    "* [DDSP ICLR paper](https://openreview.net/forum?id=B1x1ma4tDr)\n",
    "* [Audio Examples](http://goo.gl/magenta/ddsp-examples) \n",
    "* marine mammal recordings from [Watkins Marine Mammal Sound Database](https://cis.whoi.edu/science/B/whalesounds/index.cfm), Woods Hole Oceanographic Institution\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/datasets-predictions/Logging_Timbre_Transfer_with_W%26B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.Dependencies and helper functions\n",
    "\n",
    "Install dependencies and wandb, and download the model. The DDSP part transfers a lot of data and _should take a minute or two according to the source colab_. Also define helper functions to process audio data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x\n",
    "print('Installing from pip package...')\n",
    "!pip install -qU ddsp==1.0.1\n",
    "!pip install -qqq wandb\n",
    "# Ignore a bunch of deprecation warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import time\n",
    "\n",
    "import crepe\n",
    "import ddsp\n",
    "import ddsp.training\n",
    "from ddsp.colab import colab_utils\n",
    "from ddsp.colab.colab_utils import (\n",
    "    audio_bytes_to_np,\n",
    "    auto_tune, detect_notes, fit_quantile_transform, \n",
    "    get_tuning_factor, download, play, record, \n",
    "    specplot, upload, DEFAULT_SAMPLE_RATE)\n",
    "from ddsp import core\n",
    "from ddsp import spectral_ops\n",
    "\n",
    "import gin\n",
    "from google.colab import files\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import wandb\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "TRIM = -15\n",
    "DEFAULT_SAMPLE_RATE = spectral_ops.CREPE_SAMPLE_RATE # 16000\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_song(audio, song_id, save_fig=\"_wave.png\"):\n",
    "  # Setup the session.\n",
    "  ddsp.spectral_ops.reset_crepe()\n",
    "\n",
    "  # Compute features.\n",
    "  start_time = time.time()\n",
    "  audio_features = ddsp.training.metrics.compute_audio_features(audio)\n",
    "  audio_features['loudness_db'] = audio_features['loudness_db'].astype(np.float32)\n",
    "  audio_features_mod = None\n",
    "  print('Audio features took %.1f seconds' % (time.time() - start_time))\n",
    "\n",
    "  TRIM = -15\n",
    "  # Plot Features.\n",
    "  fig, ax = plt.subplots(nrows=3, \n",
    "                        ncols=1, \n",
    "                        sharex=True,\n",
    "                        figsize=(6, 8))\n",
    "  ax[0].plot(audio_features['loudness_db'][:TRIM])\n",
    "  ax[0].set_ylabel('loudness_db')\n",
    "\n",
    "\n",
    "  ax[1].plot(librosa.hz_to_midi(audio_features['f0_hz'][:TRIM]))\n",
    "  ax[1].set_ylabel('f0 [midi]')\n",
    "\n",
    "  ax[2].plot(audio_features['f0_confidence'][:TRIM])\n",
    "  ax[2].set_ylabel('f0 confidence')\n",
    "  _ = ax[2].set_xlabel('Time step [frame]')\n",
    "  save_fig_path = song_id + save_fig\n",
    "  fig.savefig(save_fig_path)\n",
    "\n",
    "  return audio_features, save_fig_path\n",
    "\n",
    "def specplot_local(audio, song_id, save_fig=\"_spec.png\",\n",
    "             vmin=-5,\n",
    "             vmax=1,\n",
    "             rotate=True,\n",
    "             size=512 + 256,\n",
    "             **matshow_kwargs):\n",
    "  \"\"\"Plot the log magnitude spectrogram of audio.\"\"\"\n",
    "  # If batched, take first element.\n",
    "  if len(audio.shape) == 2:\n",
    "    audio = audio[0]\n",
    "\n",
    "  logmag = spectral_ops.compute_logmag(core.tf_float32(audio), size=size)\n",
    "  if rotate:\n",
    "    logmag = np.rot90(logmag)\n",
    "\n",
    "  #plt.xticks([])\n",
    "  #plt.yticks([])\n",
    "  #plt.xlabel('Time')\n",
    "  #plt.ylabel('Frequency')\n",
    "\n",
    "  save_fig_path = song_id + save_fig\n",
    "  plt.imsave(save_fig_path,\n",
    "             logmag,\n",
    "             vmin=vmin,\n",
    "             vmax=vmax,\n",
    "             cmap=plt.cm.magma)\n",
    "  \n",
    "  return save_fig_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Initialize and login to W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_PROJECT = \"timbre_demo\"\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Song setup (run for every new song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=WANDB_PROJECT)\n",
    "\n",
    "# generate one random song id, feel free to replace\n",
    "SONG_ID = str(np.random.choice(1000, 1)[0])\n",
    "\n",
    "# hardcoded to a favorite marine mammal melody, feel free to replace\n",
    "SONG_URL = \"https://whoicf2.whoi.edu/science/B/whalesounds/WhaleSounds/6301900Y.wav\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Audio input: Record, upload, or URL\n",
    "\n",
    "You have several options for audio input:\n",
    "1. **Record** audio from your microphone (NOTE: allow microphone access in your browser to do this)\n",
    "2. **Upload** audio from a file (.mp3 or .wav)\n",
    "3. **Download a URL** (this is hardcoded for the demo, and you can change SONG_URL in Step 2 to edit this)\n",
    "\n",
    "Additional notes:\n",
    "* Audio should be monophonic (single instrument / voice)\n",
    "* Extracts fundmanetal frequency (f0) and loudness features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_or_upload = \"Record\"  #@param [\"Record\", \"Upload (.mp3 or .wav)\", \"URL\"]\n",
    "\n",
    "record_seconds =     5#@param {type:\"number\", min:1, max:10, step:1}\n",
    "\n",
    "if record_or_upload == \"Record\":\n",
    "  audio = record(seconds=record_seconds)\n",
    "elif record_or_upload == \"URL\":\n",
    "  filename = SONG_URL.strip().split('/')[-1]\n",
    "  urlretrieve(song_url, filename)\n",
    "  wav_bytes = open(filename, \"rb\").read()\n",
    "  audio = audio_bytes_to_np(wav_bytes)\n",
    "else:\n",
    "  # Load audio sample here (.mp3 or .wav3 file)\n",
    "  # Just use the first file.\n",
    "  filenames, audios = upload()\n",
    "  audio = audios[0]\n",
    "\n",
    "audio = audio[np.newaxis, :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload sample song to W&B\n",
    "\n",
    "You will see a URL to your W&B run, which will show a playable version of the song and some audio visualizations in a new Table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"id\", \"orig_song\", \"orig_plot\", \"orig_spec\"]\n",
    "\n",
    "audio_features, orig_waveplot = process_song(audio, SONG_ID, \"_orig_plot.png\")\n",
    "orig_specplot = specplot_local(audio, SONG_ID, \"_orig_spec.png\")\n",
    "orig_song = wandb.Audio(np.squeeze(audio), sample_rate=DEFAULT_SAMPLE_RATE)\n",
    "\n",
    "data = [[SONG_ID, orig_song, wandb.Image(orig_waveplot), wandb.Image(orig_specplot)]]\n",
    "table = wandb.Table(data=data, columns=columns)\n",
    "wandb.run.log({\"sample_song\" : table}) \n",
    "wandb.run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Synthetic output (run for every new song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Choose an instrument (load a model)\n",
    "#@markdown Run for every new audio input\n",
    "model = 'Tenor_Saxophone' #@param ['Violin', 'Flute', 'Flute2', 'Trumpet', 'Tenor_Saxophone', 'Upload your own (checkpoint folder as .zip)']\n",
    "MODEL = model\n",
    "\n",
    "def find_model_dir(dir_name):\n",
    "  # Iterate through directories until model directory is found\n",
    "  for root, dirs, filenames in os.walk(dir_name):\n",
    "    for filename in filenames:\n",
    "      if filename.endswith(\".gin\") and not filename.startswith(\".\"):\n",
    "        model_dir = root\n",
    "        break\n",
    "  return model_dir \n",
    "\n",
    "if model in ('Violin', 'Flute', 'Flute2', 'Trumpet', 'Tenor_Saxophone'):\n",
    "  # Pretrained models.\n",
    "  PRETRAINED_DIR = '/content/pretrained'\n",
    "  # Copy over from gs:// for faster loading.\n",
    "  !rm -r $PRETRAINED_DIR &> /dev/null\n",
    "  !mkdir $PRETRAINED_DIR &> /dev/null\n",
    "  GCS_CKPT_DIR = 'gs://ddsp/models/timbre_transfer_colab/2021-01-06'\n",
    "  model_dir = os.path.join(GCS_CKPT_DIR, 'solo_%s_ckpt' % model.lower())\n",
    "  \n",
    "  !gsutil cp $model_dir/* $PRETRAINED_DIR &> /dev/null\n",
    "  model_dir = PRETRAINED_DIR\n",
    "  gin_file = os.path.join(model_dir, 'operative_config-0.gin')\n",
    "\n",
    "else:\n",
    "  # User models.\n",
    "  UPLOAD_DIR = '/content/uploaded'\n",
    "  !mkdir $UPLOAD_DIR\n",
    "  uploaded_files = files.upload()\n",
    "\n",
    "  for fnames in uploaded_files.keys():\n",
    "    print(\"Unzipping... {}\".format(fnames))\n",
    "    !unzip -o \"/content/$fnames\" -d $UPLOAD_DIR &> /dev/null\n",
    "  model_dir = find_model_dir(UPLOAD_DIR)\n",
    "  gin_file = os.path.join(model_dir, 'operative_config-0.gin')\n",
    "\n",
    "\n",
    "# Load the dataset statistics.\n",
    "DATASET_STATS = None\n",
    "dataset_stats_file = os.path.join(model_dir, 'dataset_statistics.pkl')\n",
    "print(f'Loading dataset statistics from {dataset_stats_file}')\n",
    "try:\n",
    "  if tf.io.gfile.exists(dataset_stats_file):\n",
    "    with tf.io.gfile.GFile(dataset_stats_file, 'rb') as f:\n",
    "      DATASET_STATS = pickle.load(f)\n",
    "except Exception as err:\n",
    "  print('Loading dataset statistics from pickle failed: {}.'.format(err))\n",
    "\n",
    "\n",
    "# Parse gin config,\n",
    "with gin.unlock_config():\n",
    "  gin.parse_config_file(gin_file, skip_unknown=True)\n",
    "\n",
    "# Assumes only one checkpoint in the folder, 'ckpt-[iter]`.\n",
    "ckpt_files = [f for f in tf.io.gfile.listdir(model_dir) if 'ckpt' in f]\n",
    "ckpt_name = ckpt_files[0].split('.')[0]\n",
    "ckpt = os.path.join(model_dir, ckpt_name)\n",
    "\n",
    "# Ensure dimensions and sampling rates are equal\n",
    "time_steps_train = gin.query_parameter('F0LoudnessPreprocessor.time_steps')\n",
    "n_samples_train = gin.query_parameter('Harmonic.n_samples')\n",
    "hop_size = int(n_samples_train / time_steps_train)\n",
    "\n",
    "time_steps = int(audio.shape[1] / hop_size)\n",
    "n_samples = time_steps * hop_size\n",
    "\n",
    "# print(\"===Trained model===\")\n",
    "# print(\"Time Steps\", time_steps_train)\n",
    "# print(\"Samples\", n_samples_train)\n",
    "# print(\"Hop Size\", hop_size)\n",
    "# print(\"\\n===Resynthesis===\")\n",
    "# print(\"Time Steps\", time_steps)\n",
    "# print(\"Samples\", n_samples)\n",
    "# print('')\n",
    "\n",
    "gin_params = [\n",
    "    'Harmonic.n_samples = {}'.format(n_samples),\n",
    "    'FilteredNoise.n_samples = {}'.format(n_samples),\n",
    "    'F0LoudnessPreprocessor.time_steps = {}'.format(time_steps),\n",
    "    'oscillator_bank.use_angular_cumsum = True',  # Avoids cumsum accumulation errors.\n",
    "]\n",
    "\n",
    "with gin.unlock_config():\n",
    "  gin.parse_config(gin_params)\n",
    "\n",
    "\n",
    "# Trim all input vectors to correct lengths \n",
    "for key in ['f0_hz', 'f0_confidence', 'loudness_db']:\n",
    "  audio_features[key] = audio_features[key][:time_steps]\n",
    "audio_features['audio'] = audio_features['audio'][:, :n_samples]\n",
    "\n",
    "\n",
    "# Set up the model just to predict audio given new conditioning\n",
    "model = ddsp.training.models.Autoencoder()\n",
    "model.restore(ckpt)\n",
    "\n",
    "# Build model by running a batch through it.\n",
    "start_time = time.time()\n",
    "_ = model(audio_features, training=False)\n",
    "print('Restoring model took %.1f seconds' % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Modify conditioning\n",
    "\n",
    "#@markdown These models were not explicitly trained to perform timbre transfer, so they may sound unnatural if the incoming loudness and frequencies are very different then the training data (which will always be somewhat true). \n",
    "\n",
    "\n",
    "#@markdown ## Note Detection\n",
    "\n",
    "#@markdown You can leave this at 1.0 for most cases\n",
    "threshold = 1 #@param {type:\"slider\", min: 0.0, max:2.0, step:0.01}\n",
    "\n",
    "\n",
    "#@markdown ## Automatic\n",
    "\n",
    "ADJUST = True #@param{type:\"boolean\"}\n",
    "\n",
    "#@markdown Quiet parts without notes detected (dB)\n",
    "quiet = 20 #@param {type:\"slider\", min: 0, max:60, step:1}\n",
    "\n",
    "#@markdown Force pitch to nearest note (amount)\n",
    "autotune = 0 #@param {type:\"slider\", min: 0.0, max:1.0, step:0.1}\n",
    "\n",
    "#@markdown ## Manual\n",
    "\n",
    "\n",
    "#@markdown Shift the pitch (octaves)\n",
    "pitch_shift =  0 #@param {type:\"slider\", min:-2, max:2, step:1}\n",
    "\n",
    "#@markdown Adjsut the overall loudness (dB)\n",
    "loudness_shift = 0 #@param {type:\"slider\", min:-20, max:20, step:1}\n",
    "\n",
    "# save settings\n",
    "SONG_CFG = {\n",
    "    \"threshold\" : threshold,\n",
    "    \"adjust\" : ADJUST,\n",
    "    \"quiet\" : quiet,\n",
    "    \"autotune\" : autotune,\n",
    "    \"pitch_shift\" : pitch_shift,\n",
    "    \"loudness_shift\" : loudness_shift\n",
    "}\n",
    "    \n",
    "audio_features_mod = {k: v.copy() for k, v in audio_features.items()}\n",
    "\n",
    "\n",
    "## Helper functions.\n",
    "def shift_ld(audio_features, ld_shift=0.0):\n",
    "  \"\"\"Shift loudness by a number of ocatves.\"\"\"\n",
    "  audio_features['loudness_db'] += ld_shift\n",
    "  return audio_features\n",
    "\n",
    "\n",
    "def shift_f0(audio_features, pitch_shift=0.0):\n",
    "  \"\"\"Shift f0 by a number of ocatves.\"\"\"\n",
    "  audio_features['f0_hz'] *= 2.0 ** (pitch_shift)\n",
    "  audio_features['f0_hz'] = np.clip(audio_features['f0_hz'], \n",
    "                                    0.0, \n",
    "                                    librosa.midi_to_hz(110.0))\n",
    "  return audio_features\n",
    "\n",
    "\n",
    "mask_on = None\n",
    "\n",
    "if ADJUST and DATASET_STATS is not None:\n",
    "  # Detect sections that are \"on\".\n",
    "  mask_on, note_on_value = detect_notes(audio_features['loudness_db'],\n",
    "                                        audio_features['f0_confidence'],\n",
    "                                        threshold)\n",
    "\n",
    "  if np.any(mask_on):\n",
    "    # Shift the pitch register.\n",
    "    target_mean_pitch = DATASET_STATS['mean_pitch']\n",
    "    pitch = ddsp.core.hz_to_midi(audio_features['f0_hz'])\n",
    "    mean_pitch = np.mean(pitch[mask_on])\n",
    "    p_diff = target_mean_pitch - mean_pitch\n",
    "    p_diff_octave = p_diff / 12.0\n",
    "    round_fn = np.floor if p_diff_octave > 1.5 else np.ceil\n",
    "    p_diff_octave = round_fn(p_diff_octave)\n",
    "    audio_features_mod = shift_f0(audio_features_mod, p_diff_octave)\n",
    "\n",
    "\n",
    "    # Quantile shift the note_on parts.\n",
    "    _, loudness_norm = colab_utils.fit_quantile_transform(\n",
    "        audio_features['loudness_db'],\n",
    "        mask_on,\n",
    "        inv_quantile=DATASET_STATS['quantile_transform'])\n",
    "\n",
    "    # Turn down the note_off parts.\n",
    "    mask_off = np.logical_not(mask_on)\n",
    "    loudness_norm[mask_off] -=  quiet * (1.0 - note_on_value[mask_off][:, np.newaxis])\n",
    "    loudness_norm = np.reshape(loudness_norm, audio_features['loudness_db'].shape)\n",
    "    \n",
    "    audio_features_mod['loudness_db'] = loudness_norm \n",
    "\n",
    "    # Auto-tune.\n",
    "    if autotune:\n",
    "      f0_midi = np.array(ddsp.core.hz_to_midi(audio_features_mod['f0_hz']))\n",
    "      tuning_factor = get_tuning_factor(f0_midi, audio_features_mod['f0_confidence'], mask_on)\n",
    "      f0_midi_at = auto_tune(f0_midi, tuning_factor, mask_on, amount=autotune)\n",
    "      audio_features_mod['f0_hz'] = ddsp.core.midi_to_hz(f0_midi_at)\n",
    "\n",
    "  else:\n",
    "    print('\\nSkipping auto-adjust (no notes detected or ADJUST box empty).')\n",
    "\n",
    "else:\n",
    "  print('\\nSkipping auto-adujst (box not checked or no dataset statistics found).')\n",
    "\n",
    "# Manual Shifts.\n",
    "audio_features_mod = shift_ld(audio_features_mod, loudness_shift)\n",
    "audio_features_mod = shift_f0(audio_features_mod, pitch_shift)\n",
    "\n",
    "# Plot Features.\n",
    "has_mask = int(mask_on is not None)\n",
    "n_plots = 3 if has_mask else 2 \n",
    "fig, axes = plt.subplots(nrows=n_plots, \n",
    "                      ncols=1, \n",
    "                      sharex=True,\n",
    "                      figsize=(2*n_plots, 8))\n",
    "\n",
    "if has_mask:\n",
    "  ax = axes[0]\n",
    "  ax.plot(np.ones_like(mask_on[:TRIM]) * threshold, 'k:')\n",
    "  ax.plot(note_on_value[:TRIM])\n",
    "  ax.plot(mask_on[:TRIM])\n",
    "  ax.set_ylabel('Note-on Mask')\n",
    "  ax.set_xlabel('Time step [frame]')\n",
    "  ax.legend(['Threshold', 'Likelihood','Mask'])\n",
    "\n",
    "ax = axes[0 + has_mask]\n",
    "ax.plot(audio_features['loudness_db'][:TRIM])\n",
    "ax.plot(audio_features_mod['loudness_db'][:TRIM])\n",
    "ax.set_ylabel('loudness_db')\n",
    "ax.legend(['Original','Adjusted'])\n",
    "\n",
    "ax = axes[1 + has_mask]\n",
    "ax.plot(librosa.hz_to_midi(audio_features['f0_hz'][:TRIM]))\n",
    "ax.plot(librosa.hz_to_midi(audio_features_mod['f0_hz'][:TRIM]))\n",
    "ax.set_ylabel('f0 [midi]')\n",
    "_ = ax.legend(['Original','Adjusted'])\n",
    "\n",
    "final_wave_plot = SONG_ID + \"_final.png\"\n",
    "fig.savefig(final_wave_plot)\n",
    "SONG_CFG[\"final_wave_plot\"] = wandb.Image(final_wave_plot)\n",
    "SONG_CFG[\"instrument\"] = MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title #Resynthesize audio input\n",
    "\n",
    "af = audio_features if audio_features_mod is None else audio_features_mod\n",
    "\n",
    "# Run a batch of predictions.\n",
    "start_time = time.time()\n",
    "outputs = model(af, training=False)\n",
    "audio_gen = model.get_audio_from_outputs(outputs)\n",
    "print('Prediction took %.1f seconds' % (time.time() - start_time))\n",
    "\n",
    "# Plot\n",
    "print('Original')\n",
    "play(audio)\n",
    "orig_song = wandb.Audio(np.squeeze(audio), sample_rate=DEFAULT_SAMPLE_RATE)\n",
    "SONG_CFG[\"orig_song\"] = orig_song\n",
    "print('Resynthesis')\n",
    "play(audio_gen)\n",
    "synth_song = wandb.Audio(np.squeeze(audio_gen), sample_rate=DEFAULT_SAMPLE_RATE)\n",
    "SONG_CFG[\"synth_song\"] = synth_song\n",
    "final_spec_plot = specplot_local(audio_gen, SONG_ID, fig_name=\"_final_spec.png\")\n",
    "SONG_CFG[\"final_spec\"] = wandb.Image(final_spec_plot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.Upload synthesized song to W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=WANDB_PROJECT)\n",
    "output_columns = [\"id\", \"orig_song\", \"synth_song\", \"synth_waves\", \"synth_spec\",\n",
    "                  \"instrument\", \"threshold\", \"adjust\", \"quiet\", \"autotune\",\n",
    "                  \"pitch_shift\", \"loudness_shift\"]\n",
    "\n",
    "# populate relevant fields\n",
    "s = SONG_CFG\n",
    "data = [[SONG_ID, orig_song, synth_song, s[\"final_wave_plot\"], s[\"final_spec\"],\n",
    "         s[\"instrument\"], s[\"threshold\"],  s[\"adjust\"], s[\"quiet\"],\n",
    "         s[\"autotune\"], s[\"pitch_shift\"], s[\"loudness_shift\"]]]\n",
    "\n",
    "table = wandb.Table(data=data, columns=output_columns)\n",
    "wandb.run.log({'synth_song' : table})\n",
    "wandb.run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source colab: Timbre transfer demo from Magenta\n",
    "\n",
    "This colab relies substantially on the following Timbre Transfer demo:\n",
    "<a href=\"https://colab.research.google.com/github/magenta/ddsp/blob/master/ddsp/colab/demos/timbre_transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "##### Copyright 2021 Google LLC.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC. All Rights Reserved.\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
