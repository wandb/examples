{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-model-registry/Model_Registry_E2E.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "<!--- @wandbcode{model-reg-e2e} -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
    "<!--- @wandbcode{model-reg-e2e} -->\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Model Registry Tutorial\n",
    "The model registry is a central place to house and organize all the model tasks and their associated artifacts being worked on across an org:\n",
    "- Model checkpoint management\n",
    "- Document your models with rich model cards\n",
    "- Maintain a history of all the models being used/deployed\n",
    "- Facilitate clean hand-offs and stage management of models\n",
    "- Tag and organize various model tasks\n",
    "- Set up automatic notifications when models progress\n",
    "\n",
    "This tutorial will walkthrough how to track the model development lifecycle for a simple image classification task. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🛠️ Install `wandb`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q wandb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Login to W&B\n",
    "- You can explicitly login using `wandb login` or `wandb.login()` (See below)\n",
    "- Alternatively you can set environment variables. There are several env variables which you can set to change the behavior of W&B logging. The most important are:\n",
    "    - `WANDB_API_KEY` - find this in your \"Settings\" section under your profile \n",
    "    - `WANDB_BASE_URL` - this is the url of the W&B server\n",
    "- Find your API Token in \"Profile\" -> \"Setttings\" in the W&B App\n",
    "\n",
    "![api_token](https://drive.google.com/uc?export=view&id=1Xn7hnn0rfPu_EW0A_-32oCXqDmpA0-kx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# Login to W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Data and Model Checkpoints as Artifacts  \n",
    "W&B Artifacts allows you to track and version arbitrary serialized data (e.g. datasets, model checkpoints, evaluation results). When you create an artifact, you give it a name and a type, and that artifact is forever linked to the experimental system of record. If the underlying data changes, and you log that data asset again, W&B will automatically create new versions through checksummming its contents. W&B Artifacts can be thought of as a lightweight abstraction layer on top of shared unstructured file systems. \n",
    "\n",
    "### Anatomy of an artifact \n",
    "\n",
    "The `Artifact` class will correspond to an entry in the W&B Artifact registry.  The artifact has \n",
    "* a name\n",
    "* a type\n",
    "* metadata\n",
    "* description\n",
    "* files, directory of files, or references\n",
    "\n",
    "Example usage:\n",
    "```\n",
    "run = wandb.init(project = \"my-project\")\n",
    "artifact = wandb.Artifact(name = \"my_artifact\", type = \"data\")\n",
    "artifact.add_file(\"/path/to/my/file.txt\")\n",
    "run.log_artifact(artifact)\n",
    "run.finish()\n",
    "```\n",
    "\n",
    "In this tutorial, the first thing we will do is download a training dataset and log it as an artifact to be used downstream in the training job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# FORM VARIABLES\n",
    "PROJECT_NAME = \"model-registry-tutorial\"\n",
    "ENTITY = wandb.api.default_entity # replace with your Team name or username\n",
    "\n",
    "# Dataset constants\n",
    "DATASET_NAME = \"nature_100\"\n",
    "DATA_DIR = (Path(sys.path[0]) / \"data\")\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DATA_SRC = DATA_DIR / DATASET_NAME\n",
    "IMAGES_PER_LABEL = 10\n",
    "BALANCED_SPLITS = {\"train\" : 8, \"val\" : 1, \"test\": 1}\n",
    "MODEL_TYPE = \"squeezenet\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab a version of our Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, zipfile, io\n",
    "\n",
    "# Download the dataset from a bucket\n",
    "src_url = f\"https://storage.googleapis.com/wandb_datasets/{DATASET_NAME}.zip\"\n",
    "src_zip = f\"{DATASET_NAME}.zip\"\n",
    "\n",
    "# Download the zip file from W&B\n",
    "r = requests.get(src_url)\n",
    "\n",
    "# Create a file object using the string data\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall(path=DATA_DIR)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to generate a file containing the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with wandb.init(project=PROJECT_NAME, entity=ENTITY, job_type='log_datasets') as run:\n",
    "  train_art = wandb.Artifact(name=DATASET_NAME, \n",
    "                             type='raw_images', \n",
    "                             description='nature image dataset with 10 classes, 10 images per class')\n",
    "  train_art.add_dir(DATA_SRC)\n",
    "  wandb.log_artifact(train_art)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Artifact names and aliases to easily hand-off and abstract data assets\n",
    "- By simply referring to the `name:alias` combination of a dataset or model, we can better standardize components of a workflow\n",
    "- For instance, you can build PyTorch `Dataset`'s or `DataModule`'s which take as arguments W&B Artifact names and aliases to load appropriately\n",
    "\n",
    "You can now see all the metadata associated with this dataset, the W&B runs consuming it, and the whole lineage of upstream and downstream artifacts!\n",
    "\n",
    "![api_token](https://drive.google.com/uc?export=view&id=1fEEddXMkabgcgusja0g8zMz8whlP2Y5P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "class NatureDataset(Dataset):\n",
    "    def __init__(self, artifact_name_alias: str, transform=None):\n",
    "        self.transform = transform\n",
    "\n",
    "        # Pull down the artifact locally to load it into memory\n",
    "        art = wandb.use_artifact(artifact_name_alias)\n",
    "        self.path_at = Path(art.download())\n",
    "\n",
    "        self.img_paths = list(DATA_SRC.rglob(\"*.jpg\"))\n",
    "        labels = [image_path.parent.name for image_path in self.img_paths]\n",
    "        self.class_names = sorted(set(labels))\n",
    "        self.idx_to_class = {k: v for k, v in enumerate(self.class_names)}\n",
    "        self.class_to_idx = {v: k for k, v in enumerate(self.class_names)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        image_path = Path(self.path_at) / self.img_paths[idx]\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "        label = image_path.parent.name\n",
    "        label = torch.tensor(self.class_to_idx[label], dtype=torch.long)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "    \n",
    "class Dataloaders:\n",
    "    def __init__(self,\n",
    "                 artifact_name_alias: str,\n",
    "                 batch_size: int,\n",
    "                 input_size: int,\n",
    "                 seed: int = 42):\n",
    "        self.artifact_name_alias = artifact_name_alias\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        self.seed = seed\n",
    "\n",
    "        tfms = transforms.Compose([transforms.ToTensor(),\n",
    "                                   transforms.CenterCrop(self.input_size),\n",
    "                                   transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                                                        (0.229, 0.224, 0.225))])\n",
    "\n",
    "        print(f\"Setting up data from artifact: {self.artifact_name_alias}\")\n",
    "        self.dataset = NatureDataset(artifact_name_alias=self.artifact_name_alias,\n",
    "                                     transform=tfms)\n",
    "\n",
    "        nature_length = len(self.dataset)\n",
    "        train_size = math.floor(0.8 * nature_length)\n",
    "        val_size = math.floor(0.2 * nature_length)\n",
    "        print(f\"Splitting dataset into {train_size} training samples and {val_size} validation samples\")\n",
    "        self.ds_train, self.ds_valid = random_split(\n",
    "            self.dataset,\n",
    "            [train_size, val_size],\n",
    "            generator=torch.Generator().manual_seed(self.seed))\n",
    "    \n",
    "        self.train = DataLoader(self.ds_train, batch_size=self.batch_size)\n",
    "        self.valid = DataLoader(self.ds_valid, batch_size=self.batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the Model Class and Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "def initialize_model(num_classes, feature_extract, use_pretrained=True):\n",
    "    \"Create a model from torchvision.models\"\n",
    "    model_ft = None\n",
    "\n",
    "    # SqueezeNet\n",
    "    model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "    set_parameter_requires_grad(model_ft, feature_extract)\n",
    "    model_ft.classifier[1] = torch.nn.Conv2d(512, num_classes, kernel_size=(1, 1), stride=(1, 1))\n",
    "    model_ft.num_classes = num_classes\n",
    "\n",
    "    return model_ft, 224\n",
    "\n",
    "class NaturePyTorchModule(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 model_name,\n",
    "                 num_classes=10,\n",
    "                 feature_extract=True,\n",
    "                 lr=0.01):\n",
    "        '''method used to define our model parameters'''\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.num_classes = num_classes\n",
    "        self.feature_extract = feature_extract\n",
    "        self.lr = lr\n",
    "        self.model, self.input_size = initialize_model(num_classes=self.num_classes,\n",
    "                                                       feature_extract=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''method used for inference input -> output'''\n",
    "        return self.model(x)\n",
    "\n",
    "def evaluate_model(model, val_dl, idx_to_class, class_names):\n",
    "    device = torch.device(\"cpu\")\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    preds = []\n",
    "    actual = []\n",
    "    \n",
    "    val_table = wandb.Table(columns=['pred', 'actual', 'image'])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_dl:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(\n",
    "                output, target, reduction=\"sum\"\n",
    "            ).item()  # sum up batch loss\n",
    "            pred = output.argmax(\n",
    "                dim=1, keepdim=True\n",
    "            )  # get the index of the max log-probability\n",
    "            preds += list(pred.flatten().tolist())\n",
    "            actual += target.numpy().tolist()\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "            for idx, img in enumerate(data):\n",
    "                img = img.numpy().transpose(1, 2, 0)\n",
    "                pred_class = idx_to_class[pred.numpy()[idx][0]]\n",
    "                target_class = idx_to_class[target.numpy()[idx]]\n",
    "                val_table.add_data(pred_class, target_class, wandb.Image(img))\n",
    "\n",
    "    test_loss /= len(val_dl.dataset)\n",
    "    accuracy = 100.0 * correct / len(val_dl.dataset)\n",
    "    conf_mat = wandb.plot.confusion_matrix(y_true=actual, preds=preds, class_names=class_names)\n",
    "    return test_loss, accuracy, preds, val_table, conf_mat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking the Training Loop\n",
    "During training, it is a best practice to checkpoint your models overtime, so if training gets interrupted or your instance crashes you can resume from where you left off. With artifact logging, we can track all our checkpoints with W&B and attach any metadata we want (like format of serialization, class labels, etc.). That way, when someone needs to consume a checkpoint they know how to use it. When logging models of any form as artifacts, ensure to set the `type` of the artifact to `model`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%wandb -h 600\n",
    "\n",
    "run = wandb.init(project=PROJECT_NAME,\n",
    "                     entity=ENTITY,\n",
    "                     job_type='training',\n",
    "                     config={'model_type': MODEL_TYPE,\n",
    "                             'lr': 1.0,\n",
    "                             'gamma': 0.75,\n",
    "                             'batch_size': 16,\n",
    "                             'epochs': 5})\n",
    "\n",
    "model = NaturePyTorchModule(wandb.config['model_type'])\n",
    "\n",
    "wandb.config['input_size'] = 224\n",
    "\n",
    "dls = Dataloaders(artifact_name_alias=f\"{DATASET_NAME}:latest\",\n",
    "                  batch_size=wandb.config['batch_size'],\n",
    "                  input_size=wandb.config['input_size'])\n",
    "\n",
    "# Train the model\n",
    "learning_rate = wandb.config[\"lr\"]\n",
    "gamma = wandb.config[\"gamma\"]\n",
    "epochs = wandb.config[\"epochs\"]\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=wandb.config['lr'])\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=wandb.config['gamma'])\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "best_model = None\n",
    "\n",
    "for epoch_ndx in range(epochs):\n",
    "    model.train()\n",
    "    for batch_ndx, batch in enumerate(dls.train):\n",
    "        data, target = batch[0].to(\"cpu\"), batch[1].to(\"cpu\")\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(data)\n",
    "        loss = F.cross_entropy(preds, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        ### Log your metrics ###\n",
    "        wandb.log({\n",
    "            \"train/epoch_ndx\": epoch_ndx,\n",
    "            \"train/batch_ndx\": batch_ndx,\n",
    "            \"train/train_loss\": loss,\n",
    "            \"train/learning_rate\": optimizer.param_groups[0][\"lr\"]\n",
    "        })\n",
    "        print(f\"Epoch: {epoch_ndx}, Batch: {batch_ndx}, Loss: {loss}\")\n",
    "\n",
    "    ### Evaluation at the end of each epoch ###\n",
    "    test_loss, accuracy, preds, val_table, conf_mat = evaluate_model(\n",
    "        model,\n",
    "        dls.valid, \n",
    "        dls.dataset.idx_to_class,\n",
    "        dls.dataset.class_names,\n",
    "    )\n",
    "\n",
    "    is_best = test_loss < best_loss\n",
    "\n",
    "    wandb.log({\n",
    "        'eval/test_loss': test_loss,\n",
    "        'eval/accuracy': accuracy,\n",
    "        'eval/conf_mat': conf_mat,\n",
    "        'eval/val_table': val_table})\n",
    "\n",
    "  ### Checkpoing your model weights ###\n",
    "    torch.save(model.state_dict(), \"model.pth\")\n",
    "    art = wandb.Artifact(f\"nature-{wandb.run.id}\", \n",
    "                        type=\"model\",\n",
    "                        metadata={'format': 'onnx',\n",
    "                                  'num_classes': len(dls.dataset.class_names),\n",
    "                                  'model_type': wandb.config['model_type'],\n",
    "                                  'model_input_size': wandb.config['input_size'],\n",
    "                                  'index_to_class': dls.dataset.idx_to_class})\n",
    "\n",
    "    art.add_file(\"model.pth\")\n",
    "\n",
    "    ### Add aliases to keep track of your best checkpoints over time\n",
    "    wandb.log_artifact(art, aliases=[\"best\", \"latest\"] if is_best else None)\n",
    "    if is_best:\n",
    "        best_model = art"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manage all your model checkpoints for a project under one roof. \n",
    "\n",
    "![api_token](https://drive.google.com/uc?export=view&id=1z7nXRgqHTPYjfR1SoP-CkezyxklbAZlM)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Registry \n",
    "After logging a bunch of checkpoints across multiple runs during experimentation, now comes time to hand-off the best checkpoint to the next stage of the workflow (e.g. testing, deployment). \n",
    "\n",
    "The Model Registry is a central page that lives above individual W&B projects. It houses **Registered Models**, portfolios that store \"links\" to the valuable checkpoints living in individual W&B Projects. \n",
    "\n",
    "The model registry offers a centralized place to house the best checkpoints for all your model tasks. Any `model` artifact you log can be \"linked\" to a Registered Model. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating **Registered Models** and Linking through the UI\n",
    "#### 1. Access your team's model registry by going the team page and selecting `Model Registry`\n",
    "\n",
    "![model registry](https://drive.google.com/uc?export=view&id=1ZtJwBsFWPTm4Sg5w8vHhRpvDSeQPwsKw)\n",
    "\n",
    "#### 2. Create a new Registered Model. \n",
    "\n",
    "![model registry](https://drive.google.com/uc?export=view&id=1RuayTZHNE0LJCxt1t0l6-2zjwiV4aDXe)\n",
    "\n",
    "#### 3. Go to the artifacts tab of the project that holds all your model checkpoints\n",
    "\n",
    "![model registry](https://drive.google.com/uc?export=view&id=1LfTLrRNpBBPaUb_RmBIE7fWFMG0h3e0E)\n",
    "\n",
    "#### 4. Click \"Link to Registry\" for the model artifact version you want. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Registered Models and Linking through the **API**\n",
    "You can [link a model via api](https://docs.wandb.ai/guides/models) with `wandb.run.link_artifact` passing in the artifact object, and the name of the **Registered Model**, along with aliases you want to append to it. **Registered Models** are entity (team) scoped in W&B so only members of a team can see and access the **Registered Models** there. You indicate a registered model name via api with `<entity>/model-registry/<registered-model-name>`. If a Registered Model doesn't exist, one will be created automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENTITY is not None:\n",
    "  wandb.run.link_artifact(best_model, f'{ENTITY}/model-registry/Model Registry Tutorial', aliases=['staging'])\n",
    "else:\n",
    "  print('Must indicate entity where Registered Model will exist')\n",
    "wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is \"Linking\"?\n",
    "When you link to the registry, this creates a new version of that Registered Model, which is just a pointer to the artifact version living in that project. There's a reason W&B segregates the versioning of artifacts in a project from the versioning of a Registered Model. The process of linking a model artifact version is equivalent to \"bookmarking\" that artifact version under a Registered Model task. \n",
    "\n",
    "Typically during R&D/experimentation, researchers generate 100s, if not 1000s of model checkpoint artifacts, but only one or two of them actually \"see the light of day.\" This process of linking those checkpoints to a separate, versioned registry helps delineate the model development side from the model deployment/consumption side of the workflow. The globally understood version/alias of a model should be unpolluted from all the experimental versions being generated in R&D and thus the versioning of a Registered Model increments according to new \"bookmarked\" models as opposed to model checkpoint logging. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Centralized Hub for all your models\n",
    "- Add a model card, tags, slack notifactions to your Registered Model\n",
    "- Change aliases to reflect when models move through different phases\n",
    "- Embed the model registry in reports for model documentation and regression reports. See this report as an [example](https://api.wandb.ai/links/wandb-smle/r82bj9at)\n",
    "![model registry](https://drive.google.com/uc?export=view&id=1lKPgaw-Ak4WK_91aBMcLvUMJL6pDQpgO)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Slack Notifications when new models get linked to the registry\n",
    "\n",
    "![model registry](https://drive.google.com/uc?export=view&id=1RsWCa6maJYD5y34gQ0nwWiKSWUCqcjT9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consuming a Registered Model\n",
    "You now can consume any registered model via API by referring the corresponding `name:alias`. Model consumers, whether they are engineers, researchers, or CI/CD processes, can go to the model registry as the central hub for all models that should \"see the light of day\": those that need to go through testing or move to production. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%wandb -h 600\n",
    "\n",
    "run = wandb.init(project=PROJECT_NAME, entity=ENTITY, job_type='inference')\n",
    "artifact = run.use_artifact(f'{ENTITY}/model-registry/Model Registry Tutorial:staging', type='model')\n",
    "artifact_dir = artifact.download()\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
