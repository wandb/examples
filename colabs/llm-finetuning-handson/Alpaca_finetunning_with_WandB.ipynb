{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c7c21b5-4457-481f-b2cc-fb20cdcbfbe3"
      },
      "source": [
        "<h1> From Llama to Alpaca: Finetunning and LLM with Weights & Biases </h1>\n",
        "In this notebooks you will learn how to finetune a model on an Instruction dataset. We will use an updated version of the Alpaca dataset that, instead of davinci-003 (GPT3) generations uses GPT4 to get an even better instruction dataset!\n",
        "\n",
        "<a href=\"https://colab.research.google.com/drive/1bprbJ4HAKEg_1AGse6cmK7-xkMauuNoh?usp=sharing\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "<!--- @wandbcode{mmdetection-wandb-colab} -->\n",
        "\n",
        "original github: https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM#how-good-is-the-data"
      ],
      "id": "3c7c21b5-4457-481f-b2cc-fb20cdcbfbe3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03ef319f-bf26-4192-8951-8d536181ab67"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git\n",
        "!pip install -q wandb\n",
        "!pip install -q ctranslate2\n",
        "!pip install -q bitsandbytes datasets accelerate loralib"
      ],
      "id": "03ef319f-bf26-4192-8951-8d536181ab67"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-psLFcagZD0D"
      },
      "outputs": [],
      "source": [
        "import bitsandbytes as bnb\n",
        "import copy\n",
        "import glob\n",
        "import os\n",
        "import wandb\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from types import SimpleNamespace\n",
        "import datasets\n",
        "from datasets import Dataset\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, Trainer, TrainingArguments, default_data_collator,GenerationConfig\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import pandas as pd\n",
        "from peft import PeftModel, PeftConfig, LoraConfig, get_peft_model\n",
        "\n",
        "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
      ],
      "id": "-psLFcagZD0D"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7804f904-5746-4530-867d-c766f4501dea"
      },
      "source": [
        "## Prepare your Instruction Dataset"
      ],
      "id": "7804f904-5746-4530-867d-c766f4501dea"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59bd7517-70d9-4dee-9f92-1a2891caf385"
      },
      "source": [
        "An Instruction dataset is a list of instructions/outputs pairs that are relevant to your own domain. For instance it could be question and answers from an specific domain, problems and solution for a technical domain, or just instruction and outputs.\n",
        "\n",
        "\n",
        "https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM#how-good-is-the-data\n",
        "\n",
        "So let's explore how one could do this?\n",
        "After grabbing a finetuned model and curated your own dataset, how do I create a dataset that has the right format to fine tune a model?"
      ],
      "id": "59bd7517-70d9-4dee-9f92-1a2891caf385"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da04c0a5-f481-4364-880d-10c254388987"
      },
      "source": [
        "Let's grab the Alpaca (GPT-4 curated instructions and outputs) dataset:"
      ],
      "id": "da04c0a5-f481-4364-880d-10c254388987"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52ff363e-8a24-4085-9b7e-6564d106d2e9"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/main/data/alpaca_gpt4_data.json"
      ],
      "id": "52ff363e-8a24-4085-9b7e-6564d106d2e9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fce67d2-3703-4042-816c-7a13ba9eab3e"
      },
      "outputs": [],
      "source": [
        "dataset_file = \"alpaca_gpt4_data.json\"\n",
        "with open(dataset_file, \"r\") as f:\n",
        "    alpaca = json.load(f)\n",
        "print(alpaca[0])"
      ],
      "id": "0fce67d2-3703-4042-816c-7a13ba9eab3e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e596e369-56aa-4721-9271-6686eed8fb35"
      },
      "source": [
        "So the dataset has instruction and outputs. The model is trained to predict the next token, so one option would be just to concat both, and train on that. We ideally format the prompt in a way that we make explicit where is the input and output. Let's log the dataset to W&B so we keep everything organised"
      ],
      "id": "e596e369-56aa-4721-9271-6686eed8fb35"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqAFsykRoQGh"
      },
      "outputs": [],
      "source": [
        "os.environ[\"WANDB_ENTITY\"]=\"keisuke-kamata\"\n",
        "os.environ[\"WANDB_PROJECT\"]=\"alpaca_finetuning_with_wandb\"\n",
        "wandb.login()"
      ],
      "id": "gqAFsykRoQGh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9786466-4012-4cc4-8f9a-e673c74aa965"
      },
      "outputs": [],
      "source": [
        "# log to wandb\n",
        "with wandb.init():\n",
        "    # log as a table\n",
        "    table = wandb.Table(columns=list(alpaca[0].keys()))\n",
        "    for row in alpaca:\n",
        "        table.add_data(*row.values())\n",
        "    wandb.log({\"alpaca_gpt4_table\": table})\n",
        "\n",
        "    # log file with artifact\n",
        "    artifact = wandb.Artifact(\n",
        "        name=\"alpaca_gpt4\",\n",
        "        type=\"dataset\",\n",
        "        description=\"A GPT4 generated Alpaca like dataset for instruction finetunning\",\n",
        "        metadata={\"url\":\"https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM#how-good-is-the-data\"},\n",
        "    )\n",
        "    artifact.add_file(dataset_file)\n",
        "    wandb.log_artifact(artifact)"
      ],
      "id": "c9786466-4012-4cc4-8f9a-e673c74aa965"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--J-1AEPoodp"
      },
      "source": [
        "Data split"
      ],
      "id": "--J-1AEPoodp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dfa1958-c58d-4b40-a7d9-5e0cc6d2abf5"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "random.shuffle(alpaca)  # this could also be a parameter\n",
        "train_dataset_alpaca = alpaca[:10000]\n",
        "val_dataset_alpaca = alpaca[-1000:]"
      ],
      "id": "5dfa1958-c58d-4b40-a7d9-5e0cc6d2abf5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72fc9e43-55b5-4a7b-902f-b8a3b82dbf06"
      },
      "source": [
        "We should save the split to W&B"
      ],
      "id": "72fc9e43-55b5-4a7b-902f-b8a3b82dbf06"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgJI83G-wKz6"
      },
      "outputs": [],
      "source": [
        "artifact_path = f'{os.environ[\"WANDB_ENTITY\"]}/{os.environ[\"WANDB_PROJECT\"]}/alpaca_gpt4:latest'"
      ],
      "id": "BgJI83G-wKz6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdbc0fd0-de6c-447d-abb9-3176c6ceeaf6"
      },
      "outputs": [],
      "source": [
        "with wandb.init(job_type=\"split_data\") as run:\n",
        "    artifact = run.use_artifact(artifact_path, type='dataset')\n",
        "    #artifact_folder = artifact.download()\n",
        "\n",
        "    train_df = pd.DataFrame(train_dataset_alpaca)\n",
        "    eval_df = pd.DataFrame(val_dataset_alpaca)\n",
        "\n",
        "    train_df.to_json(\"alpaca_gpt4_train.jsonl\", orient='records', lines=True)\n",
        "    eval_df.to_json(\"alpaca_gpt4_eval.jsonl\", orient='records', lines=True)\n",
        "\n",
        "\n",
        "    at = wandb.Artifact(\n",
        "        name=\"alpaca_gpt4_splitted\",\n",
        "        type=\"dataset\",\n",
        "        description=\"A GPT4 generated Alpaca like dataset for instruction finetunning\",\n",
        "        metadata={\"url\":\"https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM#how-good-is-the-data\"},\n",
        "    )\n",
        "    at.add_file(\"alpaca_gpt4_train.jsonl\")\n",
        "    at.add_file(\"alpaca_gpt4_eval.jsonl\")\n",
        "    train_table = wandb.Table(dataframe=train_df)\n",
        "    eval_table  = wandb.Table(dataframe=eval_df)\n",
        "    run.log_artifact(at)\n",
        "    run.log({\"train_dataset\":train_table, \"eval_dataset\":eval_table})"
      ],
      "id": "fdbc0fd0-de6c-447d-abb9-3176c6ceeaf6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26cb96e0-b2f2-4a79-ba65-e1c8d6395d54"
      },
      "source": [
        "Let's log the dataset also as a table so we can inspect it on the workspace."
      ],
      "id": "26cb96e0-b2f2-4a79-ba65-e1c8d6395d54"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb99a7dc-0654-4985-ac3f-60ecdc6f6558"
      },
      "source": [
        "## Train"
      ],
      "id": "fb99a7dc-0654-4985-ac3f-60ecdc6f6558"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2Q7FFhOYRj1"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"BASE_MODEL\":\"facebook/opt-125m\",\n",
        "    \"lora_config\":{\n",
        "        \"r\":32,\n",
        "        \"lora_alpha\":16,\n",
        "        'target_modules': [f\"model.decoder.layers.{i}.self_attn.{proj}_proj\" for i in range(31) for proj in ['q', 'k', 'v']],\n",
        "        \"lora_dropout\":.1,\n",
        "        \"bias\":\"none\",\n",
        "        \"task_type\":\"CAUSAL_LM\"\n",
        "    },\n",
        "    \"training_args\":{\n",
        "        \"dataloader_num_workers\":16,\n",
        "        \"evaluation_strategy\":\"steps\",\n",
        "        \"per_device_train_batch_size\":8,\n",
        "        \"max_steps\": 50,\n",
        "        \"gradient_accumulation_steps\":2,\n",
        "        \"report_to\":\"wandb\",#wandb integration\n",
        "        \"warmup_steps\":10,\n",
        "        \"num_train_epochs\":1,\n",
        "        \"learning_rate\":2e-4,\n",
        "        \"fp16\":True,\n",
        "        \"logging_steps\":10,\n",
        "        \"save_steps\":10,\n",
        "        \"output_dir\":'./outputs'\n",
        "    }\n",
        "}"
      ],
      "id": "Y2Q7FFhOYRj1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFMlXGgmYVh9"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    config[\"BASE_MODEL\"],\n",
        "    #load_in_8bit=True,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(config[\"BASE_MODEL\"])\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "id": "iFMlXGgmYVh9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89XTPJhjhz2H"
      },
      "outputs": [],
      "source": [
        "PROMPT_DICT = {\n",
        "    \"prompt_input\": (\n",
        "        \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
        "        \"### Instruction:{instruction} \\n\\n Input:{input} \\n\\n ###Response\"\n",
        "    ),\n",
        "    \"prompt_no_input\": (\n",
        "        \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
        "        \"### Instruction:{instruction} \\n\\n ###Response\"\n",
        "    )\n",
        "}\n",
        "\n",
        "class InstructDataset(Dataset):\n",
        "    def __init__(self, json_list, tokenizer, ignore_index=-100):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.ignore_index = ignore_index\n",
        "        self.features = []\n",
        "\n",
        "        for j in tqdm(json_list):\n",
        "            if 'input' in j:\n",
        "                source_text = PROMPT_DICT['prompt_input'].format_map(j)\n",
        "            else:\n",
        "                source_text = PROMPT_DICT['prompt_no_input'].format_map(j)\n",
        "            example_text = source_text + j['output'] + self.tokenizer.eos_token\n",
        "\n",
        "            source_tokenized = self.tokenizer(\n",
        "                source_text,\n",
        "                padding='longest',\n",
        "                truncation=True,\n",
        "                max_length=512,\n",
        "                return_length=True,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            example_tokenized = self.tokenizer(\n",
        "                example_text,\n",
        "                padding='longest',\n",
        "                truncation=True,\n",
        "                max_length=512,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            input_ids = example_tokenized['input_ids'][0]\n",
        "            labels = copy.deepcopy(input_ids)\n",
        "            source_len = source_tokenized['length'][0]\n",
        "            labels[:source_len] = self.ignore_index\n",
        "\n",
        "            self.features.append({\n",
        "                'input_ids': input_ids,\n",
        "                'labels': labels\n",
        "            })\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx]\n",
        "\n",
        "\n",
        "class InstructCollator():\n",
        "    def __init__(self, tokenizer, ignore_index=-100):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.ignore_index = -100\n",
        "\n",
        "    def __call__(self, examples):\n",
        "        input_batch = []\n",
        "        label_batch = []\n",
        "        for example in examples:\n",
        "            input_batch.append(example['input_ids'])\n",
        "            label_batch.append(example['labels'])\n",
        "        input_ids = pad_sequence(\n",
        "            input_batch, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
        "        )\n",
        "        labels = pad_sequence(\n",
        "            label_batch, batch_first=True, padding_value=self.ignore_index\n",
        "        )\n",
        "        attention_mask = input_ids.ne(self.tokenizer.pad_token_id)\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'labels': labels,\n",
        "            'attention_mask': attention_mask\n",
        "        }\n",
        "\n",
        "\n",
        "train_dataset = InstructDataset(train_dataset_alpaca, tokenizer)\n",
        "val_dataset = InstructDataset(val_dataset_alpaca , tokenizer)\n",
        "\n",
        "# Create the collator with the device\n",
        "collator = InstructCollator(tokenizer, ignore_index=-100)"
      ],
      "id": "89XTPJhjhz2H"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1COOZoWsoHj"
      },
      "outputs": [],
      "source": [
        "# cast the small parameters (e.g. layernorm) to fp32 for stability\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False # freeze the model - train adapters later\n",
        "    if param.ndim == 1:\n",
        "        param.data = param.data.to(torch.float32)\n",
        "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
        "model.enable_input_require_grads()\n",
        "class CastOutputToFloat(nn.Sequential):\n",
        "    def forward(self, x): return super().forward(x).to(torch.float32)\n",
        "model.lm_head = CastOutputToFloat(model.lm_head)"
      ],
      "id": "x1COOZoWsoHj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoAiDU3c_xYG"
      },
      "outputs": [],
      "source": [
        "path_dataset_for_trainig = f'{os.environ[\"WANDB_ENTITY\"]}/{os.environ[\"WANDB_PROJECT\"]}/alpaca_gpt4_splitted:latest'"
      ],
      "id": "WoAiDU3c_xYG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cttlNl_3XkgF"
      },
      "outputs": [],
      "source": [
        "with wandb.init(config=config, job_type=\"training\") as run:\n",
        "    # track data\n",
        "    run.use_artifact(path_dataset_for_trainig)\n",
        "    # Setup for LoRa\n",
        "    lora_config = LoraConfig(**wandb.config[\"lora_config\"])\n",
        "    model_peft = get_peft_model(model, lora_config)\n",
        "    model_peft.print_trainable_parameters()\n",
        "    model_peft.config.use_cache = False\n",
        "\n",
        "    trainer = transformers.Trainer(\n",
        "        model=model_peft,\n",
        "        data_collator= collator,\n",
        "        args=transformers.TrainingArguments(**wandb.config[\"training_args\"]),\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset\n",
        "    )\n",
        "\n",
        "\n",
        "    trainer.train()\n",
        "    run.log_code()"
      ],
      "id": "cttlNl_3XkgF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a936e007-47fa-400f-873c-5e038bd685c5"
      },
      "source": [
        "## Full Eval Dataset evaluation"
      ],
      "id": "a936e007-47fa-400f-873c-5e038bd685c5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "399fafb9-b41f-401b-a1c4-37e1ede2e639"
      },
      "source": [
        "Let's log a table with model predictions on the eval_dataset (or at least the 250 first samples)"
      ],
      "id": "399fafb9-b41f-401b-a1c4-37e1ede2e639"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjDqorpCB7K-"
      },
      "outputs": [],
      "source": [
        "def create_prompt(row):\n",
        "    return prompt_no_input(row) if row[\"input\"] == \"\" else prompt_input(row)\n",
        "\n",
        "def prompt_no_input(row):\n",
        "    return (\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
        "        \"### Instruction:{instruction} \\n\\n ###Response\").format_map(row)\n",
        "def prompt_input(row):\n",
        "    return (\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
        "        \"### Instruction:{instruction} \\n\\n Input:{input} \\n\\n ###Response\").format_map(row)\n",
        "\n",
        "def pad_eos(ds):\n",
        "    EOS_TOKEN = \"</s>\"\n",
        "    return [f\"{row['output']}{EOS_TOKEN}\" for row in ds]\n",
        "\n",
        "eval_prompts = [create_prompt(row) for row in val_dataset_alpaca]\n",
        "eval_outputs = pad_eos(val_dataset_alpaca)\n",
        "eval_dataset = [{\"prompt\":s, \"output\":t, \"example\": s + t} for s, t in zip(eval_prompts, eval_outputs)]"
      ],
      "id": "OjDqorpCB7K-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6gVu8YEBXWF"
      },
      "outputs": [],
      "source": [
        "model_artifact_path ='' # change here!"
      ],
      "id": "b6gVu8YEBXWF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3b89717a-ac70-43e8-9b7c-edd8d2c54cdc"
      },
      "outputs": [],
      "source": [
        "gen_config = GenerationConfig.from_pretrained(config[\"BASE_MODEL\"])\n",
        "test_config = SimpleNamespace(\n",
        "    max_new_tokens=256,\n",
        "    gen_config=gen_config)\n",
        "\n",
        "def prompt_table(examples, log=False, table_name=\"predictions\"):\n",
        "    table = wandb.Table(columns=[\"prompt\", \"generation\", \"concat\", \"GPT-4 output\"])\n",
        "    for example in tqdm(examples, leave=False):\n",
        "        prompt, gpt4_output = example[\"prompt\"], example[\"output\"]\n",
        "        out = generate(prompt, test_config.max_new_tokens, test_config.gen_config)\n",
        "        table.add_data(prompt, out, prompt+out, gpt4_output)\n",
        "    if log:\n",
        "        wandb.log({table_name:table})\n",
        "    return table\n",
        "\n",
        "def generate(prompt, max_new_tokens=test_config.max_new_tokens, gen_config=gen_config):\n",
        "    tokenized_prompt = tokenizer(prompt, return_tensors='pt')['input_ids'].cuda()\n",
        "    with torch.inference_mode():\n",
        "        output = model.generate(tokenized_prompt,\n",
        "                                max_new_tokens=max_new_tokens,\n",
        "                                generation_config=gen_config,\n",
        "                                temperature=0.9,\n",
        "                                top_k=40,\n",
        "                                top_p=0.70,\n",
        "                                do_sample=True)\n",
        "    return tokenizer.decode(output[0][len(tokenized_prompt[0]):], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "with wandb.init(entity=wandb_entity,\n",
        "           project=wandb_project,\n",
        "           job_type=\"eval\",\n",
        "           config=config):\n",
        "\n",
        "    artifact = wandb.use_artifact(model_artifact_path)\n",
        "    artifact_dir = artifact.download()\n",
        "\n",
        "    merged_model = PeftModel.from_pretrained(model, artifact_dir)\n",
        "    merged_model = merged_model.merge_and_unload()\n",
        "\n",
        "    merged_model.eval();\n",
        "    prompt_table(eval_dataset[:10], log=True, table_name=\"eval_predictions\")"
      ],
      "id": "3b89717a-ac70-43e8-9b7c-edd8d2c54cdc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQqpBZXAG0dj"
      },
      "source": [
        "# (Advanced) Sweep"
      ],
      "id": "NQqpBZXAG0dj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOgTNIydwcwn"
      },
      "outputs": [],
      "source": [
        "#wandb.sdk.wandb_setup._setup(_reset=True)"
      ],
      "id": "sOgTNIydwcwn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9oEu7S6BfQe"
      },
      "outputs": [],
      "source": [
        "sweep_configuration= {\n",
        "    \"method\": \"random\",\n",
        "    \"metric\": {\"goal\": \"minimize\", \"name\": \"eval/loss\"},\n",
        "    \"parameters\": {\n",
        "        \"r\":{\"values\": [2,4,8,16,32]},\n",
        "        \"lora_alpha\":{\"values\": [2,4,8,16]},\n",
        "        \"learning_rate\":{'max': 2e-3, 'min': 2e-4}\n",
        "    }\n",
        "}\n",
        "\n",
        "default_config = {\n",
        "    \"BASE_MODEL\":\"facebook/opt-125m\",\n",
        "    \"lora_config\":{\n",
        "        \"r\":32,\n",
        "        \"lora_alpha\":16,\n",
        "        \"target_modules\":[f\"model.decoder.layers.{i}.self_attn.{proj}_proj\" for i in range(31) for proj in ['q', 'k', 'v']],\n",
        "        \"lora_dropout\":.1,\n",
        "        \"bias\":\"none\",\n",
        "        \"task_type\":\"CAUSAL_LM\"\n",
        "    },\n",
        "    \"training_args\":{\n",
        "        \"dataloader_num_workers\":16,\n",
        "        \"evaluation_strategy\":\"steps\",\n",
        "        \"per_device_train_batch_size\":8,\n",
        "        \"max_steps\": 50,\n",
        "        \"gradient_accumulation_steps\":2,\n",
        "        \"report_to\":\"wandb\",#wandb integration\n",
        "        \"warmup_steps\":10,\n",
        "        \"num_train_epochs\":1,\n",
        "        \"learning_rate\":2e-4,\n",
        "        \"fp16\":True,\n",
        "        \"logging_steps\":10,\n",
        "        \"save_steps\":10,\n",
        "        \"output_dir\":'./outputs'\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "def train_func():\n",
        "    with wandb.init(project=wandb_project, config=config, job_type=\"training\") as run:\n",
        "        # Setup for LoRa\n",
        "        run.use_artifact(path_dataset_for_trainig)\n",
        "\n",
        "        default_config[\"lora_config\"][\"r\"] = wandb.config[\"r\"]\n",
        "        default_config[\"lora_config\"][\"lora_alpha\"] = wandb.config[\"lora_alpha\"]\n",
        "        default_config[\"training_args\"][\"learning_rate\"] = wandb.config[\"learning_rate\"]\n",
        "\n",
        "        lora_config = LoraConfig(**default_config[\"lora_config\"])\n",
        "        model_peft = get_peft_model(model, lora_config)\n",
        "        model_peft.print_trainable_parameters()\n",
        "        model_peft.config.use_cache = False\n",
        "\n",
        "        trainer = transformers.Trainer(\n",
        "            model=model_peft,\n",
        "            data_collator=collator,\n",
        "            args=transformers.TrainingArguments(**default_config[\"training_args\"]),\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=val_dataset\n",
        "        )\n",
        "        trainer.train()\n",
        "        run.log_code()\n",
        "\n",
        "sweep_id = wandb.sweep(sweep=sweep_configuration)\n",
        "wandb.agent(sweep_id, function=train_func, count=20)"
      ],
      "id": "-9oEu7S6BfQe"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}