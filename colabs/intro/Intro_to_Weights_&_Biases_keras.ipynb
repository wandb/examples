{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/intro/Intro_to_Weights_&_Biases_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "<!--- @wandbcode{intro-colab-keras} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
    "\n",
    "<!--- @wandbcode{intro-colab-keras} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèÉ‚Äç‚ôÄÔ∏è Quickstart\n",
    "Use **[Weights & Biases](https://wandb.ai/site?utm_source=keras_intro_colab&utm_medium=code&utm_campaign=keras_intro)** for machine learning experiment tracking, model checkpointing, and collaboration with your team. See the full Weights & Biases Documentation **[here](https://docs.wandb.ai/guides/integrations/keras)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§© A shared dashboard for your experiments\n",
    "\n",
    "With just a few lines of code,\n",
    "you'll get rich, interactive, shareable dashboards [which you can see yourself here](https://wandb.ai/wandb/wandb_example).\n",
    "![](https://i.imgur.com/Pell4Oo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üîí Data & Privacy\n",
    "\n",
    "We take security very seriously, and our cloud-hosted dashboard uses industry standard best practices for encryption. If you're working with datasets that cannot leave your enterprise cluster, we have [on-prem](https://docs.wandb.com/self-hosted) installations available.\n",
    "\n",
    "It's also easy to download all your data and export it to other tools ‚Äî like custom analysis in a Jupyter notebook. Here's [more on our API](https://docs.wandb.com/library/api).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by installing the library and logging in to your free account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in to your W&B account\n",
    "import wandb\n",
    "\n",
    "# Use wandb-core\n",
    "wandb.require(\"core\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üëü Run an experiment\n",
    "1Ô∏è‚É£. **Start a new run** and pass in hyperparameters to track\n",
    "\n",
    "2Ô∏è‚É£. **Log metrics** from training or evaluation\n",
    "\n",
    "3Ô∏è‚É£. **Visualize results** in the dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Launch 5 simulated experiments\n",
    "for run in range(5):\n",
    "    # 1Ô∏è‚É£ Start a new run to track this script\n",
    "    wandb.init(\n",
    "        # Set entity to specify your username or team name\n",
    "        # ex: entity=\"carey\",\n",
    "        # Set the project where this run will be logged\n",
    "        project=\"basic-intro\",\n",
    "        # Track hyperparameters and run metadata\n",
    "        config={\n",
    "            \"learning_rate\": 0.02,\n",
    "            \"architecture\": \"CNN\",\n",
    "            \"dataset\": \"CIFAR-100\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # This simple block simulates a training loop logging metrics\n",
    "    offset = random.random() / 5\n",
    "    for ii in range(2, 10):\n",
    "        acc = 1 - 2**-ii - random.random() / ii - offset\n",
    "        loss = 2**-ii + random.random() / ii + offset\n",
    "        # 2Ô∏è‚É£ Log metrics from your script to W&B\n",
    "        wandb.log({\"acc\": acc, \"loss\": loss})\n",
    "\n",
    "    # Mark the run as finished\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now trained your first model using wandb! üëÜ Click on the wandb link above to see your metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü•ï Simple Keras Classifier\n",
    "Run this model to train a simple MNIST classifier, and click on the project page link to see your results stream in live to a W&B project. For a full guide on how to use Weights & Biases with Keras, **[see here](https://docs.wandb.ai/guides/integrations/keras)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
    "\n",
    "# Simple Keras Model\n",
    "\n",
    "# Launch 5 experiments, trying different dropout rates\n",
    "for run in range(5):\n",
    "    # Start a run, tracking hyperparameters\n",
    "    wandb.init(\n",
    "        project=\"keras-intro\",\n",
    "        # (optional) set entity to specify your username or team name\n",
    "        # entity=\"my_team\",\n",
    "        config={\n",
    "            \"layer_1\": 512,\n",
    "            \"activation_1\": \"relu\",\n",
    "            \"dropout\": random.uniform(0.01, 0.80),\n",
    "            \"layer_2\": 10,\n",
    "            \"activation_2\": \"softmax\",\n",
    "            \"optimizer\": \"sgd\",\n",
    "            \"loss\": \"sparse_categorical_crossentropy\",\n",
    "            \"metric\": \"accuracy\",\n",
    "            \"epoch\": 6,\n",
    "            \"batch_size\": 256,\n",
    "        },\n",
    "    )\n",
    "    config = wandb.config\n",
    "\n",
    "    # Get the data\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "    x_train, y_train = x_train[::5], y_train[::5]  # Subset data for a faster demo\n",
    "    x_test, y_test = x_test[::20], y_test[::20]\n",
    "    labels = [str(digit) for digit in range(np.max(y_train) + 1)]\n",
    "\n",
    "    # Build a model\n",
    "    model = tf.keras.models.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "            tf.keras.layers.Dense(config.layer_1, activation=config.activation_1),\n",
    "            tf.keras.layers.Dropout(config.dropout),\n",
    "            tf.keras.layers.Dense(config.layer_2, activation=config.activation_2),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model.compile(optimizer=config.optimizer, loss=config.loss, metrics=[config.metric])\n",
    "\n",
    "    # Add WandbMetricsLogger to log metrics and WandbModelCheckpoint to log model checkpoints\n",
    "    wandb_callbacks = [\n",
    "        WandbMetricsLogger(),\n",
    "        WandbModelCheckpoint(filepath=\"my_model_{epoch:02d}\"),\n",
    "    ]\n",
    "\n",
    "    model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        epochs=config.epoch,\n",
    "        batch_size=config.batch_size,\n",
    "        validation_data=(x_test, y_test),\n",
    "        callbacks=wandb_callbacks,\n",
    "    )\n",
    "\n",
    "    # Mark the run as finished\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now trained your first model using wandb! üëÜ Click on the wandb link above to see your metrics.\n",
    "\n",
    "For a full guide on how to use Weights & Biases with Keras, **[see here](https://docs.wandb.ai/guides/integrations/keras)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîî Try W&B Alerts\n",
    "\n",
    "**[W&B Alerts](https://docs.wandb.ai/guides/track/alert)** allows you to send alerts, triggered from your Python code, to your Slack or email. There are 2 steps to follow the first time you'd like to send a Slack or email alert, triggered from your code:\n",
    "\n",
    "1) Turn on Alerts in your W&B [User Settings](https://wandb.ai/settings)\n",
    "\n",
    "2) Add `wandb.alert()` to your code:\n",
    "\n",
    "```python\n",
    "wandb.alert(\n",
    "    title=\"Low accuracy\",\n",
    "    text=f\"Accuracy is below the acceptable threshold\"\n",
    ")\n",
    "```\n",
    "\n",
    "See the minimal example below to see how to use `wandb.alert`. You can find the full docs for **[W&B Alerts here](https://docs.wandb.ai/guides/track/alert)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a wandb run\n",
    "wandb.init(project=\"keras-intro\")\n",
    "\n",
    "# Simulating a model training loop\n",
    "acc_threshold = 0.3\n",
    "for training_step in range(1000):\n",
    "\n",
    "    # Generate a random number for accuracy\n",
    "    accuracy = round(random.random() + random.random(), 3)\n",
    "    print(f\"Accuracy is: {accuracy}, {acc_threshold}\")\n",
    "\n",
    "    # üêù Log accuracy to wandb\n",
    "    wandb.log({\"Accuracy\": accuracy})\n",
    "\n",
    "    # üîî If the accuracy is below the threshold, fire a W&B Alert and stop the run\n",
    "    if accuracy <= acc_threshold:\n",
    "        # üêù Send the wandb Alert\n",
    "        wandb.alert(\n",
    "            title=\"Low Accuracy\",\n",
    "            text=f\"Accuracy {accuracy} at step {training_step} is below the acceptable theshold, {acc_threshold}\",\n",
    "        )\n",
    "        print(\"Alert triggered\")\n",
    "        break\n",
    "\n",
    "# Mark the run as finished (useful in Jupyter notebooks)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# What's next üöÄ ?\n",
    "The next tutorial you will learn how to do hyperparameter optimization using W&B Sweeps:\n",
    "## üëâ [Hyperparameters sweeps using PyTorch](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W%26B.ipynb)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
