{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72koFupIXP8x"
   },
   "source": [
    "<img src=\"http://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
    "<!--- @wandbcode{intro-colab} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/intro/Intro_to_Weights_&_Biases.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# üèÉ‚Äç‚ôÄÔ∏è Quickstart\n",
    "Use [Weights & Biases](https://wandb.ai) for machine learning experiment tracking, dataset versioning, and project collaboration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5iWuxMsTXi7k"
   },
   "source": [
    "## ü§© A shared dashboard for your experiments\n",
    "\n",
    "With just a few lines of code,\n",
    "you'll get rich, interactive, shareable dashboards [which you can see yourself here](https://wandb.ai/wandb/wandb_example).\n",
    "![](https://i.imgur.com/Pell4Oo.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ru4MPZP6XjAY"
   },
   "source": [
    "\n",
    "## üîí Data & Privacy\n",
    "\n",
    "We take security very seriously, and our cloud-hosted dashboard uses industry standard best practices for encryption. If you're working with datasets that cannot leave your enterprise cluster, we have [on-prem](https://docs.wandb.com/self-hosted) installations available. \n",
    "\n",
    "It's also easy to download all your data and export it to other tools ‚Äî like custom analysis in a Jupyter notebook. Here's [more on our API](https://docs.wandb.com/library/api).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmUqfZqaYRG-"
   },
   "source": [
    "## ü™Ñ Install `wandb` library and login\n",
    "\n",
    "\n",
    "Start by installing the library and logging in to your free account.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7WNIfYF2XbSo"
   },
   "outputs": [],
   "source": [
    "!pip install wandb -qqq\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93ub0LzeYwUZ"
   },
   "outputs": [],
   "source": [
    "# Log in to your W&B account\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lVohJSCaYMU9"
   },
   "source": [
    "## üëü Run an experiment\n",
    "1Ô∏è‚É£. **Start a new run** and pass in hyperparameters to track\n",
    "\n",
    "2Ô∏è‚É£. **Log metrics** from training or evaluation\n",
    "\n",
    "3Ô∏è‚É£. **Visualize results** in the dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X0uayNOy7AKP"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Launch 5 simulated experiments\n",
    "total_runs = 5\n",
    "for run in range(total_runs):\n",
    "  # 1Ô∏è‚É£ Start a new run to track this script\n",
    "  wandb.init(\n",
    "      # Set the project where this run will be logged\n",
    "      project=\"basic-intro\", \n",
    "      # Track hyperparameters and run metadata\n",
    "      config={\n",
    "      \"learning_rate\": 0.02,\n",
    "      \"architecture\": \"CNN\",\n",
    "      \"dataset\": \"CIFAR-100\",\n",
    "      \"epochs\": 10,\n",
    "      })\n",
    "  \n",
    "  # This simple block simulates a training loop logging metrics\n",
    "  epochs = 10\n",
    "  offset = random.random() / 5\n",
    "  for epoch in range(2, epochs):\n",
    "      acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n",
    "      loss = 2 ** -epoch + random.random() / epoch + offset\n",
    "      # 2Ô∏è‚É£ Log metrics from your script to W&B\n",
    "      wandb.log({\"acc\": acc, \"loss\": loss})\n",
    "      \n",
    "  # Mark the run as finished\n",
    "  wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCGgaTB92ENz"
   },
   "source": [
    "3Ô∏è‚É£ You can find your interactive dashboard by clicking any of the  üëÜ wandb links above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1nG5CWk7L02"
   },
   "source": [
    "# üî• Simple Pytorch Neural Network\n",
    "\n",
    "üí™ Run this model to train a simple MNIST classifier, and click on the project page link to see your results stream in live to a W&B project.\n",
    "\n",
    "\n",
    "Any run in `wandb` automatically logs [metrics](https://docs.wandb.ai/ref/app/pages/run-page#charts-tab),\n",
    "[system information](https://docs.wandb.ai/ref/app/pages/run-page#system-tab),\n",
    "[hyperparameters](https://docs.wandb.ai/ref/app/pages/run-page#overview-tab),\n",
    "[terminal output](https://docs.wandb.ai/ref/app/pages/run-page#logs-tab) and\n",
    "you'll see an [interactive table](https://docs.wandb.ai/guides/data-vis)\n",
    "with model inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "l6lzQ8iNaP7i"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "import wandb\n",
    "import random\n",
    "import torch, torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def get_dataloader(is_train, batch_size, slice=5):\n",
    "    \"Get a training dataloader\"\n",
    "    full_dataset = torchvision.datasets.MNIST(root=\".\", train=is_train, transform=T.ToTensor(), download=True)\n",
    "    sub_dataset = torch.utils.data.Subset(full_dataset, indices=range(0, len(full_dataset), slice))\n",
    "    loader = torch.utils.data.DataLoader(dataset=sub_dataset, \n",
    "                                         batch_size=batch_size, \n",
    "                                         shuffle=True if is_train else False, \n",
    "                                         pin_memory=True, num_workers=2)\n",
    "    return loader\n",
    "\n",
    "def get_model(dropout):\n",
    "    \"A simple model\"\n",
    "    model = nn.Sequential(nn.Flatten(),\n",
    "                         nn.Linear(28*28, 256),\n",
    "                         nn.BatchNorm1d(256),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Dropout(dropout),\n",
    "                         nn.Linear(256,10)).to(device)\n",
    "    return model\n",
    "\n",
    "def validate_model(model, valid_dl, loss_func, log_images=False, batch_idx=0):\n",
    "    \"Compute performance of the model on the validation dataset and log a wandb.Table\"\n",
    "    model.eval()\n",
    "    val_loss = 0.\n",
    "    with torch.inference_mode():\n",
    "        correct = 0\n",
    "        for i, (images, labels) in tqdm(enumerate(valid_dl), leave=False):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass ‚û°\n",
    "            outputs = model(images)\n",
    "            val_loss += loss_func(outputs, labels)*labels.size(0)\n",
    "\n",
    "            # Compute accuracy and accumulate\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Log one batch of images to the dashboard, always same batch_idx.\n",
    "            if i==batch_idx and log_images:\n",
    "                log_image_table(images, predicted, labels, outputs.softmax(dim=1))\n",
    "    return val_loss / len(valid_dl.dataset), correct / len(valid_dl.dataset)\n",
    "\n",
    "def log_image_table(images, predicted, labels, probs):\n",
    "    \"Log a wandb.Table with (img, pred, target, scores)\"\n",
    "    table = wandb.Table(columns=[\"image\", \"pred\", \"target\"]+[f\"score_{i}\" for i in range(10)])\n",
    "    for img, pred, targ, prob in zip(images.to(\"cpu\"), predicted.to(\"cpu\"), labels.to(\"cpu\"), probs.to(\"cpu\")):\n",
    "        table.add_data(wandb.Image(img[0].numpy()*255), pred, targ, *prob.numpy())\n",
    "    wandb.log({\"predictions_table\":table}, commit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8xiyDcsJu-hX"
   },
   "outputs": [],
   "source": [
    "# Launch 10 experiments, trying different dropout rates\n",
    "for _ in range(10):\n",
    "    with wandb.init(\n",
    "        project=\"pytorch-intro\",\n",
    "        config={\n",
    "            \"epochs\": 10,\n",
    "            \"batch_size\": 128,\n",
    "            \"lr\": 1e-3,\n",
    "            \"dropout\": random.uniform(0.01, 0.80),\n",
    "            }):\n",
    "        config = wandb.config\n",
    "\n",
    "        # Get the data\n",
    "        train_dl = get_dataloader(is_train=True, batch_size=config.batch_size)\n",
    "        valid_dl = get_dataloader(is_train=False, batch_size=2*config.batch_size)\n",
    "\n",
    "        # A simple MLP model\n",
    "        model = get_model(config.dropout)\n",
    "\n",
    "        # Make the loss and optimizer\n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "\n",
    "        # Training\n",
    "        example_ct = 0\n",
    "        for epoch in tqdm(range(config.epochs)):\n",
    "            model.train()\n",
    "            for images, labels in tqdm(train_dl, leave=False):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "                # Forward pass ‚û°\n",
    "                outputs = model(images)\n",
    "                train_loss = loss_func(outputs, labels)\n",
    "\n",
    "                # Backward pass ‚¨Ö\n",
    "                optimizer.zero_grad()\n",
    "                train_loss.backward()\n",
    "                \n",
    "                # Step with optimizer\n",
    "                optimizer.step()\n",
    "\n",
    "                # Log training loss and epoch count\n",
    "                example_ct += len(images)\n",
    "                wandb.log({\"train_loss\": train_loss, \"epoch\": example_ct/len(train_dl.dataset)}, step=example_ct)\n",
    "            val_loss, accuracy = validate_model(model, valid_dl, loss_func, log_images=(epoch==(config.epochs-1)))\n",
    "            \n",
    "            # Log validation metrics\n",
    "            wandb.log({\"val_loss\": val_loss, \"val_accuracy\": accuracy}, step=example_ct)\n",
    "            print(f\"Train Loss: {train_loss:.3f}, Valid Loss: {val_loss:3f}, accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fqbrrr7h7hII"
   },
   "source": [
    "You have trained your first model using wandb. üëÜ Click on the wandb link above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CW-kqgAy7hE-"
   },
   "source": [
    "\n",
    "# What's next üöÄ ?\n",
    "The next tutorial you will learn how to do hyperparameter optimization using W&B Sweeps:\n",
    "## üëâ [Hyperparameters sweeps using PyTorch](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W%26B.ipynb)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
