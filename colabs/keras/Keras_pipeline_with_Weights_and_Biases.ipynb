{
 "accelerator": "GPU",
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3253a2c0",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/keras/Keras_pipeline_with_Weights_and_Biases.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "<!--- @wandbcode{keras-wandbcallback-demo} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\"/> <br>\n",
    "\n",
    "<!--- @wandbcode{keras-wandbcallback-demo, v=examples} -->\n",
    "\n",
    "<img src=\"http://wandb.me/mini-diagram\" width=\"600\" alt=\"Weights & Biases\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üçÄ Integrate Weights and Biases in your TensorFlow/Keras workflow\n",
    "\n",
    "In this example, we will we will train an image classifier for the **bloodMNIST** dataset. The primary focus however will be the use of Weights and Biases and how easily it can be added in your TensorFlow/Keras workflow. \n",
    "\n",
    "Consider **[Weights and Biases](https://wandb.ai/site)** (W&B) to be the GitHub for machine learning. Use W&B for machine learning experiment tracking, dataset and model versioning, project collaboration, hyperparameter optimization, dataset exploration, model evaluation and so much more.\n",
    "\n",
    "W&B comes with a lightweight **[integration for Keras](https://docs.wandb.ai/guides/integrations/keras)** (`WandbCallback`) and with just a few lines of code you can log your metrics, save model, training configuration, evaluate model and more. W&B is intrumented with most of your favourite machine learning frameworks. \n",
    "\n",
    "This notebook also introduces **[W&B Tables](https://docs.wandb.ai/guides/data-vis)**. Tables accelerate the ML development lifecycle by giving users the ability to rapidly extract meaningful insights from the data. The WB Table Visualizer provides an interactive interface to perform powerful analytics functions like grouping, joining, and creating custom fields while simultaneously supporting rich media annotations such as bounding boxes and segmentation masks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üå¥ In this Notebook\n",
    "\n",
    "In this colab we'll cover:\n",
    "\n",
    "- training an image classifier for medMNIST (bloodMNIST) dataset,\n",
    "- use of W&B Tables for dataset exploration and evaluation,\n",
    "- `WandbCallback` for experiment tracking and model evaluation.\n",
    "\n",
    "In addition we will also cover some of the best practices of using Weights and Biases to get the most out of your data and model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by installing the dependencies and importing required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Weights and Biases\n",
    "!pip install -qq wandb\n",
    "# To download the dataset\n",
    "!pip install -qq medmnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Dependencies\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# For Deep Learning\n",
    "import tensorflow as tf\n",
    "print(\"TF: \", tf.__version__)\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "# For MLOps\n",
    "import wandb\n",
    "print(\"W&B: \", wandb.__version__)\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "# For medMNIST dataset\n",
    "import medmnist\n",
    "print(\"medMNIST: \", medmnist.__version__)\n",
    "from medmnist import INFO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this is your first time using W&B or you are not logged in, the link that appears after running `wandb.login()` will take you to sign-up/login page. Signing up for a [free account](https://wandb.ai/signup) is as easy as a few clicks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to W&B\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéã Configs\n",
    "\n",
    "Configuration files in `.yaml` or `.json` format is an integral part of most mature machine learning systems. Keeping the track of hyperparameters used to train/evaluate your model is essential for reproducing the experiments. \n",
    "\n",
    "W&B can keep track of your configs. Here we will first define all the hyperparameters needed for training our classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = dict(\n",
    "    data_flag = 'bloodmnist',\n",
    "    image_width = 32,\n",
    "    image_height = 32,\n",
    "    batch_size = 128,\n",
    "    model_name = 'vgg16',\n",
    "    pretrain_weights = 'imagenet',\n",
    "    epochs = 100,\n",
    "    init_learning_rate = 0.001,\n",
    "    lr_decay_rate = 0.1,\n",
    "    optimizer = 'adam',\n",
    "    loss_fn = 'sparse_categorical_crossentropy',\n",
    "    metrics = ['acc'],\n",
    "    earlystopping_patience = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üçÅ Prepare Dataset\n",
    "\n",
    "[MedMNIST](https://medmnist.com/) is a large-scale MNIST-like collection of standardized biomedical images, including 12 datasets for 2D and 6 datasets for 3D. All the images are pre-processed to image size of `28x 28` and doesn't require any prior domain knowledge to start with. \n",
    "\n",
    "In this tutorial, we will be using `BloodMNIST` dataset. From the dataset description. \n",
    "\n",
    "> The BloodMNIST is based on a dataset of individual normal cells, captured from individuals without infection, hematologic or oncologic disease and free of any pharmacologic treatment at the moment of blood collection. It contains a total of 17,092 images and is organized into 8 classes. We split the source dataset with a ratio of 7:1:2 into training, validation and test set. The source images with resolution 3√ó360√ó363 pixels are center-cropped into 3√ó200√ó200, and then resized into 3√ó28√ó28."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = INFO[configs['data_flag']]\n",
    "configs['class_names'] = info['label']\n",
    "configs['image_channels'] = info['n_channels']\n",
    "\n",
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each MedMNIST dataset can be downloaded using the `download_and_prepare_dataset` function below and the downloaded dataset is in the `.npz` format. \n",
    "\n",
    "Each subset (e.g., `bloodmnist.npz`) is comprised of 6 keys: `train_images`, `train_labels`, `val_images`, `val_labels`, `test_images` and `test_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "def download_and_prepare_dataset(data_info: dict):\n",
    "    \"\"\"\n",
    "    Utility function to download the dataset and return train/valid/test images/labels.\n",
    "\n",
    "    Arguments:\n",
    "        data_info (dict): Dataset metadata\n",
    "    \"\"\"\n",
    "    data_path = tf.keras.utils.get_file(origin=data_info['url'], md5_hash=data_info['MD5'])\n",
    "\n",
    "    with np.load(data_path) as data:\n",
    "        # Get images\n",
    "        train_images = data['train_images']\n",
    "        valid_images = data['val_images']\n",
    "        test_images = data['test_images']\n",
    "\n",
    "        # Get labels\n",
    "        train_labels = data['train_labels'].flatten()\n",
    "        valid_labels = data['val_labels'].flatten()\n",
    "        test_labels = data['test_labels'].flatten()\n",
    "\n",
    "    return train_images, train_labels, valid_images, valid_labels, test_images, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_labels, valid_images, valid_labels, test_images, test_labels = download_and_prepare_dataset(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üå≥ Explore the Dataset using W&B Tables\n",
    "\n",
    "As TensorFlow/Keras users you might be familar with the `show_batch` function. Or you might have written some `matplotlib` based code to visualize few batches of dataset. This is good for quick inspection of the dataset but for most real life scenario it's not enough. \n",
    "\n",
    "Here we will use W&B Tables (`wandb.Table`) to log the training data and visualize and query iteractively with W&B. As the name suggests it is a table of data specified by you. Check out more on Tables [here](https://docs.wandb.ai/guides/data-vis).\n",
    "\n",
    "You can log data to W&B Tables row wise or column wise. In the section below, **we have created the table column wise**. Use `add_column` to define the name of the column and provide array of data associated with that column. Simply adding array of images will not render in the W&B Tables UI. You will have to wrap each image array with `wandb.Image`. To do so, `add_computed_columns` is used. You can learn about these methods [here](https://docs.wandb.ai/ref/python/data-types/table).\n",
    "\n",
    "Finally, note that W&B Tables is built on top of **[W&B Artifacts](https://docs.wandb.ai/guides/artifacts)**, which can be viewed as a file (usually for dataset and models) storage system in W&B. In this section, we have explicitly initialized an artifact using `wandb.Artifact` and have added both the `train_table` and `validation_table` to the artifact. Alternatively, we could have prepared the table and logged it using `wandb.log`. Here's a quick [example](https://docs.wandb.ai/guides/data-vis#quickly-log-your-first-table) if you are interested. \n",
    "\n",
    "If you want to log the entire dataset place a tick in the `log_full` checkbox. Note that we are logging the entire validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demonstration purposes\n",
    "log_full = False #@param {type:\"boolean\"}\n",
    "\n",
    "if log_full:\n",
    "    log_train_samples = len(train_images)\n",
    "else:\n",
    "    log_train_samples = 1000 \n",
    "\n",
    "print(f'Number of train images : {log_train_samples} to be logged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Initialize a new W&B run\n",
    "run = wandb.init(project='medmnist-bloodmnist', group='viz_data')\n",
    "\n",
    "# Intialize a W&B Artifacts\n",
    "ds = wandb.Artifact(\"medmnist_bloodmnist_dataset\", \"dataset\")\n",
    "\n",
    "# Initialize an empty table\n",
    "train_table = wandb.Table(columns=[], data=[])\n",
    "# Add training data\n",
    "train_table.add_column('image', train_images[:log_train_samples])\n",
    "# Add training label_id\n",
    "train_table.add_column('label_id', train_labels[:log_train_samples])\n",
    "# Add training class names\n",
    "train_table.add_computed_columns(lambda ndx, row:{\n",
    "    \"images\": wandb.Image(row[\"image\"]),\n",
    "    \"class_names\": configs['class_names'][str(row[\"label_id\"])]\n",
    "    })\n",
    "\n",
    "# Add the table to the Artifact\n",
    "ds['train_data'] = train_table\n",
    "\n",
    "# Let's do the same for the validation data\n",
    "valid_table = wandb.Table(columns=[], data=[])\n",
    "valid_table.add_column('image', valid_images)\n",
    "valid_table.add_column('label_id', valid_labels)\n",
    "valid_table.add_computed_columns(lambda ndx, row:{\n",
    "    \"images\": wandb.Image(row[\"image\"]),\n",
    "    \"class_name\": configs['class_names'][str(row[\"label_id\"])]\n",
    "    })\n",
    "ds['valid_data'] = valid_table\n",
    "\n",
    "# Save the dataset as an Artifact\n",
    "ds.save()\n",
    "\n",
    "# Finish the run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of the logged **[train table](https://wandb.ai/ayush-thakur/medmnist-bloodmnist/artifacts/dataset/medmnist_bloodmnist_dataset/61dc68d3fb90fa00168a/files/train_data.table.json)** and **[validation table](https://wandb.ai/ayush-thakur/medmnist-bloodmnist/artifacts/dataset/medmnist_bloodmnist_dataset/61dc68d3fb90fa00168a/files/valid_data.table.json)** of the logged table. You can group, filter and interactively query the dataset $‚Üí$\n",
    "\n",
    "![1st_gif.gif](https://s10.gifyu.com/images/1st_gif.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üå≤ Data Pipeline\n",
    "\n",
    "`tf.data.Dataset` is used to build the data pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "@tf.function\n",
    "def preprocess(image: tf.Tensor, label: tf.Tensor):\n",
    "    \"\"\"\n",
    "    Preprocess the image tensors and parse the labels\n",
    "    \"\"\"\n",
    "    # Preprocess images\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    \n",
    "    # Parse label\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    \n",
    "    return image, label\n",
    "\n",
    "\n",
    "def prepare_dataloader(images: np.ndarray,\n",
    "                       labels: np.ndarray,\n",
    "                       loader_type: str='train',\n",
    "                       batch_size: int=128):\n",
    "    \"\"\"\n",
    "    Utility function to prepare dataloader.\n",
    "    \"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "\n",
    "    if loader_type=='train':\n",
    "        dataset = dataset.shuffle(1024)\n",
    "\n",
    "    dataloader = (\n",
    "        dataset\n",
    "        .map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = prepare_dataloader(train_images, train_labels, 'train', configs.get('batch_size', 64))\n",
    "validloader = prepare_dataloader(valid_images, valid_labels, 'valid', configs.get('batch_size', 64))\n",
    "testloader = prepare_dataloader(test_images, test_labels, 'test', configs.get('batch_size', 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü™¥ Data Augmentation\n",
    "\n",
    "We will apply simple image augmentation policies using the Keras preprocessing layers API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_augmentation = models.Sequential(\n",
    "    [\n",
    "        layers.RandomRotation(factor=0.15),\n",
    "        layers.RandomFlip()],\n",
    "    name=\"img_augmentation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåø Visualize Different Augmented View. \n",
    "\n",
    "Here, let's use W&B Tables to visualize augmented images of a subset of training images. \n",
    "\n",
    "Augmentation policies should make sense for the given classification task. By using W&B Tables here we can visualize how the original images are augmented. For the sake of simplicity, we will just be visualizing the first 100 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "def augment_5_times(img):\n",
    "    augmented_imgs = []\n",
    "    for _ in range(5):\n",
    "        aug_img = tf.squeeze(img_augmentation(img), axis=0)\n",
    "        # Notice the use of wrapping the images with wandb.Image\n",
    "        wandb_image = wandb.Image(aug_img.numpy())\n",
    "        augmented_imgs.append(wandb_image)\n",
    "\n",
    "    return augmented_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can download the dataset that we have logged as W&B Tables as shown in the code cell below. Since Tables are saved as W&B Artifacts, we first need to pass in the name (path as shown in the UI) of the artifact to `use_artifact`. You can find the name if you head over to the [artifact tab](https://docs.wandb.ai/ref/app/pages/project-page#artifacts-tab) on the W&B dashboard and click on the [API panel](https://docs.wandb.ai/ref/app/pages/project-page#api-panel).\n",
    "\n",
    "Get the required table by using the `get` method and provide the name of the table. Use the `get_column` method get the data associated with that column. Here, the `augment_table` is initialized with the column names and data are added **row-wise** iteratively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "viz_augment_samples = 100\n",
    "\n",
    "# Initialize a W&B run\n",
    "run = wandb.init(project='medmnist-bloodmnist', group='viz_augmentation')\n",
    "\n",
    "# Use the already logged dataset\n",
    "train_art = run.use_artifact('ayush-thakur/medmnist-bloodmnist/medmnist_bloodmnist_dataset:latest', type='dataset')\n",
    "\n",
    "# Get the train_table to access the data\n",
    "train_table = train_art.get(\"train_data\")\n",
    "\n",
    "# Get the images, ground truth label, and row index\n",
    "images = train_table.get_column(\"images\", convert_to=\"numpy\")\n",
    "labels = train_table.get_column(\"label_id\", convert_to=\"numpy\")\n",
    "ids = train_table.get_index()\n",
    "# Shuffle the ids and slice\n",
    "random.shuffle(ids)\n",
    "sample_ids = ids[0:viz_augment_samples]\n",
    "\n",
    "# Create augmentation table\n",
    "augment_table = wandb.Table(columns=['image', 'truth', 'label_id', 'aug1', 'aug2', 'aug3', 'aug4', 'aug5'])\n",
    "\n",
    "# Get augmented images and log it onto the table\n",
    "for sample_id in sample_ids:\n",
    "    img = images[sample_id]\n",
    "    label = labels[sample_id]\n",
    "    augmented_imgs = augment_5_times(tf.expand_dims(img, axis=0))\n",
    "    augment_table.add_data(wandb.Image(img),\n",
    "                           np.argmax(label),\n",
    "                           configs['class_names'][str(label)],\n",
    "                           augmented_imgs[0],\n",
    "                           augmented_imgs[1],\n",
    "                           augmented_imgs[2],\n",
    "                           augmented_imgs[3],\n",
    "                           augmented_imgs[4])\n",
    "\n",
    "# Log the table\n",
    "wandb.log({'augmented data': augment_table})\n",
    "\n",
    "# Finish the run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an **[example](https://wandb.ai/ayush-thakur/medmnist-bloodmnist/runs/52pannnd?workspace=user-ayush-thakur)** of the logged table $‚Üí$\n",
    "![3rd_gif_2.gif](https://s10.gifyu.com/images/3rd_gif_2.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéÑ Model\n",
    "\n",
    "We will be using [VGG16](https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg16/VGG16) as the backbone CNN block. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_shape: tuple=(28, 28, 3), \n",
    "              resize: tuple=(32, 32, 3),\n",
    "              dropout_rate: float=0.5,\n",
    "              num_classes: int=8,\n",
    "              output_activation: str='softmax'):\n",
    "  \n",
    "    inputs = layers.Input(input_shape)\n",
    "    resize_img = layers.Resizing(resize[0], resize[1], interpolation='bilinear')(inputs)\n",
    "    augment_img = img_augmentation(resize_img)\n",
    "  \n",
    "    base_model = tf.keras.applications.VGG16(include_top=False, \n",
    "                                             weights=configs['pretrain_weights'], \n",
    "                                             input_shape=resize,\n",
    "                                             input_tensor=augment_img)\n",
    "    base_model.trainabe = True\n",
    "\n",
    "    \n",
    "    x = base_model.output\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=output_activation)(x)\n",
    "\n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚òòÔ∏è Callback\n",
    "\n",
    "Here we will define early stopping callback. We will define the `WandbCallback` later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopper = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=configs['earlystopping_patience'], verbose=0, mode='auto',\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `wandb.log` to log any useful metric/parameter that's not logged by `WandbCallback`. Here we are using a learning rate scheduler to exponentially decay the learning rate after 10 epochs. Notice the use of `wandb.log` to capture the learning rate and `commit=False` in particular.\n",
    "\n",
    "You can learn more about `wandb.log` [here](https://docs.wandb.ai/guides/track/log)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_scheduler(epoch, lr):\n",
    "    # log the current learning rate onto W&B\n",
    "    if wandb.run is None:\n",
    "        raise wandb.Error(\"You must call wandb.init() before WandbCallback()\")\n",
    "\n",
    "    wandb.log({'learning_rate': lr}, commit=False)\n",
    "    \n",
    "    if epoch < 7:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-configs['lr_decay_rate'])\n",
    "\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåª Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config: dict, \n",
    "          callbacks: list,\n",
    "          verbose: int=0):\n",
    "    \"\"\"\n",
    "    Utility function to train the model.\n",
    "\n",
    "    Arguments:\n",
    "        config (dict): Dictionary of hyperparameters.\n",
    "        callbacks (list): List of callbacks passed to `model.fit`.\n",
    "        verbose (int): 0 for silent and 1 for progress bar.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initalize model\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = get_model(resize=(config.image_width, config.image_height, config.image_channels))\n",
    "\n",
    "    # Compile the model\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=config.init_learning_rate)\n",
    "    model.compile(opt,\n",
    "                  config.loss_fn,\n",
    "                  metrics=config.metrics)\n",
    "\n",
    "    # Train the model\n",
    "    _ = model.fit(trainloader,\n",
    "                  epochs=config.epochs,\n",
    "                  validation_data=validloader,\n",
    "                  callbacks=callbacks,\n",
    "                  verbose=verbose)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üçÑ Train using `WandbCallback`\n",
    "\n",
    "In the section below we will train our classifier using `WandbCallback` to by default log all the training and validation metrics to a wandb dashboard. \n",
    "\n",
    "`WandbCallback` enables to you keep track of your experiments, saves the best model, and helps visualize model performance with just one line of code. \n",
    "\n",
    "In the section below, we have used the following arguments:\n",
    "\n",
    "* `monitor = 'val_loss'` will monitor the mentioned metric to save the best model. Note that `'val_loss'` is the default value for `monitor`.\n",
    "* `log_weights = True` save histograms of the model's layer's weights. \n",
    "* `log_evaluation = True` will create a W&B Table of validation data and model prediction. The number of validation samples is controlled by `validation_steps` if a generator is passed to `model.fit`. \n",
    "\n",
    "Check out the documentation [here](https://docs.wandb.ai/ref/python/integrations/keras/wandbcallback) to know more about the `WandbCallback`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the W&B run\n",
    "run = wandb.init(project='medmnist-bloodmnist', config=configs, job_type='train')\n",
    "config = wandb.config\n",
    "\n",
    "# Define WandbCallback for experiment tracking\n",
    "wandb_callback = WandbCallback(monitor='val_loss',\n",
    "                               log_weights=True,\n",
    "                               log_evaluation=True,\n",
    "                               validation_steps=5)\n",
    "\n",
    "# callbacks\n",
    "callbacks = [earlystopper, wandb_callback, lr_callback]\n",
    "\n",
    "# Train\n",
    "model = train(config, callbacks=callbacks, verbose=1)\n",
    "\n",
    "# Evaluate the trained model\n",
    "loss, acc = model.evaluate(validloader)\n",
    "wandb.log({'evaluate/accuracy': acc})\n",
    "\n",
    "# Close the W&B run.\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the example **[W&B run page](https://wandb.ai/ayush-thakur/medmnist-bloodmnist/runs/3ceo33mx)** after training the model $‚Üí$\n",
    "![2nd_gif.gif](https://s10.gifyu.com/images/2nd_gif.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üå± Advanced Usage\n",
    "\n",
    "In this section, we will see an advance usage of `WandbCallback`. \n",
    "\n",
    "We will use `WandbCallback` to log the GradCAM for each validation examples along with gound truth labels and model predictions.\n",
    "\n",
    "We will be using this [tutorial](https://keras.io/examples/vision/grad_cam/) on GradCAM by Fran√ßois Chollet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    # First, we create a model that maps the input image to the activations\n",
    "    # of the last conv layer as well as the output predictions\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    # Then, we compute the gradient of the top predicted class for our input image\n",
    "    # with respect to the activations of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(preds[0])\n",
    "        class_channel = preds[:, pred_index]\n",
    "\n",
    "    # This is the gradient of the output neuron (top predicted or chosen)\n",
    "    # with regard to the output feature map of the last conv layer\n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "\n",
    "    # This is a vector where each entry is the mean intensity of the gradient\n",
    "    # over a specific feature map channel\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    # We multiply each channel in the feature map array\n",
    "    # by \"how important this channel is\" with regard to the top predicted class\n",
    "    # then sum all the channels to obtain the heatmap class activation\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy()\n",
    "\n",
    "def create_gradcam(image, model, last_conv_layer_name, pred_index=None):\n",
    "    # Preprocess the image array\n",
    "    image, _ = preprocess(tf.expand_dims(image, axis=0), 0)\n",
    "    # Get GradCAM\n",
    "    heatmap = make_gradcam_heatmap(image, model, last_conv_layer_name, pred_index)\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "    # Use jet colormap to colorize heatmap\n",
    "    jet = cm.get_cmap(\"jet\")\n",
    "\n",
    "    # Use RGB values of the colormap\n",
    "    jet_colors = jet(np.arange(256))[:, :3]\n",
    "    jet_heatmap = jet_colors[heatmap]\n",
    "    jet_heatmap = tf.image.resize(jet_heatmap, size=(28,28))\n",
    "\n",
    "    # Overlay\n",
    "    superimposed_img = jet_heatmap * 0.4 + tf.squeeze(image, axis=0)\n",
    "    superimposed_img = tf.clip_by_value(superimposed_img, 0.0, 1.0)\n",
    "\n",
    "    return superimposed_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_conv_layer_name = 'block4_conv3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell block below, we will be using `WandbCallback`'s `validation_row_processor` and `prediction_row_processor` to log the images, ground truth label, model prediction and the GradCAM for model interpretability. \n",
    "\n",
    "The processors' take a callable function that receive an `ndx` (index) and a `row` (dict of data). The `validation_processor` function below receives the input image array along with target label as `row` dict. The `prediction_processor` receives  the model output prediction and the validation data row index. \n",
    "\n",
    "The `validation_row_processor` is executed when `WandbCallback` is initialized (i.e, before model training) while `prediction_row_processor` is called once the training is over. The `validation_row_processor` creates a table with two columns `input:image` and `target:class`. Notice that in the `prediction_processor` function we can get the logged image at a given `val_row` using the `get_row` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_processor(ndx, row):\n",
    "    return {\n",
    "        \"input:image\": wandb.Image(row[\"input\"]),\n",
    "        \"target:class\": class_table.index_ref(row[\"target\"])\n",
    "    }\n",
    "\n",
    "def prediction_processor(ndx, row):\n",
    "    # Get the validation image\n",
    "    valid_image = np.array(row[\"val_row\"].get_row()[\"input:image\"].image)\n",
    "\n",
    "    return {\n",
    "        \"output:class\": class_table.index_ref(np.argmax(row[\"output\"])),\n",
    "        \"gradcam\": wandb.Image(create_gradcam(valid_image, model, last_conv_layer_name)),\n",
    "        \"output:logits\": {class_name: value for (class_name, value) in zip(list(config.class_names.values()), row[\"output\"].tolist())}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the W&B run\n",
    "run = wandb.init(project='medmnist-bloodmnist', config=configs, job_type='train')\n",
    "config = wandb.config\n",
    "\n",
    "# Get validation table\n",
    "data_art = run.use_artifact('ayush-thakur/medmnist-bloodmnist/medmnist_bloodmnist_dataset:latest', type='dataset')\n",
    "valid_table = data_art.get(\"valid_data\")\n",
    "\n",
    "# Create a class table\n",
    "class_table = wandb.Table(columns=[], data=[])\n",
    "class_table.add_column(\"class_name\", list(config.class_names.values()))\n",
    "\n",
    "# Define WandbCallback for experiment tracking\n",
    "wandb_callback = WandbCallback(\n",
    "                    log_evaluation=True,\n",
    "                    validation_row_processor=lambda ndx, row: validation_processor(ndx, row),\n",
    "                    prediction_row_processor=lambda ndx, row: prediction_processor(ndx, row),\n",
    "                    validation_steps=4,\n",
    "                    save_model=False\n",
    "                )\n",
    "\n",
    "# callbacks\n",
    "callbacks = [earlystopper, wandb_callback, lr_callback]\n",
    "\n",
    "# Train\n",
    "model = train(config, callbacks=callbacks, verbose=1)\n",
    "\n",
    "# Evaluate the trained model\n",
    "loss, acc = model.evaluate(validloader)\n",
    "wandb.log({'evaluate/accuracy': acc})\n",
    "\n",
    "# Close the W&B run.\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example **[W&B run page](https://wandb.ai/ayush-thakur/medmnist-bloodmnist/runs/3befja9v?workspace=user-ayush-thakur)** with logged GradCAM and logits of the model $‚Üí$\n",
    "\n",
    "![3rd_gif.gif](https://s10.gifyu.com/images/3rd_gif.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåæ Conclusion\n",
    "\n",
    "Weights and Biases's Keras integration enables experiment tracking and so much more with just few lines of code. In this notebook, we have seen some advanced usage of Keras `WandbCallback` and different ways of using W&B Tables for evaluation and data exploration. \n",
    "\n",
    "To sum up all you need is a free W&B account, import the `WandbCallback` and pass it to `model.fit(callbacks=[.])` just like any callback. There are few more arguments that you can learn about in the documentation page [here](https://docs.wandb.ai/ref/python/integrations/keras/wandbcallback). In particular, \n",
    "\n",
    "* you can log the metrics for each batch by setting `log_batch_frequency=1`,\n",
    "* you can log the gradients of each layer to debug vanishing or exploding gradient issue by setting `log_gradients=True`. You will also have to provide the `training_data` in the format of `(X, y)`.\n",
    "* if your task is semantic segmentation you can set `input_type=segmentation_mask`. \n",
    "\n",
    "If a usecase is not covered by the `WandbCallback` you can easily write a [custom Keras callback](https://keras.io/guides/writing_your_own_callbacks/) and use `wandb.log` to log the required data to W&B dashboard."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
