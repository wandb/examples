{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/keras/Simple_Keras_Integration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "<!--- @wandbcode{keras-video} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
    "\n",
    "<!--- @wandbcode{keras-video} -->\n",
    "\n",
    "# W&B ðŸ’˜ Keras\n",
    "Use Weights & Biases for machine learning experiment tracking, model checkpointing, and project collaboration.\n",
    "\n",
    "<img src=\"https://wandb.me/mini-diagram\" width=\"650\" alt=\"Weights & Biases\" />\n",
    "\n",
    "## Keras Documentation\n",
    "\n",
    "For a full guide of how to log to Weights & Biases using Keras, see **[our Keras documentation](https://docs.wandb.ai/guides/integrations/keras)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## What this notebook covers:\n",
    "\n",
    "We show you how to integrate Weights & Biases with your Keras code to add experiment tracking to your pipeline. That includes:\n",
    "\n",
    "1. Storing hyperparameters and metadata in a `config`.\n",
    "2. Passing the wandb Keras callbacks to `model.fit`. This will automatically log training metrics, like loss, and system metrics, like GPU and CPU utilization.\n",
    "3. Using the `wandb.log` API to log custom metrics.\n",
    "\n",
    "all using the CIFAR-10 dataset.\n",
    "\n",
    "Then, we'll show you how to catch your model making mistakes by logging both the output predictions and the input images the network used to generate them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Follow along with a [video tutorial](http://wandb.me/keras-video)!\n",
    "**Note**: Sections starting with _Step_ are all you need to integrate W&B in an existing pipeline. The rest just loads data and defines a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install, Import, and Log In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Install W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import W&B and Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint, WandbEvalCallback\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Side note: If this is your first time using W&B or you are not logged in, the link that appears after running `wandb.login` will take you to sign-up/login page. Signing up is easy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and Prepare the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Subsetting train data and normalizing to [0., 1.]\n",
    "x_train, x_test = x_train[::5] / 255., x_test / 255.\n",
    "y_train = y_train[::5]\n",
    "\n",
    "CLASS_NAMES = [\"airplane\", \"automobile\", \"bird\", \"cat\",\n",
    "               \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "\n",
    "print('Shape of x_train: ', x_train.shape)\n",
    "print('Shape of y_train: ', y_train.shape)\n",
    "print('Shape of x_test: ', x_test.shape)\n",
    "print('Shape of y_test: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define a standard CNN (with convolution and max-pooling) in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model():\n",
    "  inputs = keras.layers.Input(shape=(32, 32, 3))\n",
    "\n",
    "  x = keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(inputs)\n",
    "  x = keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(x)\n",
    "  x = keras.layers.MaxPooling2D(pool_size=2)(x)\n",
    "\n",
    "  x = keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(x)\n",
    "  x = keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(x)\n",
    "\n",
    "  x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "  x = keras.layers.Dense(128, activation='relu')(x)\n",
    "  x = keras.layers.Dense(32, activation='relu')(x)\n",
    "  \n",
    "  outputs = keras.layers.Dense(len(CLASS_NAMES), activation='softmax')(x)\n",
    "\n",
    "  return keras.models.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Give `wandb.init` your `config`\n",
    "\n",
    "You first initialize your wandb run, letting us know some training is about to happen. [Check out the official documentation for `.init` here $\\rightarrow$](https://docs.wandb.com/library/init)\n",
    "\n",
    "That's when you need to set your hyperparameters.\n",
    "They're passed in as a dictionary via the `config` argument,\n",
    "and then become available as the `config` attribute of `wandb`.\n",
    "\n",
    "Learn more about `config` in this [Colab Notebook $\\rightarrow$](http://wandb.me/config-colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb with your project name\n",
    "run = wandb.init(project='my-keras-project',\n",
    "                 config={  # and include hyperparameters and metadata\n",
    "                     \"learning_rate\": 0.005,\n",
    "                     \"epochs\": 5,\n",
    "                     \"batch_size\": 1024,\n",
    "                     \"loss_function\": \"sparse_categorical_crossentropy\",\n",
    "                     \"architecture\": \"CNN\",\n",
    "                     \"dataset\": \"CIFAR-10\"\n",
    "                 })\n",
    "config = wandb.config  # We'll use this to configure our experiment\n",
    "\n",
    "# Initialize model like you usually do.\n",
    "tf.keras.backend.clear_session()\n",
    "model = Model()\n",
    "model.summary()\n",
    "\n",
    "# Compile model like you usually do.\n",
    "# Notice that we use config, so our metadata matches what gets executed\n",
    "optimizer = tf.keras.optimizers.Adam(config.learning_rate) \n",
    "model.compile(optimizer, config.loss_function, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Pass `WandbMetricsLogger` and `WandbModelCheckpoint` to `model.fit`\n",
    "\n",
    "Keras has a [robust callbacks system](https://keras.io/api/callbacks/) that\n",
    "allows users to separate model definition and the core training logic\n",
    "from other behaviors that occur during training and testing.\n",
    "\n",
    "That includes, for example, \n",
    "\n",
    "**Click on the Project page link above to see your results!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add WandbMetricsLogger to log metrics and WandbModelCheckpoint to log model checkpoints\n",
    "wandb_callbacks = [\n",
    "    WandbMetricsLogger(),\n",
    "    WandbModelCheckpoint(filepath=\"my_model_{epoch:02d}\"),\n",
    "]\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=config.epochs, \n",
    "          batch_size=config.batch_size,\n",
    "          validation_data=(x_test, y_test),\n",
    "          callbacks=wandb_callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use `wandb.log` for custom metrics\n",
    "\n",
    "Here, we log the error rate on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print('Test Error Rate: ', round((1 - accuracy) * 100, 2))\n",
    "\n",
    "# With wandb.log, we can easily pass in metrics as key-value pairs.\n",
    "wandb.log({'Test Error Rate': round((1 - accuracy) * 100, 2)})\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log predictions on test data using `WandbEvalCallback`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u50GwKJ70WeJ"
   },
   "source": [
    "The `WandbEvalCallback` is an abstract base class to build Keras callbacks for primarily model prediction visualization and secondarily dataset visualization.\n",
    "\n",
    "This is a dataset and task agnostic abstract callback. To use this, inherit from this base callback class and implement the `add_ground_truth` and `add_model_prediction` methods.\n",
    "\n",
    "The `WandbEvalCallback` is a utility class that provides helpful methods to:\n",
    "\n",
    "- create data and prediction `wandb.Table` instances,\n",
    "- log data and prediction Tables as `wandb.Artifact`,\n",
    "- logs the data table `on_train_begin`,\n",
    "- logs the prediction table `on_epoch_end`.\n",
    "\n",
    "As an example, we have implemented `WandbClsEvalCallback` below for an image classification task. This example callback:\n",
    "- logs the validation data (`data_table`) to W&B,\n",
    "- performs inference and logs the prediction (`pred_table`) to W&B on every epoch end.\n",
    "\n",
    "\n",
    "## How the memory footprint is reduced?\n",
    "\n",
    "We log the `data_table` to W&B when the `on_train_begin` method is ivoked. Once it's uploaded as a W&B Artifact, we get a reference to this table which can be accessed using `data_table_ref` class variable. The `data_table_ref` is a 2D list that can be indexed like `self.data_table_ref[idx][n]` where `idx` is the row number while `n` is the column number. Let's see the usage in the example below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-class `WandbEvalCallback`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WandbClsEvalCallback(WandbEvalCallback):\n",
    "    def __init__(\n",
    "        self, validloader, data_table_columns, pred_table_columns, num_samples=100\n",
    "    ):\n",
    "        super().__init__(data_table_columns, pred_table_columns)\n",
    "\n",
    "        self.val_data = validloader.unbatch().take(num_samples)\n",
    "\n",
    "    def add_ground_truth(self, logs=None):\n",
    "        for idx, (image, label) in enumerate(self.val_data):\n",
    "            self.data_table.add_data(\n",
    "                idx,\n",
    "                wandb.Image(image),\n",
    "                np.argmax(label, axis=-1)\n",
    "            )\n",
    "\n",
    "    def add_model_predictions(self, epoch, logs=None):\n",
    "        # Get predictions\n",
    "        preds = self._inference()\n",
    "        table_idxs = self.data_table_ref.get_index()\n",
    "\n",
    "        for idx in table_idxs:\n",
    "            pred = preds[idx]\n",
    "            logit = logits[idx]\n",
    "            self.pred_table.add_data(\n",
    "                epoch,\n",
    "                self.data_table_ref.data[idx][0],\n",
    "                self.data_table_ref.data[idx][1],\n",
    "                self.data_table_ref.data[idx][2],\n",
    "                pred\n",
    "            )\n",
    "\n",
    "    def _inference(self):\n",
    "      preds = []\n",
    "      for image, label in self.val_data:\n",
    "          pred = self.model(tf.expand_dims(image, axis=0))\n",
    "          argmax_pred = tf.argmax(pred, axis=-1).numpy()[0]\n",
    "          preds.append(argmax_pred)\n",
    "          \n",
    "      return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dataset processing and Dataloaders functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFlLHxGKAC9I"
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def parse_data(example):\n",
    "    # Get image\n",
    "    image = example[\"image\"]\n",
    "    # image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "\n",
    "    # Get label\n",
    "    label = example[\"label\"]\n",
    "    label = tf.one_hot(label, depth=configs[\"num_classes\"])\n",
    "\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def get_dataloader(ds, configs, dataloader_type=\"train\"):\n",
    "    dataloader = ds.map(parse_data, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    if dataloader_type==\"train\":\n",
    "        dataloader = dataloader.shuffle(configs[\"shuffle_buffer\"])\n",
    "      \n",
    "    dataloader = (\n",
    "        dataloader\n",
    "        .batch(configs[\"batch_size\"])\n",
    "        .prefetch(AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hGFNaLP3Frey"
   },
   "outputs": [],
   "source": [
    "def get_model(configs):\n",
    "    backbone = tf.keras.applications.mobilenet_v2.MobileNetV2(weights='imagenet', include_top=False)\n",
    "    backbone.trainable = False\n",
    "\n",
    "    inputs = layers.Input(shape=(configs[\"image_size\"], configs[\"image_size\"], configs[\"image_channels\"]))\n",
    "    resize = layers.Resizing(32, 32)(inputs)\n",
    "    neck = layers.Conv2D(3, (3,3), padding=\"same\")(resize)\n",
    "    preprocess_input = tf.keras.applications.mobilenet.preprocess_input(neck)\n",
    "    x = backbone(preprocess_input)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = layers.Dense(configs[\"num_classes\"], activation=\"softmax\")(x)\n",
    "\n",
    "    return models.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set our config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZ1zdI93Cwlu"
   },
   "outputs": [],
   "source": [
    "configs = dict(\n",
    "    num_classes = 10,\n",
    "    shuffle_buffer = 1024,\n",
    "    batch_size = 64,\n",
    "    image_size = 28,\n",
    "    image_channels = 1,\n",
    "    earlystopping_patience = 3,\n",
    "    learning_rate = 1e-3,\n",
    "    epochs = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWHbCi1N95kp"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "In this example, we will be using [CIFAR100](https://www.tensorflow.org/datasets/catalog/cifar100) dataset from TensorFlow Dataset catalog. We aim to build a simple image classification pipeline using TensorFlow/Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ObHZIRX52dTV"
   },
   "outputs": [],
   "source": [
    "train_ds, valid_ds = tfds.load('fashion_mnist', split=['train', 'test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create our Dataloaders and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7W7-qNU_Dqxw"
   },
   "outputs": [],
   "source": [
    "trainloader = get_dataloader(train_ds, configs)\n",
    "validloader = get_dataloader(valid_ds, configs, dataloader_type=\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Q_bQfITM4Yo"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = get_model(configs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lVLMACLcRmOn"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = \"adam\",\n",
    "    loss = \"categorical_crossentropy\",\n",
    "    metrics = [\"accuracy\", tf.keras.metrics.TopKCategoricalAccuracy(k=5, name='top@5_accuracy')]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model and log the predictions to a W&B Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a W&B run\n",
    "run = wandb.init(\n",
    "    project = \"my-keras-project\",\n",
    "    config = configs\n",
    ")\n",
    "\n",
    "wandb_callbacks = [\n",
    "        WandbMetricsLogger(log_freq=10),\n",
    "        WandbModelCheckpoint(filepath=\"my_model_{epoch:02d}\"),\n",
    "        WandbClsEvalCallback(\n",
    "            validloader,\n",
    "            data_table_columns=[\"idx\", \"image\", \"ground_truth\"],\n",
    "            pred_table_columns=[\"epoch\", \"idx\", \"image\", \"ground_truth\", \"prediction\"]\n",
    "        ) \n",
    "    ]\n",
    "\n",
    "# Train your model\n",
    "model.fit(\n",
    "    trainloader,\n",
    "    epochs = configs[\"epochs\"],\n",
    "    validation_data = validloader,\n",
    "    callbacks = wandb_callbacks\n",
    ")\n",
    "\n",
    "# Close the W&B run\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on the **W&B project page** link above to see your live results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whats Next? Hyperparameters with Sweeps\n",
    "\n",
    "We tried out two different hyperparameter settings by hand. You can use Weights & Biases Sweeps to automate hyperparameter testing and explore the space of possible models and optimization strategies.\n",
    "\n",
    "## [Check out Hyperparameter Optimization in TensorFlow uisng W&B Sweep $\\rightarrow$](https://colab.research.google.com/drive/18TOgqiEsKxM0SMuIgBuDllE-dl9iisDz?usp=sharing)\n",
    "\n",
    "Running a hyperparameter sweep with Weights & Biases is very easy. There are just 3 simple steps:\n",
    "\n",
    "1. **Define the sweep:** We do this by creating a dictionary or a [YAML file](https://docs.wandb.com/library/sweeps/configuration) that specifies the parameters to search through, the search strategy, the optimization metric et all.\n",
    "\n",
    "2. **Initialize the sweep:** \n",
    "`sweep_id = wandb.sweep(sweep_config)`\n",
    "\n",
    "3. **Run the sweep agent:** \n",
    "`wandb.agent(sweep_id, function=train)`\n",
    "\n",
    "And voila! That's all there is to running a hyperparameter sweep! In the notebook below, we'll walk through these 3 steps in more detail.\n",
    "\n",
    "<img src=\"https://imgur.com/UiQKg0L.png\" alt=\"Weights & Biases\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation\n",
    "\n",
    "For a full guide of how to log to Weights & Biases using Keras, see **[our Keras documentation](https://docs.wandb.ai/guides/integrations/keras)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
