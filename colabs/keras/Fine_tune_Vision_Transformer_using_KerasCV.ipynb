{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/keras/Fine_tune_Vision_Transformer_using_KerasCV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "<!--- @wandbcode{keras-vit} -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installations and Imports\n",
    "<!--- @wandbcode{keras-vit} -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq keras-cv\n",
    "!pip install -qq wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from argparse import Namespace\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "import keras_cv as kcv\n",
    "from keras_cv.models import ViTTiny16\n",
    "from keras_cv.layers import preprocessing\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbMetricsLogger\n",
    "from wandb.keras import WandbEvalCallback\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = Namespace(\n",
    "    learning_rate = 1e-4,\n",
    "    batch_size = 64,\n",
    "    num_epochs = 10,\n",
    "    image_size = 224,\n",
    "    num_classes = 120,\n",
    "    num_steps = 1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "\n",
    "def parse_data(example):\n",
    "    \"Apply preprocessing to one data sample at a time.\"\n",
    "    image = example[\"image\"]\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    image = tf.image.resize(image, (configs.image_size, configs.image_size))\n",
    "\n",
    "    label = example[\"label\"]\n",
    "    label = tf.one_hot(label, configs.num_classes)\n",
    "\n",
    "    return image, label\n",
    "\n",
    "\n",
    "base_augmentations = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "        tf.keras.layers.RandomRotation(factor=0.02),\n",
    "        tf.keras.layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
    "    ],\n",
    "    name=\"base_augmentation\",\n",
    ")\n",
    "\n",
    "mixup = preprocessing.MixUp(alpha=0.8)\n",
    "\n",
    "\n",
    "def apply_base_augmentations(images, labels):\n",
    "    images = base_augmentations(images)\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "ds_train, ds_test = tfds.load('stanford_dogs', split=['train', 'test'])\n",
    "\n",
    "trainloader = (\n",
    "    ds_train\n",
    "    .map(parse_data, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(configs.batch_size)\n",
    "    .map(apply_base_augmentations, num_parallel_calls=AUTOTUNE)\n",
    "    .map(lambda images, labels: mixup({\"images\": images, \"labels\": labels}), num_parallel_calls=AUTOTUNE)\n",
    "    .map(lambda x: (x[\"images\"], x[\"labels\"]), num_parallel_calls=AUTOTUNE)\n",
    "    .shuffle(1024)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "testloader = (\n",
    "    ds_test\n",
    "    .map(parse_data, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(configs.batch_size)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    inputs = tf.keras.layers.Input(shape=(configs.image_size, configs.image_size, 3))\n",
    "\n",
    "    vit = ViTTiny16(\n",
    "        include_rescaling=False,\n",
    "        include_top=False,\n",
    "        name=\"ViTTiny32\",\n",
    "        weights=\"imagenet\",\n",
    "        input_tensor=inputs,\n",
    "        pooling=\"token_pooling\",\n",
    "        activation=tf.keras.activations.gelu,\n",
    "    )\n",
    "    \n",
    "    vit.trainable = True\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(configs.num_classes, activation=\"softmax\")(vit.output)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile the Model\n",
    "\n",
    "We will use `CosineDecay` learning rate scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps = len(trainloader)*configs.num_epochs\n",
    "decay_steps = total_steps * configs.num_steps\n",
    "\n",
    "cosine_decay_scheduler = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    configs.learning_rate, decay_steps, alpha=0.1\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=cosine_decay_scheduler),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [OPTIONAL] Model Prediction Visualization\n",
    "\n",
    "We will build a custom Keras callback by subclassing `WandbEvalCallback` for model prediction visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WandbClfEvalCallback(WandbEvalCallback):\n",
    "    def __init__(\n",
    "        self, validloader, data_table_columns, pred_table_columns, num_samples=100\n",
    "    ):\n",
    "        super().__init__(data_table_columns, pred_table_columns)\n",
    "\n",
    "        self.val_data = validloader.unbatch().take(num_samples)\n",
    "\n",
    "    def add_ground_truth(self, logs=None):\n",
    "        for idx, (image, label) in enumerate(self.val_data):\n",
    "            self.data_table.add_data(\n",
    "                idx,\n",
    "                wandb.Image(image),\n",
    "                np.argmax(label, axis=-1)\n",
    "            )\n",
    "\n",
    "    def add_model_predictions(self, epoch, logs=None):\n",
    "        # Get predictions\n",
    "        preds = self._inference()\n",
    "        table_idxs = self.data_table_ref.get_index()\n",
    "\n",
    "        for idx in table_idxs:\n",
    "            pred = preds[idx]\n",
    "            self.pred_table.add_data(\n",
    "                epoch,\n",
    "                self.data_table_ref.data[idx][0],\n",
    "                self.data_table_ref.data[idx][1],\n",
    "                self.data_table_ref.data[idx][2],\n",
    "                pred\n",
    "            )\n",
    "\n",
    "    def _inference(self):\n",
    "      preds = []\n",
    "      for image, label in self.val_data:\n",
    "          pred = self.model(tf.expand_dims(image, axis=0))\n",
    "          argmax_pred = tf.argmax(pred, axis=-1).numpy()[0]\n",
    "          preds.append(argmax_pred)\n",
    "\n",
    "      return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model with W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a W&B run\n",
    "run = wandb.init(\n",
    "    project=\"keras_cv_vit\",\n",
    "    save_code=False,\n",
    "    config=vars(configs),\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "model.fit(\n",
    "    trainloader,\n",
    "    epochs=configs.num_epochs,\n",
    "    validation_data=testloader,\n",
    "    callbacks=[\n",
    "        WandbMetricsLogger(log_freq=2),\n",
    "        WandbClfEvalCallback(\n",
    "            validloader = testloader,\n",
    "            data_table_columns = [\"idx\", \"image\", \"label\"],\n",
    "            pred_table_columns = [\"epoch\", \"idx\", \"image\", \"label\", \"pred\"]\n",
    "        )\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loss, eval_acc = model.evaluate(testloader)\n",
    "wandb.log({\n",
    "    \"eval_loss\": eval_loss,\n",
    "    \"eval_acc\": eval_acc\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the W&B run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
