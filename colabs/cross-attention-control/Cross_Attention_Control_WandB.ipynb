{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soumik12345/examples/blob/master/colabs/cross-attention-control/Cross_Attention_Control_WandB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üî•üî• Cross-Attention Control with Stable Diffusion + WandB Playground ü™Ñüêù\n",
        "\n",
        "<!--- @wandbcode{cross-attention-control} -->\n",
        "\n",
        "An implementation of Prompt-to-Prompt Image Editing\n",
        "with Cross Attention Control using [Stable Diffusion](https://github.com/CompVis/stable-diffusion), [HuggingFace Diffusers](https://github.com/huggingface/diffusers) and [Weights & Biases](https://wandb.ai/site)."
      ],
      "metadata": {
        "id": "bDLvF9KJf_mV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Setup required libraries"
      ],
      "metadata": {
        "id": "0J1IpII-kK0-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_rIUU9kd2cV",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "\n",
        "!pip install -q diffusers transformers ftfy wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ty2rvG3rePwe",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "\n",
        "import io\n",
        "import wandb\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm\n",
        "from difflib import SequenceMatcher\n",
        "from google.colab import files as colab_files\n",
        "\n",
        "import torch\n",
        "from torch import autocast\n",
        "\n",
        "from transformers import CLIPModel, CLIPTextModel, CLIPTokenizer\n",
        "from diffusers import (\n",
        "    AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Set up Models and Weights & Biases Run\n",
        "\n",
        "- `wandb_project`: Weights & Biases project.\n",
        "- `wandb_project`: Weights & Biases entity.\n",
        "- `huggingface_access_token`: HuggingFace Access Token. Check out this page from the official HuggingFace docs as to how to generate your own access token.\n",
        "- `config.device`: Accelerator device. Choose `mps` if you're running the code on an M1 Mac.\n",
        "- `config.model_path_clip`: Alias for pre-trained CLIP Model.\n",
        "- `config.model_path_diffusion`: Alias for pre-trained Stable Diffusion Model."
      ],
      "metadata": {
        "id": "icR8uhL9kUI2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mk2Mogzwecpo",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "wandb_project = \"cross-attention-control\" #@param {\"type\": \"string\"}\n",
        "wandb_entity = \"wandb\" #@param {\"type\": \"string\"}\n",
        "\n",
        "wandb.init(project=wandb_project, entity=wandb_entity, job_type=\"generate\")\n",
        "config = wandb.config\n",
        "\n",
        "huggingface_access_token = \"\" #@param {\"type\": \"string\"}\n",
        "torch_dtype = torch.float16\n",
        "\n",
        "config.model_precision_type = \"fp16\"\n",
        "config.device = \"cuda\" #@param['cuda', 'cpu', 'mps']\n",
        "config.model_path_clip = \"openai/clip-vit-large-patch14\" #@param['openai/clip-vit-large-patch14']\n",
        "config.model_path_diffusion = \"CompVis/stable-diffusion-v1-4\" #@param['CompVis/stable-diffusion-v1-4']\n",
        "\n",
        "\n",
        "clip_tokenizer = CLIPTokenizer.from_pretrained(config.model_path_clip)\n",
        "clip_model = CLIPModel.from_pretrained(\n",
        "    config.model_path_clip,\n",
        "    torch_dtype=torch_dtype\n",
        ")\n",
        "clip = clip_model.text_model\n",
        "\n",
        "\n",
        "model_path_diffusion = \"CompVis/stable-diffusion-v1-4\"\n",
        "unet = UNet2DConditionModel.from_pretrained(\n",
        "    model_path_diffusion,\n",
        "    subfolder=\"unet\",\n",
        "    use_auth_token=huggingface_access_token,\n",
        "    revision=config.model_precision_type,\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "vae = AutoencoderKL.from_pretrained(\n",
        "    model_path_diffusion,\n",
        "    subfolder=\"vae\",\n",
        "    use_auth_token=huggingface_access_token,\n",
        "    revision=config.model_precision_type,\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "\n",
        "unet.to(config.device)\n",
        "vae.to(config.device)\n",
        "clip.to(config.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6iZXdCVPfLDc",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "\n",
        "def init_attention_weights(weight_tuples):\n",
        "    tokens_length = clip_tokenizer.model_max_length\n",
        "    weights = torch.ones(tokens_length)\n",
        "    \n",
        "    for i, w in weight_tuples:\n",
        "        if i < tokens_length and i >= 0:\n",
        "            weights[i] = w\n",
        "    \n",
        "    \n",
        "    for name, module in unet.named_modules():\n",
        "        module_name = type(module).__name__\n",
        "        if module_name == \"CrossAttention\" and \"attn2\" in name:\n",
        "            module.last_attn_slice_weights = weights.to(config.device)\n",
        "        if module_name == \"CrossAttention\" and \"attn1\" in name:\n",
        "            module.last_attn_slice_weights = None\n",
        "    \n",
        "\n",
        "def init_attention_edit(tokens, tokens_edit):\n",
        "    tokens_length = clip_tokenizer.model_max_length\n",
        "    mask = torch.zeros(tokens_length)\n",
        "    indices_target = torch.arange(tokens_length, dtype=torch.long)\n",
        "    indices = torch.zeros(tokens_length, dtype=torch.long)\n",
        "\n",
        "    tokens = tokens.input_ids.numpy()[0]\n",
        "    tokens_edit = tokens_edit.input_ids.numpy()[0]\n",
        "    \n",
        "    for name, a0, a1, b0, b1 in SequenceMatcher(\n",
        "        None, tokens, tokens_edit\n",
        "    ).get_opcodes():\n",
        "        if b0 < tokens_length:\n",
        "            if name == \"equal\" or (name == \"replace\" and a1-a0 == b1-b0):\n",
        "                mask[b0:b1] = 1\n",
        "                indices[b0:b1] = indices_target[a0:a1]\n",
        "\n",
        "    for name, module in unet.named_modules():\n",
        "        module_name = type(module).__name__\n",
        "        if module_name == \"CrossAttention\" and \"attn2\" in name:\n",
        "            module.last_attn_slice_mask = mask.to(config.device)\n",
        "            module.last_attn_slice_indices = indices.to(config.device)\n",
        "        if module_name == \"CrossAttention\" and \"attn1\" in name:\n",
        "            module.last_attn_slice_mask = None\n",
        "            module.last_attn_slice_indices = None\n",
        "\n",
        "\n",
        "def init_attention_func():\n",
        "    def new_attention(self, query, key, value, sequence_length, dim):\n",
        "        batch_size_attention = query.shape[0]\n",
        "        hidden_states = torch.zeros(\n",
        "            (batch_size_attention, sequence_length, dim // self.heads), device=query.device, dtype=query.dtype\n",
        "        )\n",
        "        slice_size = self._slice_size if self._slice_size is not None else hidden_states.shape[0]\n",
        "        for i in range(hidden_states.shape[0] // slice_size):\n",
        "            start_idx = i * slice_size\n",
        "            end_idx = (i + 1) * slice_size\n",
        "            attn_slice = (\n",
        "                torch.einsum(\"b i d, b j d -> b i j\", query[start_idx:end_idx], key[start_idx:end_idx]) * self.scale\n",
        "            )\n",
        "            attn_slice = attn_slice.softmax(dim=-1)\n",
        "            \n",
        "            if self.use_last_attn_slice:\n",
        "                if self.last_attn_slice_mask is not None:\n",
        "                    new_attn_slice = torch.index_select(self.last_attn_slice, -1, self.last_attn_slice_indices)\n",
        "                    attn_slice = attn_slice * (1 - self.last_attn_slice_mask) + new_attn_slice * self.last_attn_slice_mask\n",
        "                else:\n",
        "                    attn_slice = self.last_attn_slice\n",
        "                \n",
        "                self.use_last_attn_slice = False\n",
        "                    \n",
        "            if self.save_last_attn_slice:\n",
        "                self.last_attn_slice = attn_slice\n",
        "                self.save_last_attn_slice = False\n",
        "                \n",
        "            if self.use_last_attn_weights and self.last_attn_slice_weights is not None:\n",
        "                attn_slice = attn_slice * self.last_attn_slice_weights\n",
        "                self.use_last_attn_weights = False\n",
        "\n",
        "            attn_slice = torch.einsum(\"b i j, b j d -> b i d\", attn_slice, value[start_idx:end_idx])\n",
        "\n",
        "            hidden_states[start_idx:end_idx] = attn_slice\n",
        "\n",
        "        # reshape hidden_states\n",
        "        hidden_states = self.reshape_batch_dim_to_heads(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "    for name, module in unet.named_modules():\n",
        "        module_name = type(module).__name__\n",
        "        if module_name == \"CrossAttention\":\n",
        "            module.last_attn_slice = None\n",
        "            module.use_last_attn_slice = False\n",
        "            module.use_last_attn_weights = False\n",
        "            module.save_last_attn_slice = False\n",
        "            module._attention = new_attention.__get__(module, type(module))\n",
        "            \n",
        "def use_last_tokens_attention(use=True):\n",
        "    for name, module in unet.named_modules():\n",
        "        module_name = type(module).__name__\n",
        "        if module_name == \"CrossAttention\" and \"attn2\" in name:\n",
        "            module.use_last_attn_slice = use\n",
        "            \n",
        "def use_last_tokens_attention_weights(use=True):\n",
        "    for name, module in unet.named_modules():\n",
        "        module_name = type(module).__name__\n",
        "        if module_name == \"CrossAttention\" and \"attn2\" in name:\n",
        "            module.use_last_attn_weights = use\n",
        "            \n",
        "def use_last_self_attention(use=True):\n",
        "    for name, module in unet.named_modules():\n",
        "        module_name = type(module).__name__\n",
        "        if module_name == \"CrossAttention\" and \"attn1\" in name:\n",
        "            module.use_last_attn_slice = use\n",
        "            \n",
        "def save_last_tokens_attention(save=True):\n",
        "    for name, module in unet.named_modules():\n",
        "        module_name = type(module).__name__\n",
        "        if module_name == \"CrossAttention\" and \"attn2\" in name:\n",
        "            module.save_last_attn_slice = save\n",
        "            \n",
        "def save_last_self_attention(save=True):\n",
        "    for name, module in unet.named_modules():\n",
        "        module_name = type(module).__name__\n",
        "        if module_name == \"CrossAttention\" and \"attn1\" in name:\n",
        "            module.save_last_attn_slice = save\n",
        "\n",
        "\n",
        "def postprocess(image):\n",
        "    image = (image / 2 + 0.5).clamp(0, 1)\n",
        "    image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
        "    image = (image[0] * 255).round().astype(\"uint8\")\n",
        "    return Image.fromarray(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_TrNFOhqQxx",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "\n",
        "@torch.no_grad()\n",
        "def stablediffusion(\n",
        "    prompt=\"\",\n",
        "    prompt_edit=\"\",\n",
        "    prompt_edit_token_weights=[],\n",
        "    prompt_edit_tokens_start=0.0,\n",
        "    prompt_edit_tokens_end=1.0,\n",
        "    prompt_edit_spatial_start=0.0,\n",
        "    prompt_edit_spatial_end=1.0,\n",
        "    guidance_scale=7.5,\n",
        "    steps=50,\n",
        "    seed=None,\n",
        "    width=512,\n",
        "    height=512,\n",
        "    init_image=None,\n",
        "    init_image_strength=0.5,\n",
        "):\n",
        "    log_key = (\n",
        "        \"Generated Image without Promp Edit\"\n",
        "        if prompt_edit == \"\"\n",
        "        else \"Generated Image with Promp Edit\"\n",
        "    )\n",
        "    print(log_key)\n",
        "\n",
        "    # Change size to multiple of 64 to prevent size mismatches inside model\n",
        "    width = width - width % 64\n",
        "    height = height - height % 64\n",
        "    \n",
        "    #If seed is None, randomly select seed from 0 to 2^32-1\n",
        "    if seed is None: seed = random.randrange(2**32 - 1)\n",
        "    generator = torch.cuda.manual_seed(seed)\n",
        "    \n",
        "    # Set inference timesteps to scheduler\n",
        "    scheduler = LMSDiscreteScheduler(\n",
        "        beta_start=0.00085,\n",
        "        beta_end=0.012,\n",
        "        beta_schedule=\"scaled_linear\",\n",
        "        num_train_timesteps=1000\n",
        "    )\n",
        "    scheduler.set_timesteps(steps)\n",
        "    \n",
        "    # Preprocess image if it exists (img2img)\n",
        "    if init_image is not None:\n",
        "        #Resize and transpose for numpy b h w c -> torch b c h w\n",
        "        init_image = init_image.resize(\n",
        "            (width, height), resample=Image.LANCZOS\n",
        "        )\n",
        "        init_image = np.array(\n",
        "            init_image\n",
        "        ).astype(np.float32) / 255.0 * 2.0 - 1.0\n",
        "        init_image = torch.from_numpy(\n",
        "            init_image[np.newaxis, ...].transpose(0, 3, 1, 2)\n",
        "        )\n",
        "        \n",
        "        # If there is alpha channel, composite alpha for white,\n",
        "        # as the diffusion model does not support alpha channel\n",
        "        if init_image.shape[1] > 3:\n",
        "            init_image = init_image[:, :3] * init_image[:, 3:] + (\n",
        "                1 - init_image[:, 3:]\n",
        "            )\n",
        "            \n",
        "        #Move image to GPU\n",
        "        init_image = init_image.to(config.device)\n",
        "        \n",
        "        #Encode image\n",
        "        with autocast(config.device):\n",
        "            init_latent = vae.encode(\n",
        "                init_image\n",
        "            ).latent_dist.sample(generator=generator) * 0.18215\n",
        "            \n",
        "        t_start = steps - int(steps * init_image_strength)\n",
        "            \n",
        "    else:\n",
        "        init_latent = torch.zeros(\n",
        "            (1, unet.in_channels, height // 8, width // 8),\n",
        "            device=config.device\n",
        "        )\n",
        "        t_start = 0\n",
        "    \n",
        "    # Generate random normal noise\n",
        "    noise = torch.randn(\n",
        "        init_latent.shape, generator=generator, device=config.device\n",
        "    )\n",
        "    latent = scheduler.add_noise(\n",
        "        init_latent, noise, t_start\n",
        "    ).to(config.device)\n",
        "    \n",
        "    # Process clip\n",
        "    with autocast(config.device):\n",
        "        tokens_unconditional = clip_tokenizer(\n",
        "            \"\",\n",
        "            padding=\"max_length\",\n",
        "            max_length=clip_tokenizer.model_max_length,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "            return_overflowing_tokens=True\n",
        "        )\n",
        "        embedding_unconditional = clip(\n",
        "            tokens_unconditional.input_ids.to(config.device)\n",
        "        ).last_hidden_state\n",
        "\n",
        "        tokens_conditional = clip_tokenizer(\n",
        "            prompt,\n",
        "            padding=\"max_length\",\n",
        "            max_length=clip_tokenizer.model_max_length,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "            return_overflowing_tokens=True\n",
        "        )\n",
        "        embedding_conditional = clip(\n",
        "            tokens_conditional.input_ids.to(config.device)\n",
        "        ).last_hidden_state\n",
        "\n",
        "        # Process prompt editing\n",
        "        if prompt_edit != \"\":\n",
        "            tokens_conditional_edit = clip_tokenizer(\n",
        "                prompt_edit,\n",
        "                padding=\"max_length\",\n",
        "                max_length=clip_tokenizer.model_max_length,\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\",\n",
        "                return_overflowing_tokens=True\n",
        "            )\n",
        "            embedding_conditional_edit = clip(\n",
        "                tokens_conditional_edit.input_ids.to(config.device)\n",
        "            ).last_hidden_state\n",
        "            \n",
        "            init_attention_edit(\n",
        "                tokens_conditional, tokens_conditional_edit\n",
        "            )\n",
        "            \n",
        "        init_attention_func()\n",
        "        init_attention_weights(prompt_edit_token_weights)\n",
        "            \n",
        "        timesteps = scheduler.timesteps[t_start:]\n",
        "        \n",
        "        for i, t in tqdm(enumerate(timesteps), total=len(timesteps)):\n",
        "            t_index = t_start + i\n",
        "\n",
        "            sigma = scheduler.sigmas[t_index]\n",
        "            latent_model_input = latent\n",
        "            latent_model_input = (\n",
        "                latent_model_input / ((sigma**2 + 1) ** 0.5)\n",
        "            ).to(unet.dtype)\n",
        "\n",
        "            # Predict the unconditional noise residual\n",
        "            noise_pred_uncond = unet(\n",
        "                latent_model_input,\n",
        "                t,\n",
        "                encoder_hidden_states=embedding_unconditional\n",
        "            ).sample\n",
        "            \n",
        "            # Prepare the Cross-Attention layers\n",
        "            if prompt_edit is not None:\n",
        "                save_last_tokens_attention()\n",
        "                save_last_self_attention()\n",
        "            else:\n",
        "                #Use weights on non-edited prompt when edit is None\n",
        "                use_last_tokens_attention_weights()\n",
        "                \n",
        "            # Predict the conditional noise residual and save\n",
        "            # the cross-attention layer activations\n",
        "            noise_pred_cond = unet(\n",
        "                latent_model_input,\n",
        "                t,\n",
        "                encoder_hidden_states=embedding_conditional\n",
        "            ).sample\n",
        "            \n",
        "            # Edit the Cross-Attention layer activations\n",
        "            if prompt_edit != \"\":\n",
        "                t_scale = t / scheduler.num_train_timesteps\n",
        "                if t_scale >= prompt_edit_tokens_start and t_scale <= prompt_edit_tokens_end:\n",
        "                    use_last_tokens_attention()\n",
        "                if t_scale >= prompt_edit_spatial_start and t_scale <= prompt_edit_spatial_end:\n",
        "                    use_last_self_attention()\n",
        "                    \n",
        "                # Use weights on edited prompt\n",
        "                use_last_tokens_attention_weights()\n",
        "\n",
        "                # Predict the edited conditional noise residual\n",
        "                # using the cross-attention masks\n",
        "                noise_pred_cond = unet(\n",
        "                    latent_model_input,\n",
        "                    t,\n",
        "                    encoder_hidden_states=embedding_conditional_edit\n",
        "                ).sample\n",
        "                \n",
        "            #Perform guidance\n",
        "            noise_pred = noise_pred_uncond + guidance_scale * (\n",
        "                noise_pred_cond - noise_pred_uncond\n",
        "            )\n",
        "\n",
        "            latent = scheduler.step(noise_pred, t_index, latent).prev_sample\n",
        "\n",
        "            wandb.log({\n",
        "                log_key: wandb.Image(\n",
        "                    postprocess(\n",
        "                        vae.decode((latent / 0.18215).to(vae.dtype)).sample\n",
        "                    )\n",
        "                )\n",
        "            }, step=i)\n",
        "\n",
        "        # scale and decode the image latents with vae\n",
        "        latent = latent / 0.18215\n",
        "        image = vae.decode(latent.to(vae.dtype)).sample\n",
        "\n",
        "    return postprocess(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Enter Prompts and Additional Configs\n",
        "\n",
        "- `config.prompt`: The prompt as a string.\n",
        "- `config.prompt_edit`: The second prompt as a string, used to edit the first prompt using cross attention, set `\"\"` to disable.\n",
        "- `config.prompt_edit_token_weights`: Values to scale the importance of the tokens in cross attention layers, as a list of tuples representing `(token id, strength)`, this is used to increase or decrease the importance of a word in the prompt, it is applied to prompt_edit when possible (if `prompt_edit` is `\"\"`, weights are applied to prompt).\n",
        "- `config.prompt_edit_tokens_start`: How strict is the generation with respect to the initial prompt, increasing this will let the network be more creative for smaller details/textures, should be smaller than `prompt_edit_tokens_end`.\n",
        "- `config.prompt_edit_tokens_end`: How strict is the generation with respect to the initial prompt, decreasing this will let the network be more creative for larger features/general scene composition, should be bigger than `prompt_edit_tokens_start`.\n",
        "- `config.prompt_edit_spatial_start`: How strict is the generation with respect to the initial image (generated from the first prompt, not from img2img), increasing this will let the network be more creative for smaller details/textures, should be smaller than `prompt_edit_spatial_end`.\n",
        "- `config.prompt_edit_spatial_end`: How strict is the generation with respect to the initial image (generated from the first prompt, not from img2img), decreasing this will let the network be more creative for larger features/general scene composition, should be bigger than `prompt_edit_spatial_start`.\n",
        "- `config.guidance_scale`: Standard classifier-free guidance strength for stable diffusion.\n",
        "- `config.steps`: Number of diffusion steps as an integer, higher usually produces better images but is slower.\n",
        "- `config.seed`: Random Seed.\n",
        "- `config.image_width`: Width of generated image.\n",
        "- `config.image_height`: Height of generated image."
      ],
      "metadata": {
        "id": "rh2De-3dl3-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_prompt_tokens(prompt):\n",
        "    tokens = clip_tokenizer(\n",
        "        prompt,\n",
        "        padding=\"max_length\",\n",
        "        max_length=clip_tokenizer.model_max_length,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "        return_overflowing_tokens=True\n",
        "    ).input_ids[0]\n",
        "    for idx, token in enumerate(tokens):\n",
        "        decoded_token = clip_tokenizer.decode(token)\n",
        "        if decoded_token == \"<|startoftext|>\":\n",
        "            continue\n",
        "        elif decoded_token == \"<|endoftext|>\":\n",
        "            break\n",
        "        else:\n",
        "            print(idx, \"->\", decoded_token)\n",
        "\n",
        "\n",
        "# the prompt as a string\n",
        "config.prompt = \"A photo of a Person with flower headpiece and elegant jewels\" #@param {\"type\": \"string\"}\n",
        "\n",
        "# the second prompt as a string, used to edit the first prompt\n",
        "# using cross attention, set \"\" to disable\n",
        "config.prompt_edit = \"A photo of a Person with butterfly headpiece and elegant jewels\" #@param {\"type\": \"string\"}\n",
        "\n",
        "display_prompt_tokens(config.prompt_edit)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jGr3m8p7sqfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgAJbmfivgjc",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# values to scale the importance of the tokens in\n",
        "# cross attention layers, as a list of tuples representing\n",
        "# (token id, strength), this is used to increase or decrease\n",
        "# the importance of a word in the prompt, it is applied to prompt_edit when possible (if prompt_edit is None, weights are applied to prompt)\n",
        "config.prompt_edit_token_weights = [(7, 4)] #@param {type:\"raw\"}\n",
        "\n",
        "# how strict is the generation with respect to the initial prompt,\n",
        "# increasing this will let the network be more creative for smaller\n",
        "# details/textures, should be smaller than prompt_edit_tokens_end\n",
        "config.prompt_edit_tokens_start = 0.0 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "\n",
        "# how strict is the generation with respect to the initial prompt,\n",
        "# decreasing this will let the network be more creative for larger\n",
        "# features/general scene composition, should be bigger than\n",
        "# prompt_edit_tokens_start\n",
        "config.prompt_edit_tokens_end = 1.0 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "\n",
        "# how strict is the generation with respect to the initial image\n",
        "# (generated from the first prompt, not from img2img), increasing\n",
        "# this will let the network be more creative for smaller\n",
        "# details/textures, should be smaller than prompt_edit_spatial_end\n",
        "config.prompt_edit_spatial_start = 0.0 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "\n",
        "# how strict is the generation with respect to the initial image\n",
        "# (generated from the first prompt, not from img2img), decreasing\n",
        "# this will let the network be more creative for larger\n",
        "# features/general scene composition, should be bigger than\n",
        "# prompt_edit_spatial_start\n",
        "config.prompt_edit_spatial_end = 0.8 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "\n",
        "# standard classifier-free guidance strength for stable diffusion\n",
        "config.guidance_scale = 7.5 #@param {type:\"slider\", min:0, max:100, step:0.1}\n",
        "\n",
        "# number of diffusion steps as an integer, higher usually produces\n",
        "# better images but is slower\n",
        "config.steps = 50 #@param {type:\"slider\", min:0, max:1000, step:1}\n",
        "\n",
        "# random seed as an integer\n",
        "config.seed = 98374234 #@param {type:\"number\"}\n",
        "\n",
        "# image width and heigh\n",
        "config.image_width = 768 #@param {type:\"slider\", min:512, max:1024, step:1}\n",
        "config.image_height = 512 #@param {type:\"slider\", min:512, max:1024, step:1}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Generate Images with Prompt and Prompt Edits.\n",
        "\n",
        "Image genetaed will be automatically logged to the respective **Weights & Biases** workspace as an interactive [**Table**](https://docs.wandb.ai/guides/data-vis) with all configs.\n",
        "\n",
        "![](https://i.imgur.com/CqIJgPg.png)"
      ],
      "metadata": {
        "id": "3mE4ZhjEn2tU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "\n",
        "generated_image_with_prompt_edit = stablediffusion(\n",
        "    prompt=config.prompt,\n",
        "    prompt_edit=config.prompt_edit,\n",
        "    prompt_edit_token_weights=config.prompt_edit_token_weights,\n",
        "    prompt_edit_tokens_start=config.prompt_edit_tokens_start,\n",
        "    prompt_edit_tokens_end=config.prompt_edit_tokens_end,\n",
        "    prompt_edit_spatial_start=config.prompt_edit_spatial_start,\n",
        "    prompt_edit_spatial_end=config.prompt_edit_spatial_end,\n",
        "    guidance_scale=config.guidance_scale,\n",
        "    steps=config.steps,\n",
        "    seed=config.seed,\n",
        "    width=config.image_width,\n",
        "    height=config.image_height,\n",
        "    init_image=None,\n",
        "    init_image_strength=0.5\n",
        ")\n",
        "\n",
        "if config.prompt_edit != \"\":\n",
        "    generated_image_without_prompt_edit = stablediffusion(\n",
        "        prompt=config.prompt,\n",
        "        prompt_edit=\"\",\n",
        "        prompt_edit_token_weights=config.prompt_edit_token_weights,\n",
        "        prompt_edit_tokens_start=config.prompt_edit_tokens_start,\n",
        "        prompt_edit_tokens_end=config.prompt_edit_tokens_end,\n",
        "        prompt_edit_spatial_start=config.prompt_edit_spatial_start,\n",
        "        prompt_edit_spatial_end=config.prompt_edit_spatial_end,\n",
        "        guidance_scale=config.guidance_scale ,\n",
        "        steps=config.steps,\n",
        "        seed=config.seed,\n",
        "        width=config.image_width,\n",
        "        height=config.image_height,\n",
        "        init_image=None,\n",
        "        init_image_strength=0.5\n",
        "    )\n",
        "    table = wandb.Table(\n",
        "        columns=[\n",
        "            \"Seed\",\n",
        "            \"Guidance Scale\",\n",
        "            \"Image Height\",\n",
        "            \"Image Width\",\n",
        "            \"Number of Steps\",\n",
        "            \"Prompt\",\n",
        "            \"Image Generated With Prompt\",\n",
        "            \"Prompt Edit\",\n",
        "            \"Edit Token Weights\",\n",
        "            \"Image Generated With Prompt Edit\"\n",
        "        ]\n",
        "    )\n",
        "    table.add_data(\n",
        "        config.seed,\n",
        "        config.guidance_scale,\n",
        "        config.image_height,\n",
        "        config.image_width,\n",
        "        config.steps,\n",
        "        config.prompt,\n",
        "        wandb.Image(generated_image_without_prompt_edit),\n",
        "        config.prompt_edit,\n",
        "        config.prompt_edit_token_weights,\n",
        "        wandb.Image(generated_image_with_prompt_edit)\n",
        "    )\n",
        "    wandb.log({\n",
        "        \"Image Editing with Cross Attention Control\": table\n",
        "    })\n",
        "\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "ltVIFaB50gf0",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**References:**\n",
        "- https://arxiv.org/abs/2208.01626\n",
        "- https://github.com/bloc97/CrossAttentionControl"
      ],
      "metadata": {
        "id": "_kr1FmAhjL76"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNmH/5+SfgXWTCeYwWj/q9T",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}