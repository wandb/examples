{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/jax/Simple_Training_Loop_in_JAX_and_Flax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "<!--- @wandbcode{stylegan-nada-colab} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing a Simple Training Loop in JAX and FLAX\n",
    "<!--- @wandbcode{stylegan-nada-colab} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages üì¶ and Basic Setup\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ù§Ô∏è Install Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q wandb flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import optax\n",
    "\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "from flax.serialization import (\n",
    "    to_state_dict, msgpack_serialize, from_bytes\n",
    ")\n",
    "\n",
    "import os\n",
    "import wandb\n",
    "import numpy as np\n",
    "from typing import Callable\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Project Configuration using **`wandb.config`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now call [**`wandb.init`**](https://docs.wandb.ai/guides/track/launch) to initialize a new job. This creates a new run in [**Weights & Biases**](https://wandb.ai/site) and launches a background process to sync data. We will also sync all the configs of our experiments with the W&B run, which makes it far easier for us to reproduce the results of the experiment later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"simple-training-loop\",\n",
    "    entity=\"jax-series\",\n",
    "    job_type=\"simple-train-loop\"\n",
    ")\n",
    "\n",
    "config = wandb.config\n",
    "config.seed = 42\n",
    "config.batch_size = 64\n",
    "config.validation_split = 0.2\n",
    "config.pooling = \"avg\"\n",
    "config.learning_rate = 1e-4\n",
    "config.epochs = 15\n",
    "\n",
    "MODULE_DICT = {\n",
    "    \"avg\": nn.avg_pool,\n",
    "    \"max\": nn.max_pool,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíø The Dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "most JAX practitioners prefer to use the **`tf.data`** API for building data loading pipelines for JAX and Flax-based machine learning workflow. In this notebook, we will build a simple data loading pipeline for the CIFAR-10 dataset using Tensorflow Datasets for Image Classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(full_train_set, test_dataset), ds_info = tfds.load(\n",
    "    'cifar10',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n",
    "\n",
    "def normalize_img(image, label):\n",
    "    image = tf.cast(image, tf.float32) / 255.\n",
    "    return image, label\n",
    "\n",
    "full_train_set = full_train_set.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "num_data = tf.data.experimental.cardinality(\n",
    "    full_train_set\n",
    ").numpy()\n",
    "print(\"Total number of data points:\", num_data)\n",
    "train_dataset = full_train_set.take(\n",
    "    num_data * (1 - config.validation_split)\n",
    ")\n",
    "val_dataset = full_train_set.take(\n",
    "    num_data * (config.validation_split)\n",
    ")\n",
    "print(\n",
    "    \"Number of train data points:\",\n",
    "    tf.data.experimental.cardinality(train_dataset).numpy()\n",
    ")\n",
    "print(\n",
    "    \"Number of val data points:\",\n",
    "    tf.data.experimental.cardinality(val_dataset).numpy()\n",
    ")\n",
    "\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.shuffle(\n",
    "    tf.data.experimental.cardinality(train_dataset).numpy()\n",
    ")\n",
    "train_dataset = train_dataset.batch(config.batch_size)\n",
    "\n",
    "val_dataset = val_dataset.cache()\n",
    "val_dataset = val_dataset.shuffle(\n",
    "    tf.data.experimental.cardinality(val_dataset).numpy()\n",
    ")\n",
    "val_dataset = val_dataset.batch(config.batch_size)\n",
    "\n",
    "\n",
    "test_dataset = test_dataset.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "print(\n",
    "    \"Number of test data points:\",\n",
    "    tf.data.experimental.cardinality(test_dataset).numpy()\n",
    "    )\n",
    "test_dataset = test_dataset.cache()\n",
    "test_dataset = test_dataset.batch(config.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úçÔ∏è Model Architecture\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now define a very simple classification convolution based neural network. Instead of some famous architecture we'll create a simple custom architecture by subclassing [**`linen.Module`**](https://flax.readthedocs.io/en/latest/_modules/flax/linen/module.html#Module).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    pool_module: Callable = nn.avg_pool\n",
    "\n",
    "    def setup(self):\n",
    "        self.conv_1 = nn.Conv(features=32, kernel_size=(3, 3))\n",
    "        self.conv_2 = nn.Conv(features=32, kernel_size=(3, 3))\n",
    "        self.conv_3 = nn.Conv(features=64, kernel_size=(3, 3))\n",
    "        self.conv_4 = nn.Conv(features=64, kernel_size=(3, 3))\n",
    "        self.conv_5 = nn.Conv(features=128, kernel_size=(3, 3))\n",
    "        self.conv_6 = nn.Conv(features=128, kernel_size=(3, 3))\n",
    "        self.dense_1 = nn.Dense(features=1024)\n",
    "        self.dense_2 = nn.Dense(features=512)\n",
    "        self.dense_output = nn.Dense(features=10)\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.relu(self.conv_1(x))\n",
    "        x = nn.relu(self.conv_2(x))\n",
    "        x = self.pool_module(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        x = nn.relu(self.conv_3(x))\n",
    "        x = nn.relu(self.conv_4(x))\n",
    "        x = self.pool_module(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        x = nn.relu(self.conv_5(x))\n",
    "        x = nn.relu(self.conv_6(x))\n",
    "        x = self.pool_module(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        x = x.reshape((x.shape[0], -1))\n",
    "        x = nn.relu(self.dense_1(x))\n",
    "        x = nn.relu(self.dense_2(x))\n",
    "        return self.dense_output(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined, the CNN Module, we would need to initialize it. However, unlike Tensorflow or PyTorch, the parameters of a Flax Module are not stored with the models themselves. We would need to initialize parameters by calling the init function, using a PRNG Key and a dummy input parameter with the same shape as the expected input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(config.seed)\n",
    "x = jnp.ones(shape=(config.batch_size, 32, 32, 3))\n",
    "model = CNN(pool_module=MODULE_DICT[config.pooling])\n",
    "params = model.init(rng, x)\n",
    "jax.tree_map(lambda x: x.shape, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.tabulate(model, rng)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we initialize the model we'll use the variables to create a [**TrainState**](https://flax.readthedocs.io/en/latest/flax.training.html#flax.training.train_state.TrainState), a utility class for handling parameter and gradient updates. This is a key feature of the new Flax version. Instead of initializing the model again and again with new variables we just update the \"state\" of the model and pass this as inputs to functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_train_state(\n",
    "    model, random_key, shape, learning_rate\n",
    ") -> train_state.TrainState:\n",
    "    # Initialize the Model\n",
    "    variables = model.init(random_key, jnp.ones(shape))\n",
    "    # Create the optimizer\n",
    "    optimizer = optax.adam(learning_rate)\n",
    "    # Create a State\n",
    "    return train_state.TrainState.create(\n",
    "        apply_fn = model.apply,\n",
    "        tx=optimizer,\n",
    "        params=variables['params']\n",
    "    )\n",
    "\n",
    "\n",
    "state = init_train_state(\n",
    "    model, rng, (config.batch_size, 32, 32, 3), config.learning_rate\n",
    ")\n",
    "print(type(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(*, logits, labels):\n",
    "    one_hot_encoded_labels = jax.nn.one_hot(labels, num_classes=10)\n",
    "    return optax.softmax_cross_entropy(\n",
    "        logits=logits, labels=one_hot_encoded_labels\n",
    "    ).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(*, logits, labels):\n",
    "  loss = cross_entropy_loss(logits=logits, labels=labels)\n",
    "  accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "  metrics = {\n",
    "      'loss': loss,\n",
    "      'accuracy': accuracy,\n",
    "  }\n",
    "  return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß± + üèó = üè† Training\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Any train step should take in two basic parameters; the state and the batch (or whatever format the input is) in question. \n",
    "\n",
    "* We usually define the loss function within this function as best practice. We get the logits from the model, using the `apply_fn` from the TrainState (which is just the apply method of the model) by passing the parameters and the input. We then compute the loss by using the logits and input and return the loss as well as the logits (this is key).\n",
    "\n",
    "* We then transform the function using `jax.value_and_grad()` transformation. Instead of `jax.grad()` which just creates a function which returns the derivative of the function. We use `jax.value_and_grad()` which returns the gradient as well as the evaluation of the function. (Notice the `has_aux` parameter, we set this to True because the loss function returns the loss as well as the logits, an auxiliary object)\n",
    "\n",
    "* We then calculate the gradients and obtain the logits by passing in the parameters of the state. Notice how the function returns both the gradients and the logits (because we used `jax.value_and_grad()` instead of `jax.grad()`) we'll later need these logits to calculate metrics after the step\n",
    "\n",
    "* We then essentially perform backpropagation by updating the TrainState using the calculated gradients by using the `.apply_gradients()` method\n",
    "\n",
    "* Calculate the metrics using the utility `compute_metrics` function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(\n",
    "    state: train_state.TrainState, batch: jnp.ndarray\n",
    "):\n",
    "    image, label = batch\n",
    "\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({'params': params}, image)\n",
    "        loss = cross_entropy_loss(logits=logits, labels=label)\n",
    "        return loss, logits\n",
    "\n",
    "    gradient_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (_, logits), grads = gradient_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    metrics = compute_metrics(logits=logits, labels=label)\n",
    "    return state, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to our `train_step` this function also takes the state and the batch. We simply perform a forward pass using the data and obtain the logits and then compute the corresponding metrics. As this is the `eval_step` we don't compute the gradients or update the parameters of the TrainState."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def eval_step(state, batch):\n",
    "    image, label = batch\n",
    "    logits = state.apply_fn({'params': state.params}, image)\n",
    "    return compute_metrics(logits=logits, labels=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(ckpt_path, state, epoch):\n",
    "    with open(ckpt_path, \"wb\") as outfile:\n",
    "        outfile.write(msgpack_serialize(to_state_dict(state)))\n",
    "    artifact = wandb.Artifact(\n",
    "        f'{wandb.run.name}-checkpoint', type='dataset'\n",
    "    )\n",
    "    artifact.add_file(ckpt_path)\n",
    "    wandb.log_artifact(artifact, aliases=[\"latest\", f\"epoch_{epoch}\"])\n",
    "\n",
    "\n",
    "def load_checkpoint(ckpt_file, state):\n",
    "    artifact = wandb.use_artifact(\n",
    "        f'{wandb.run.name}-checkpoint:latest'\n",
    "    )\n",
    "    artifact_dir = artifact.download()\n",
    "    ckpt_path = os.path.join(artifact_dir, ckpt_file)\n",
    "    with open(ckpt_path, \"rb\") as data_file:\n",
    "        byte_data = data_file.read()\n",
    "    return from_bytes(state, byte_data)\n",
    "\n",
    "\n",
    "def accumulate_metrics(metrics):\n",
    "    metrics = jax.device_get(metrics)\n",
    "    return {\n",
    "        k: np.mean([metric[k] for metric in metrics])\n",
    "        for k in metrics[0]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(\n",
    "    train_dataset,\n",
    "    eval_dataset,\n",
    "    test_dataset,\n",
    "    state: train_state.TrainState,\n",
    "    epochs: int,\n",
    "):\n",
    "    num_train_batches = tf.data.experimental.cardinality(train_dataset)\n",
    "    num_eval_batches = tf.data.experimental.cardinality(eval_dataset)\n",
    "    num_test_batches = tf.data.experimental.cardinality(test_dataset)\n",
    "    \n",
    "    for epoch in tqdm(range(1, epochs + 1)):\n",
    "\n",
    "        best_eval_loss = 1e6\n",
    "        \n",
    "        train_batch_metrics = []\n",
    "        train_datagen = iter(tfds.as_numpy(train_dataset))\n",
    "        for batch_idx in range(num_train_batches):\n",
    "            batch = next(train_datagen)\n",
    "            state, metrics = train_step(state, batch)\n",
    "            train_batch_metrics.append(metrics)\n",
    "        \n",
    "        train_batch_metrics = accumulate_metrics(train_batch_metrics)\n",
    "        print(\n",
    "            'TRAIN (%d/%d): Loss: %.4f, accuracy: %.2f' % (\n",
    "                epoch, epochs, train_batch_metrics['loss'],\n",
    "                train_batch_metrics['accuracy'] * 100\n",
    "            )\n",
    "        )\n",
    "\n",
    "        eval_batch_metrics = []\n",
    "        eval_datagen = iter(tfds.as_numpy(eval_dataset))\n",
    "        for batch_idx in range(num_eval_batches):\n",
    "            batch = next(eval_datagen)\n",
    "            metrics = eval_step(state, batch)\n",
    "            eval_batch_metrics.append(metrics)\n",
    "        \n",
    "        eval_batch_metrics = accumulate_metrics(eval_batch_metrics)\n",
    "        print(\n",
    "            'EVAL (%d/%d):  Loss: %.4f, accuracy: %.2f\\n' % (\n",
    "                epoch, epochs, eval_batch_metrics['loss'],\n",
    "                eval_batch_metrics['accuracy'] * 100\n",
    "            )\n",
    "        )\n",
    "\n",
    "        wandb.log({\n",
    "            \"Train Loss\": train_batch_metrics['loss'],\n",
    "            \"Train Accuracy\": train_batch_metrics['accuracy'],\n",
    "            \"Validation Loss\": eval_batch_metrics['loss'],\n",
    "            \"Validation Accuracy\": eval_batch_metrics['accuracy']\n",
    "        }, step=epoch)\n",
    "\n",
    "        if eval_batch_metrics['loss'] < best_eval_loss:\n",
    "            save_checkpoint(\"checkpoint.msgpack\", state, epoch)\n",
    "    \n",
    "    restored_state = load_checkpoint(\"checkpoint.msgpack\", state)\n",
    "    test_batch_metrics = []\n",
    "    test_datagen = iter(tfds.as_numpy(test_dataset))\n",
    "    for batch_idx in range(num_test_batches):\n",
    "        batch = next(test_datagen)\n",
    "        metrics = eval_step(restored_state, batch)\n",
    "        test_batch_metrics.append(metrics)\n",
    "    \n",
    "    test_batch_metrics = accumulate_metrics(test_batch_metrics)\n",
    "    print(\n",
    "        'Test: Loss: %.4f, accuracy: %.2f' % (\n",
    "            test_batch_metrics['loss'],\n",
    "            test_batch_metrics['accuracy'] * 100\n",
    "        )\n",
    "    )\n",
    "\n",
    "    wandb.log({\n",
    "        \"Test Loss\": test_batch_metrics['loss'],\n",
    "        \"Test Accuracy\": test_batch_metrics['accuracy']\n",
    "    })\n",
    "    \n",
    "    return state, restored_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, best_state = train_and_evaluate(\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    test_dataset,\n",
    "    state,\n",
    "    epochs=config.epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('nbs')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
