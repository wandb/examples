{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/jax/training_with_tfrecords_in_jax_imagenette.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q wandb flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from typing import Callable\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import optax\n",
    "\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "from flax.serialization import (\n",
    "    to_state_dict, msgpack_serialize, from_bytes\n",
    ")\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"simple-training-loop\",\n",
    "    entity=\"jax-series\",\n",
    "    job_type=\"tfrecord\"\n",
    ")\n",
    "\n",
    "config = wandb.config\n",
    "config.seed = 42\n",
    "config.image_size = 227\n",
    "config.batch_size = 64\n",
    "config.pooling = \"max\"\n",
    "config.learning_rate = 1e-4\n",
    "config.epochs = 15\n",
    "config.artifact_address = 'jax-series/simple-training-loop/imagenette-tfrecords:v3'\n",
    "config.labels = [\n",
    "    'tench', 'english_springer', 'english_springer', 'chain_saw',\n",
    "    'church', 'french_horn', 'grabage_truck', 'gas_pump',\n",
    "    'golf_ball', 'parachute'\n",
    "]\n",
    "\n",
    "MODULE_DICT = {\n",
    "    \"avg\": nn.avg_pool,\n",
    "    \"max\": nn.max_pool,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_image(image_data):\n",
    "    image = tf.image.decode_jpeg(image_data, channels=3)\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image\n",
    "\n",
    "def read_labeled_tfrecord(example):\n",
    "    feature = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"label\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "\n",
    "    example = tf.io.parse_single_example(example, feature)\n",
    "    image = decode_image(example['image'])\n",
    "    label = tf.cast(example['label'], tf.int32)\n",
    "    return image, label\n",
    "\n",
    "def load_dataset(filenames, ordered = False):\n",
    "    \n",
    "    ignore_order = tf.data.Options()\n",
    "    if not ordered:\n",
    "        ignore_order.experimental_deterministic = False \n",
    "        \n",
    "    dataset = tf.data.TFRecordDataset(\n",
    "        filenames, num_parallel_reads=AUTOTUNE\n",
    "    )\n",
    "    dataset_len = sum(1 for _ in dataset)\n",
    "    dataset = dataset.with_options(ignore_order)\n",
    "    dataset = dataset.map(\n",
    "        read_labeled_tfrecord, num_parallel_calls=AUTOTUNE\n",
    "    ) \n",
    "    return dataset, dataset_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact = wandb.use_artifact(\n",
    "    config.artifact_address, type='dataset'\n",
    ")\n",
    "artifact_dir = artifact.download()\n",
    "train_files = glob(os.path.join(artifact_dir, \"train\", \"*.tfrec\"))\n",
    "val_files = glob(os.path.join(artifact_dir, \"val\", \"*.tfrec\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset, _ = load_dataset(train_files)\n",
    "sample_dataset = sample_dataset.shuffle(1024)\n",
    "sample_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 16))\n",
    "for i in range(16):\n",
    "    x, y = next(iter(sample_dataset))\n",
    "    x, y = x.numpy(), y.numpy().tolist()\n",
    "    ax = plt.subplot(4, 4, i + 1)\n",
    "    plt.imshow(x)\n",
    "    plt.axis(\"off\")\n",
    "    name = config.labels[y]\n",
    "    ax.set_title(name, fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image, label):\n",
    "    image = tf.image.resize(\n",
    "        image, [config.image_size, config.image_size]\n",
    "    )\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def data_augment(image, label):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_hue(image, 0.01)\n",
    "    image = tf.image.random_saturation(image, 0.70, 1.30)\n",
    "    image = tf.image.random_contrast(image, 0.80, 1.20)\n",
    "    image = tf.image.random_brightness(image, 0.10)\n",
    "    return image, label\n",
    "\n",
    "def get_training_dataset(filenames, batch_size):\n",
    "    dataset, dataset_len = load_dataset(filenames, ordered = False)\n",
    "    dataset = dataset.map(\n",
    "        resize_image, num_parallel_calls=AUTOTUNE\n",
    "    )\n",
    "    dataset = dataset.map(\n",
    "        data_augment, num_parallel_calls=AUTOTUNE\n",
    "    )\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.shuffle(2048)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(AUTOTUNE)\n",
    "    return dataset, dataset_len // batch_size\n",
    "\n",
    "def get_val_dataset(filenames, batch_size):\n",
    "    dataset, dataset_len = load_dataset(filenames, ordered = True)\n",
    "    dataset = dataset.map(\n",
    "        resize_image, num_parallel_calls=AUTOTUNE\n",
    "    )\n",
    "    dataset = dataset.map(\n",
    "        data_augment, num_parallel_calls=AUTOTUNE\n",
    "    )\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(AUTOTUNE)\n",
    "    return dataset, dataset_len // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, num_train_batches = get_training_dataset(train_files, config.batch_size)\n",
    "val_dataset, num_val_batches = get_training_dataset(val_files, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    num_classes: int\n",
    "    pool_module: Callable = nn.avg_pool\n",
    "\n",
    "    def setup(self):\n",
    "        self.conv_1 = nn.Conv(\n",
    "            features=96, kernel_size=(11, 11), strides=4, padding=\"VALID\"\n",
    "        )\n",
    "        self.conv_2 = nn.Conv(\n",
    "            features=256, kernel_size=(5, 5), strides=1, padding=\"VALID\"\n",
    "        )\n",
    "        self.conv_3 = nn.Conv(\n",
    "            features=384, kernel_size=(3, 3), strides=1, padding=\"VALID\"\n",
    "        )\n",
    "        self.conv_4 = nn.Conv(\n",
    "            features=384, kernel_size=(3, 3), strides=1, padding=\"VALID\"\n",
    "        )\n",
    "        self.conv_5 = nn.Conv(\n",
    "            features=256, kernel_size=(3, 3), strides=1, padding=\"VALID\"\n",
    "        )\n",
    "        self.dense_1 = nn.Dense(features=1024)\n",
    "        self.dense_2 = nn.Dense(features=512)\n",
    "        self.dense_output = nn.Dense(features=self.num_classes)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        x = nn.relu(self.conv_1(x))\n",
    "        x = self.pool_module(x, window_shape=(3, 3), strides=(2, 2))\n",
    "        x = nn.relu(self.conv_2(x))\n",
    "        x = self.pool_module(x, window_shape=(3, 3), strides=(2, 2))\n",
    "        x = nn.relu(self.conv_3(x))\n",
    "        x = nn.relu(self.conv_4(x))\n",
    "        x = nn.relu(self.conv_5(x))\n",
    "        x = self.pool_module(x, window_shape=(3, 3), strides=(2, 2))\n",
    "        x = x.reshape((x.shape[0], -1))\n",
    "        x = nn.relu(self.dense_1(x))\n",
    "        x = nn.relu(self.dense_2(x))\n",
    "        return self.dense_output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(config.seed)\n",
    "x = jnp.ones(\n",
    "    shape=(config.batch_size, config.image_size, config.image_size, 3)\n",
    ")\n",
    "model = AlexNet(\n",
    "    num_classes=len(config.labels),\n",
    "    pool_module=MODULE_DICT[config.pooling]\n",
    ")\n",
    "params = model.init(rng, x)\n",
    "jax.tree_map(lambda x: x.shape, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.tabulate(model, rng)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_train_state(\n",
    "    model, random_key, shape, learning_rate\n",
    ") -> train_state.TrainState:\n",
    "    variables = model.init(random_key, jnp.ones(shape))\n",
    "    optimizer = optax.adam(learning_rate)\n",
    "    return train_state.TrainState.create(\n",
    "        apply_fn = model.apply,\n",
    "        tx=optimizer,\n",
    "        params=variables['params']\n",
    "    )\n",
    "\n",
    "\n",
    "state = init_train_state(\n",
    "    model=model,\n",
    "    random_key=rng,\n",
    "    shape=(config.batch_size, config.image_size, config.image_size, 3),\n",
    "    learning_rate=config.learning_rate\n",
    ")\n",
    "print(type(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(*, logits, labels):\n",
    "    one_hot_encoded_labels = jax.nn.one_hot(\n",
    "        labels, num_classes=len(config.labels)\n",
    "    )\n",
    "    return optax.softmax_cross_entropy(\n",
    "        logits=logits, labels=one_hot_encoded_labels\n",
    "    ).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(*, logits, labels):\n",
    "  loss = cross_entropy_loss(logits=logits, labels=labels)\n",
    "  accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "  metrics = {\n",
    "      'loss': loss,\n",
    "      'accuracy': accuracy,\n",
    "  }\n",
    "  return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(\n",
    "    state: train_state.TrainState, batch: jnp.ndarray\n",
    "):\n",
    "    image, label = batch\n",
    "\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({'params': params}, image)\n",
    "        loss = cross_entropy_loss(logits=logits, labels=label)\n",
    "        return loss, logits\n",
    "\n",
    "    gradient_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (_, logits), grads = gradient_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    metrics = compute_metrics(logits=logits, labels=label)\n",
    "    return state, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def eval_step(state, batch):\n",
    "    image, label = batch\n",
    "    logits = state.apply_fn({'params': state.params}, image)\n",
    "    return compute_metrics(logits=logits, labels=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(ckpt_path, state, epoch):\n",
    "    with open(ckpt_path, \"wb\") as outfile:\n",
    "        outfile.write(msgpack_serialize(to_state_dict(state)))\n",
    "    artifact = wandb.Artifact(\n",
    "        f'{wandb.run.name}-checkpoint', type='dataset'\n",
    "    )\n",
    "    artifact.add_file(ckpt_path)\n",
    "    wandb.log_artifact(artifact, aliases=[\"latest\", f\"epoch_{epoch}\"])\n",
    "\n",
    "\n",
    "def load_checkpoint(ckpt_file, state):\n",
    "    artifact = wandb.use_artifact(\n",
    "        f'{wandb.run.name}-checkpoint:latest'\n",
    "    )\n",
    "    artifact_dir = artifact.download()\n",
    "    ckpt_path = os.path.join(artifact_dir, ckpt_file)\n",
    "    with open(ckpt_path, \"rb\") as data_file:\n",
    "        byte_data = data_file.read()\n",
    "    return from_bytes(state, byte_data)\n",
    "\n",
    "\n",
    "def accumulate_metrics(metrics):\n",
    "    metrics = jax.device_get(metrics)\n",
    "    return {\n",
    "        k: np.mean([metric[k] for metric in metrics])\n",
    "        for k in metrics[0]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(\n",
    "    train_dataset,\n",
    "    eval_dataset,\n",
    "    num_train_batches,\n",
    "    num_eval_batches,\n",
    "    state: train_state.TrainState,\n",
    "    epochs: int,\n",
    "):\n",
    "    for epoch in tqdm(range(1, epochs + 1)):\n",
    "\n",
    "        best_eval_loss = 1e6\n",
    "        \n",
    "        train_batch_metrics = []\n",
    "        train_datagen = iter(tfds.as_numpy(train_dataset))\n",
    "        for batch_idx in range(num_train_batches):\n",
    "            batch = next(train_datagen)\n",
    "            state, metrics = train_step(state, batch)\n",
    "            train_batch_metrics.append(metrics)\n",
    "        \n",
    "        train_batch_metrics = accumulate_metrics(train_batch_metrics)\n",
    "        print(\n",
    "            'TRAIN (%d/%d): Loss: %.4f, accuracy: %.2f' % (\n",
    "                epoch, epochs, train_batch_metrics['loss'],\n",
    "                train_batch_metrics['accuracy'] * 100\n",
    "            )\n",
    "        )\n",
    "\n",
    "        eval_batch_metrics = []\n",
    "        eval_datagen = iter(tfds.as_numpy(eval_dataset))\n",
    "        for batch_idx in range(num_eval_batches):\n",
    "            batch = next(eval_datagen)\n",
    "            metrics = eval_step(state, batch)\n",
    "            eval_batch_metrics.append(metrics)\n",
    "        \n",
    "        eval_batch_metrics = accumulate_metrics(eval_batch_metrics)\n",
    "        print(\n",
    "            'EVAL (%d/%d):  Loss: %.4f, accuracy: %.2f\\n' % (\n",
    "                epoch, epochs, eval_batch_metrics['loss'],\n",
    "                eval_batch_metrics['accuracy'] * 100\n",
    "            )\n",
    "        )\n",
    "\n",
    "        wandb.log({\n",
    "            \"Train Loss\": train_batch_metrics['loss'],\n",
    "            \"Train Accuracy\": train_batch_metrics['accuracy'],\n",
    "            \"Validation Loss\": eval_batch_metrics['loss'],\n",
    "            \"Validation Accuracy\": eval_batch_metrics['accuracy']\n",
    "        }, step=epoch)\n",
    "\n",
    "        if eval_batch_metrics['loss'] < best_eval_loss:\n",
    "            save_checkpoint(\"checkpoint.msgpack\", state, epoch)\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = train_and_evaluate(\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    num_train_batches,\n",
    "    num_val_batches,\n",
    "    state,\n",
    "    epochs=config.epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('nbs')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
