{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "W-26KlXuiXul"
      },
      "source": [
        "<img src=\"https://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
        "<!--- @wandbcode{audiocraft} -->\n",
        "\n",
        "# üé∏ Generating Music using [Audiocraft](https://github.com/facebookresearch/audiocraft) and W&B üêù\n",
        "\n",
        "<!--- @wandbcode{musicgen-colab} -->\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/audiocraft/AudioCraft.ipynb)\n",
        "\n",
        "In this notebook we demonstrate how you can generate music and other types of audio from text prompts or generate new music from existing music using SoTA models such as [MusicGen](https://github.com/facebookresearch/audiocraft/blob/main/docs/MUSICGEN.md) and [AudioGen](https://github.com/facebookresearch/audiocraft/blob/main/docs/AUDIOGEN.md) from [Audiocraft](https://github.com/facebookresearch/audiocraft) and play and visualize them using [Weights & Biases](https://wandb.ai/site).\n",
        "\n",
        "If you want to know more about the underlying architectures for MusicGen and AudioGen and explore some cool audio samples generated by these models, you can check out [this W&B report](http://wandb.me/audiocraft_2mp)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EZU3hg4B1om6",
        "outputId": "3311a45d-35c3-49e8-cbd5-4618386fa2a1"
      },
      "outputs": [],
      "source": [
        "# @title Install AudioCraft + WandB\n",
        "!pip install -U git+https://git@github.com/facebookresearch/audiocraft#egg=audiocraft\n",
        "!pip install -qq -U wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RerQaiZt14r8"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import os\n",
        "import random\n",
        "from tempfile import TemporaryDirectory\n",
        "\n",
        "from scipy import signal\n",
        "from scipy.io import wavfile\n",
        "\n",
        "import torchaudio\n",
        "from audiocraft.models import AudioGen, MusicGen, MultiBandDiffusion\n",
        "from audiocraft.data.audio import audio_write\n",
        "\n",
        "import wandb\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "3MTX8GoE7AzN",
        "outputId": "ebe733d0-3a21-47e5-d217-89a622cafc62"
      },
      "outputs": [],
      "source": [
        "# @title ## Audio Generation Configs\n",
        "\n",
        "# @markdown In this section, you can interact with the user interface to chose the models you want to use to generate audio, prompts and other configs. Once you execute this cell, it initializes a [wandb run](https://docs.wandb.ai/guides/runs) which will be used to automatically log all the generated audio along with all the prompts and configs, to ensure your AI-generated music is never lost and your experiments are always reproducible and easy to share. \n",
        "\n",
        "# @markdown **Note:** If you have provided prompts, you will be prompted to provide an audio file in addition to the prompts to condition the model. If you don't want to provide a file as an additional condition to the model, just press on the `cancel` button.\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown WandB Project Name\n",
        "project_name = \"audiocraft\" # @param {type:\"string\"}\n",
        "\n",
        "wandb.init(project=project_name, job_type=\"musicgen/inference\")\n",
        "\n",
        "config = wandb.config\n",
        "\n",
        "# @markdown Select the Model for audio generation supported by [AudioCraft](https://github.com/facebookresearch/audiocraft). You can select either the MusicGen model variants (great for generating music) or the AudioGen model variants (great for generating non-musical audio). Also note that you can run all variants of MusicGen except the `large` one on the free-tier Colab GPU.\n",
        "model_name = \"musicgen-small\" # @param [\"musicgen-small\", \"musicgen-medium\", \"musicgen-large\", \"musicgen-melody\", \"audiogen-medium\"]\n",
        "config.model_name = \"facebook/\" + model_name if model_name == \"audiogen-medium\" else model_name\n",
        "\n",
        "# @markdown Whether to enable [MultiBand Diffusion](https://github.com/facebookresearch/audiocraft/blob/main/docs/MBD.md) or not. MultiBand diffusion is a collection of 4 models that can decode tokens from EnCodec tokenizer into waveform audio. Note that enabling this increases the time required to generate the audio.\n",
        "enable_multi_band_diffusion = True # @param {type:\"boolean\"}\n",
        "# config.enable_multi_band_diffusion = enable_multi_band_diffusion\n",
        "\n",
        "if \"musicgen\" not in model_name:\n",
        "    wandb.termwarn(\"Multi-band Diffusion is only available for Musicgen\")\n",
        "    config.enable_multi_band_diffusion = False\n",
        "else:\n",
        "    config.enable_multi_band_diffusion = enable_multi_band_diffusion\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown ## Conditional Generation Configs\n",
        "\n",
        "# @markdown The prompt for generating audio. You can give multiple prompts separated by `|` in the input. You can also leave it blank for unconditional generation.\n",
        "config.prompts = \"happy rock | energetic EDM | sad jazz\" # @param {type:\"string\"}\n",
        "\n",
        "descriptions = [prompt.strip() for prompt in config.prompts.split(\"|\")]\n",
        "config.is_unconditional = config.prompts.strip() == \"\"\n",
        "\n",
        "input_audio, input_sampling_rate, wandb_input_audio = None, None, None\n",
        "if not config.is_unconditional:\n",
        "    input_audio_file = files.upload()\n",
        "    if input_audio_file != {}:\n",
        "        if config.model_name == \"facebook/audiogen-medium\":\n",
        "            error = f\"{config.model_name} does not support audio-based conditioning\"\n",
        "            raise ValueError(error)\n",
        "        wandb_input_audio = wandb.Audio(list(input_audio_file.keys())[0])\n",
        "        input_audio, input_sampling_rate = torchaudio.load(\n",
        "            list(input_audio_file.keys())[0]\n",
        "        )\n",
        "        config.input_audio_available = True\n",
        "    else:\n",
        "        config.input_audio_available = False\n",
        "else:\n",
        "    if config.model_name == \"facebook/audiogen-medium\":\n",
        "        error = f\"{config.model_name} does not support unconditional generration\"\n",
        "        raise ValueError(error)\n",
        "\n",
        "# @markdown Number of audio samples generated, this is relevant only for unconditional generation, i.e, if `config.prompts` is left blank.\n",
        "config.num_samples = 4 # @param {type:\"slider\", min:1, max:10, step:1}\n",
        "\n",
        "# @markdown Specify the random seed\n",
        "seed = None # @param {type:\"raw\"}\n",
        "\n",
        "max_seed = int(1024 * 1024 * 1024)\n",
        "if not isinstance(seed, int):\n",
        "    seed = random.randint(1, max_seed)\n",
        "if seed < 0:\n",
        "    seed = - seed\n",
        "seed = seed % max_seed\n",
        "config.seed = seed\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown ## Generation Parameters\n",
        "# @markdown Use sampling if True, else do argmax decoding\n",
        "config.use_sampling = True # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown `top_k` used for sampling; limits us to `k` number of  of the top tokens to consider.\n",
        "config.top_k = 250 # @param {type:\"slider\", min:0, max:1000, step:1}\n",
        "\n",
        "# @markdown `top_p` used for sampling; limits us to the top tokens within a probability mass `p`\n",
        "config.top_p = 0.0 # @param {type:\"slider\", min:0, max:1.0, step:0.01}\n",
        "\n",
        "# @markdown Softmax temperature parameter\n",
        "config.temperature = 1.0 # @param {type:\"slider\", min:0, max:1.0, step:0.01}\n",
        "\n",
        "# @markdown Duration of the generated waveform\n",
        "config.duration = 10 # @param {type:\"slider\", min:1, max:30, step:1}\n",
        "\n",
        "# @markdown Coefficient used for classifier free guidance\n",
        "config.cfg_coef = 3 # @param {type:\"slider\", min:1, max:100, step:1}\n",
        "\n",
        "# @markdown Whether to perform 2 forward for Classifier Free Guidance instead of batching together the two. This has some impact on how things are padded but seems to have little impact in practice.\n",
        "config.two_step_cfg = False # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown When doing extended generation (i.e. more than 30 seconds), by how much should we extend the audio each time. Larger values will mean less context is preserved, and shorter value will require extra computations.\n",
        "config.extend_stride = 0 # @param {type:\"slider\", min:0, max:30, step:1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfM8rhVX7ES9",
        "outputId": "a935173b-382a-4514-97fc-eec12e188379"
      },
      "outputs": [],
      "source": [
        "# @title Generate Audio using MusicGen\n",
        "\n",
        "# @markdown In this section, the audio is generated using the configs, specified in the aforementioned section. If you wish to peek behind the curtain and checkout the code, click on the `Show Code` button. In order to know about the different APIs for audio generation, visit the [official audiocraft documentations](https://facebookresearch.github.io/audiocraft/api_docs/audiocraft/index.html).\n",
        "\n",
        "model = None\n",
        "if config.model_name == \"facebook/audiogen-medium\":\n",
        "    model = AudioGen.get_pretrained(config.model_name)\n",
        "elif \"musicgen\" in config.model_name:\n",
        "    model = MusicGen.get_pretrained(config.model_name.split(\"-\")[-1])\n",
        "\n",
        "multi_band_diffusion = None\n",
        "if config.enable_multi_band_diffusion:\n",
        "    multi_band_diffusion = MultiBandDiffusion.get_mbd_musicgen()\n",
        "\n",
        "model.set_generation_params(\n",
        "    use_sampling=config.use_sampling,\n",
        "    top_k=config.top_k,\n",
        "    top_p=config.top_p,\n",
        "    temperature=config.temperature,\n",
        "    duration=config.duration,\n",
        "    cfg_coef=config.cfg_coef,\n",
        "    two_step_cfg=config.two_step_cfg,\n",
        "    extend_stride=config.extend_stride\n",
        ")\n",
        "\n",
        "generated_wav, tokens = None, None\n",
        "if config.is_unconditional:\n",
        "    if input_audio is None:\n",
        "        if \"musicgen\" in config.model_name:\n",
        "            generated_wav, tokens = model.generate_unconditional(\n",
        "                num_samples=config.num_samples,\n",
        "                progress=True,\n",
        "                return_tokens=True\n",
        "            )\n",
        "        else:\n",
        "            generated_wav = model.generate_unconditional(\n",
        "                num_samples=config.num_samples,\n",
        "                progress=True,\n",
        "            )\n",
        "    else:\n",
        "        if \"musicgen\" in config.model_name:\n",
        "            generated_wav, tokens = model.generate_with_chroma(\n",
        "                descriptions,\n",
        "                input_audio[None].expand(3, -1, -1),\n",
        "                input_sampling_rate,\n",
        "                return_tokens=True\n",
        "            )\n",
        "        else:\n",
        "            generated_wav = model.generate_with_chroma(\n",
        "                descriptions,\n",
        "                input_audio[None].expand(3, -1, -1),\n",
        "                input_sampling_rate,\n",
        "            )\n",
        "else:\n",
        "    if \"musicgen\" in config.model_name:\n",
        "        generated_wav, tokens = model.generate(\n",
        "            descriptions,\n",
        "            progress=True,\n",
        "            return_tokens=True\n",
        "        )\n",
        "    else:\n",
        "        generated_wav = model.generate(\n",
        "            descriptions,\n",
        "            progress=True,\n",
        "        )\n",
        "\n",
        "generated_wav_diffusion = None\n",
        "if config.enable_multi_band_diffusion:\n",
        "    generated_wav_diffusion = multi_band_diffusion.tokens_to_wav(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "_n-1RthFVPYN",
        "outputId": "ac7bbc70-8114-4ef7-ef64-ecae9ba898cf"
      },
      "outputs": [],
      "source": [
        "# @title Log Audio to Weights & Biases Dashboard\n",
        "\n",
        "# @markdown In this section, we log the generated audio to Weights & Biases where you can listen and visualize them using an interactive audio player and waveform visualizer. Also, shoutout to [Atanu Sarkar](https://github.com/mratanusarkar) for building the spectrogram viusalization function which lets you visualize the spectrogram of the generated audio inside a [`wandb.Table`](https://docs.wandb.ai/guides/tables/tables-walkthrough).\n",
        "\n",
        "def get_spectrogram(audio_file, output_file):\n",
        "    sample_rate, samples = wavfile.read(audio_file)\n",
        "    frequencies, times, Sxx = signal.spectrogram(samples, sample_rate)\n",
        "\n",
        "    log_Sxx = 10 * np.log10(Sxx + 1e-10)\n",
        "    vmin = np.percentile(log_Sxx, 5)\n",
        "    vmax = np.percentile(log_Sxx, 95)\n",
        "\n",
        "    mean_spectrum = np.mean(log_Sxx, axis=1)\n",
        "    threshold_low = np.percentile(mean_spectrum, 5)\n",
        "    threshold_high = np.percentile(mean_spectrum, 95)\n",
        "\n",
        "    freq_indices = np.where(mean_spectrum > threshold_low)\n",
        "    freq_min = 20\n",
        "    freq_max = frequencies[freq_indices].max()\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    cmap = plt.get_cmap('magma')\n",
        "\n",
        "    ax.pcolormesh(\n",
        "        times,\n",
        "        frequencies,\n",
        "        log_Sxx,\n",
        "        shading='gouraud',\n",
        "        cmap=cmap,\n",
        "        vmin=vmin,\n",
        "        vmax=vmax\n",
        "    )\n",
        "    ax.axis('off')\n",
        "    ax.set_ylim([freq_min, freq_max])\n",
        "\n",
        "    plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
        "    plt.savefig(\n",
        "        output_file, format='png', bbox_inches='tight', pad_inches=0\n",
        "    )\n",
        "    plt.close()\n",
        "\n",
        "    return wandb.Image(output_file)\n",
        "\n",
        "\n",
        "temp_dir = TemporaryDirectory()\n",
        "columns = [\"Model\", \"Prompt\", \"Generated-Audio\", \"Spectrogram\", \"Seed\"]\n",
        "if input_audio is not None:\n",
        "    columns.insert(2, \"Input-Audio\")\n",
        "if config.enable_multi_band_diffusion:\n",
        "    columns.insert(4, \"Generated-Audio-Diffusion\")\n",
        "    columns.insert(5, \"Spectrogram-Diffusion\")\n",
        "wandb_table = wandb.Table(columns=columns)\n",
        "\n",
        "for idx, wav in enumerate(generated_wav):\n",
        "\n",
        "    file_name = os.path.join(temp_dir.name, str(idx))\n",
        "    audio_write(\n",
        "        file_name,\n",
        "        wav.cpu(),\n",
        "        model.sample_rate,\n",
        "        strategy=\"loudness\",\n",
        "        loudness_compressor=True,\n",
        "    )\n",
        "    wandb_audio = wandb.Audio(file_name +  \".wav\")\n",
        "    wandb.log({\"Generated-Audio\": wandb_audio}, commit=False)\n",
        "\n",
        "    file_name_diffusion, wandb_diffusion_audio = None, None\n",
        "    if config.enable_multi_band_diffusion:\n",
        "        file_name_diffusion = os.path.join(\n",
        "            temp_dir.name, str(idx) + \"_diffusion\"\n",
        "        )\n",
        "        audio_write(\n",
        "            file_name_diffusion,\n",
        "            generated_wav_diffusion[idx].cpu(),\n",
        "            model.sample_rate,\n",
        "            strategy=\"loudness\",\n",
        "            loudness_compressor=True,\n",
        "        )\n",
        "        wandb_diffusion_audio = wandb.Audio(file_name_diffusion +  \".wav\")\n",
        "        wandb.log(\n",
        "            {\"Generated-Audio-Diffusion\": wandb_diffusion_audio},\n",
        "            commit=False\n",
        "        )\n",
        "\n",
        "    wandb.log({}, commit=True)\n",
        "\n",
        "    desc = descriptions[idx] if len(descriptions) > 1 else config.prompts\n",
        "    wandb_table_row = [\n",
        "        model_name,\n",
        "        desc,\n",
        "        wandb_audio,\n",
        "        get_spectrogram(\n",
        "            audio_file=file_name +  \".wav\",\n",
        "            output_file=os.path.join(temp_dir.name, str(idx) + \".png\")\n",
        "        ),\n",
        "        config.seed\n",
        "    ]\n",
        "    if input_audio is not None:\n",
        "        wandb_table_row.insert(2, wandb_input_audio)\n",
        "    if config.enable_multi_band_diffusion:\n",
        "        wandb_table_row.insert(4, wandb_diffusion_audio)\n",
        "        wandb_table_row.insert(\n",
        "            5,\n",
        "            get_spectrogram(\n",
        "                audio_file=file_name_diffusion +  \".wav\",\n",
        "                output_file=os.path.join(\n",
        "                    temp_dir.name, str(idx) + \"_diffusion.png\"\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "    wandb_table.add_data(*wandb_table_row)\n",
        "\n",
        "wandb.log({\"Generated-Audio-Table\": wandb_table})\n",
        "\n",
        "wandb.finish()\n",
        "temp_dir.cleanup()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is how the W&B Table looks like with the interactive audio player, waveform visualizer and spectrogram visualization along with the prompts and other configs. Note that the notebook automatically sets the seed if you leave it blank, so your experiments are always reproducible.\n",
        "\n",
        "![](https://github.com/wandb/examples/blob/example/audiocraft/colabs/audiocraft/assets/music_gen.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
