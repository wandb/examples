{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-log/Log_a_Confusion_Matrix_with_W&B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "<!--- @wandbcode{confusion_matrix} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
    "\n",
    "<!--- @wandbcode{confusion_matrix} -->\n",
    "\n",
    "# Plot a Confusion Matrix with W&B\n",
    "\n",
    "How to log a [confusion matrix](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html) with [Vega](https://vega.github.io/vega/docs/) in [Weights & Biases](https://www.wandb.com).\n",
    "\n",
    "## Method: wandb.plot.confusion_matrix()\n",
    "\n",
    "- More info and customization details: [Confusion Matrix](https://wandb.ai/wandb/plots/reports/Confusion-Matrix--VmlldzozMDg1NTM)\n",
    "- More examples in this W&B project: [Custom Charts](https://app.wandb.ai/demo-team/custom-charts).\n",
    "\n",
    "This Colab explores a transfer learning problem: finetuning InceptionV3 with ImageNet weights to identify 10 types of living things (birds, plants, insects, etc) from 10K photos via [iNaturalist 2017](https://github.com/visipedia/inat_comp).\n",
    "\n",
    "![confusion_matrix](https://i.imgur.com/rvKx8RF.png)\n",
    "\n",
    "Note: Hyperparameters like number of epochs and training dataset size are set to minimum values here for demo efficiency. On the full training data, the model should get to the low 80s in validation accuracy within an epoch or so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Download data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: **this stage might take a few minutes (~3.6GB of data)**. If you end up needing to rerun this cell, comment out the first capture line (change ```%%capture``` to ```#%%capture``` ) so you can respond to the prompt about re-downloading the dataset (and see the progress bar).\n",
    "\n",
    "Download sample data: 10,000 training images and 2,000 validation images from the [iNaturalist dataset](https://github.com/visipedia/inat_comp), evenly distributed across 10 classes of living things like birds, insects, plants, and mammals (names given in Latinâ€”so Aves, Insecta, Plantae, etc :). We will fine-tune a convolutional neural network already trained on ImageNet on this task: given a photo of a living thing, correctly classify it into one of the 10 classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!curl -SL https://storage.googleapis.com/wandb_datasets/nature_12K.zip > nature_12K.zip\n",
    "!unzip nature_12K.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install dependencies\n",
    "\n",
    "Install tensorflow and wandb; log in to wandb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow -qqq\n",
    "!pip install wandb -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training code\n",
    "\n",
    "Feel free to try different values for \"NUM_TRAIN\" and \"NUM_EPOCHS\" below so you can see a variety of PR curves (generally better ones with more training examples/longer training time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this determines the name of your wandb project, where all your\n",
    "# runs will be logged\n",
    "PROJECT_NAME = \"confusion_matrix\"\n",
    "\n",
    "# EXPERIMENT CONFIG\n",
    "#---------------------------\n",
    "# try changing the number of training examples\n",
    "# to generate a range of different models\n",
    "NUM_TRAIN = 100 # try 500, 1000, 2000, or max 10000\n",
    "NUM_EPOCHS = 1 # try 3, 5, or as many as you like\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "# local paths to data\n",
    "train_data = \"inaturalist_12K/train\"\n",
    "val_data = \"inaturalist_12K/val\"\n",
    "\n",
    "# experiment configuration saved to W&B\n",
    "config_defaults = {\n",
    "  # number of images used to train--set low for demo training speed\n",
    "  # you can set this up to 10000 for the full dataset\n",
    "  # GOOD CONFIG TO TRY: 100, 500, 1000, 2000\n",
    "  \"num_train\" : NUM_TRAIN, # up to 10000,\n",
    "  # number of images used to validate--set low for demo training speed\n",
    "  # you can set this up to 2000 for the full dataset\n",
    "  \"num_val\" : 500, #2000,\n",
    "  \"num_classes\" : 10,\n",
    "  \"fc_size\" : 1024,\n",
    "\n",
    "  # inceptionV3 settings\n",
    "  \"img_width\" : 299,\n",
    "  \"img_height\": 299,\n",
    "  \"batch_size\" : 32,\n",
    "\n",
    "  # number of epochs--set low for demo training speed\n",
    "  # you can set this up to 5, 10, or more for better results\n",
    "  # GOOD CONFIG TO TRY: 3, 5, 10\n",
    "  \"pretrain_epochs\" : NUM_EPOCHS, #5,\n",
    "  # number of validation data batches to use when computing metrics\n",
    "  # at the end of each epoch\n",
    "  \"num_log_batches\": 15,\n",
    "  # random seed\n",
    "  \"random_seed\": 23\n",
    "}\n",
    "\n",
    "def build_model(fc_size, num_classes):\n",
    "  \"\"\"Load InceptionV3 with ImageNet weights, freeze it,\n",
    "  and attach a finetuning top for this classification task\"\"\"\n",
    "  # load InceptionV3 as base\n",
    "  base = InceptionV3(weights=\"imagenet\", include_top=\"False\")\n",
    "  # freeze base layers\n",
    "  for layer in base.layers:\n",
    "    layer.trainable = False\n",
    "  x = base.get_layer('mixed10').output \n",
    "\n",
    "  # attach a fine-tuning layer\n",
    "  x = GlobalAveragePooling2D()(x)\n",
    "  x = Dense(fc_size, activation='relu')(x)\n",
    "  guesses = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "  model = Model(inputs=base.input, outputs=guesses)\n",
    "  model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def pretrain():\n",
    "  \"\"\" Main training loop. This is called 'pretrain' because it freezes\n",
    "  the InceptionV3 layers of the model and only trains the new top layers\n",
    "  on the new data. A subsequent training phase would unfreeze all the layers\n",
    "  and finetune the whole model on the new data\"\"\" \n",
    "  # track this experiment with wandb: all runs will be sent\n",
    "  # to the given project name\n",
    "  wandb.init(project=PROJECT_NAME, config=config_defaults)\n",
    "  cfg = wandb.config\n",
    "\n",
    "  # set random seed\n",
    "  tf.random.set_seed(cfg.random_seed)\n",
    "  # also set numpy seed to control train/val dataset split\n",
    "  np.random.seed(cfg.random_seed)\n",
    "\n",
    "  # create train and validation data generators\n",
    "  train_datagen = ImageDataGenerator(\n",
    "      rescale=1. / 255,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True)\n",
    "  val_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "  train_generator = train_datagen.flow_from_directory(\n",
    "    train_data,\n",
    "    target_size=(cfg.img_width, cfg.img_height),\n",
    "    batch_size=cfg.batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "  val_generator = val_datagen.flow_from_directory(\n",
    "    val_data,\n",
    "    target_size=(cfg.img_width, cfg.img_height),\n",
    "    batch_size=cfg.batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "  # instantiate model and callbacks\n",
    "  model = build_model(cfg.fc_size, cfg.num_classes)\n",
    "  callbacks = [WandbCallback(), PRMetrics(val_generator, cfg.num_log_batches)]\n",
    "\n",
    "  # train!\n",
    "  model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch = cfg.num_train // cfg.batch_size,\n",
    "    epochs=cfg.pretrain_epochs,\n",
    "    validation_data=val_generator,\n",
    "    callbacks = callbacks,\n",
    "    validation_steps=cfg.num_val // cfg.batch_size)\n",
    "\n",
    "  wandb.run.finish()\n",
    "  \n",
    "class PRMetrics(Callback):\n",
    "  \"\"\" Custom callback to compute metrics at the end of each training epoch\"\"\"\n",
    "  def __init__(self, generator=None, num_log_batches=1):\n",
    "    self.generator = generator\n",
    "    self.num_batches = num_log_batches\n",
    "    # store full names of classes\n",
    "    self.flat_class_names = [k for k, v in generator.class_indices.items()]\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    # collect validation data and ground truth labels from generator\n",
    "    val_data, val_labels = zip(*(self.generator[i] for i in range(self.num_batches)))\n",
    "    val_data, val_labels = np.vstack(val_data), np.vstack(val_labels)\n",
    "\n",
    "    # use the trained model to generate predictions for the given number\n",
    "    # of validation data batches (num_batches)\n",
    "    val_predictions = self.model.predict(val_data)\n",
    "    ground_truth_class_ids = val_labels.argmax(axis=1)\n",
    "    # take the argmax for each set of prediction scores\n",
    "    # to return the class id of the highest confidence prediction\n",
    "    top_pred_ids = val_predictions.argmax(axis=1)\n",
    "\n",
    "    # Log confusion matrix\n",
    "    # the key \"conf_mat\" is the id of the plot--do not change\n",
    "    # this if you want subsequent runs to show up on the same plot\n",
    "    wandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(probs=None,\n",
    "                            preds=top_pred_ids, y_true=ground_truth_class_ids,\n",
    "                            class_names=self.flat_class_names)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to launch your experiment!\n",
    "# charts will show up in your run page under the heading \"Media\" or \n",
    "# \"Custom Charts\", which you may need to click on to expand\n",
    "pretrain()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
