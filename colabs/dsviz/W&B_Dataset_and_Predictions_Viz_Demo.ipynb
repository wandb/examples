{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/gb6B4ig.png\" width=\"400\" alt=\"Weights & Biases\" />\n",
    "\n",
    "_W&B Datasets & Predictions is currently in the early-access phase. You can use it in our production service at [wandb.ai](https://wandb.ai), with [some limitations](https://docs.wandb.com/datasets-and-predictions#current-limitations). APIs are subject to change. We'd love to hear questions, comments, and ideas! Drop us a line at feedback@wandb.com._\n",
    "\n",
    "# W&B Dataset Visualization Demo\n",
    "\n",
    "This notebook demonstrates W&B's dataset visualization features. In particular we show how [W&B Artifacts](https://docs.wandb.com/artifacts) help visualize datasets and predictions, focusing on image data. We track model and data lineage and perform interactive  analysis on the resulting datasets. The overall flow will be:\n",
    "\n",
    "1. Create a dataset\n",
    "2. Split the dataset into \"train\" and \"test\"\n",
    "3. Train a model on the \"train\" dataset\n",
    "4. Load the trained model and log predictions on both datasets\n",
    "5. Analyze the results in W&B's UI\n",
    "\n",
    "You can see live interactive examples logged to W&B in this public project: [Dsviz Demo](https://wandb.ai/stacey/dsviz-demo).\n",
    "\n",
    "### Explore your training data\n",
    "<img src=\"https://raw.githubusercontent.com/staceysv/dsviz-demo/master/notebook_images/overview_data_table.png\" width=600 height=300>\n",
    "\n",
    "### Visualize your model predictions\n",
    "<img src=\"https://raw.githubusercontent.com/staceysv/dsviz-demo/master/notebook_images/overview_val_predictions.png\" width=500 height=300>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/dsviz/W%26B_Dataset_and_Predictions_Viz_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Setup\n",
    "\n",
    "## Install requirements & utils\n",
    "\n",
    "For brevity, we put utility functions for working with the dataset in `util.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install the python dependencies\n",
    "!pip install matplotlib numpy Pillow wandb\n",
    "\n",
    "# Download a util file of helper methods for this notebook\n",
    "!curl https://raw.githubusercontent.com/staceysv/dsviz-demo/master/util.py --output util.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab sometimes has problems with Pillow. If you are facing this issue, \n",
    "# uncomment the `exit()` line and run this cell. Then rerun the `!pip install`\n",
    "# cell above.\n",
    "\n",
    "# exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import wandb\n",
    "print(wandb.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Login to wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default project name where results will be logged\n",
    "WANDB_PROJECT = \"dsviz-demo\"\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set SIZE to \"TINY\", \"MEDIUM\" or (read the warning) \"LARGE\"\n",
    "# to select one of these datasets\n",
    "# TINY dataset: 50 images\n",
    "# MEDIUM dataset: 200 images\n",
    "# warning: you may run out of RAM in Colab & need to restart the notebook\n",
    "# between steps at this size\n",
    "# LARGE dataset: 400 images \n",
    "\n",
    "SIZE = \"TINY\"\n",
    "PREFIX = \"bdd\"\n",
    "\n",
    "if SIZE == \"TINY\":\n",
    "  NUM_EXAMPLES = 50\n",
    "  SPLITS = {\"train\" : 40, \"test\" : 10}\n",
    "elif SIZE == \"MEDIUM\":\n",
    "  NUM_EXAMPLES = 200\n",
    "  SPLITS = {\"train\" : 160, \"test\" : 40}\n",
    "elif SIZE == \"LARGE\": \n",
    "  NUM_EXAMPLES = 400\n",
    "  SPLITS = {\"train\" : 320, \"test\" : 80}\n",
    "\n",
    "# set globals\n",
    "IMAGE_ROOT = \"segment_demo/images/train\"\n",
    "LABEL_ROOT = \"segment_demo/labels/train\"\n",
    "\n",
    "# set global Artifact names (allowing steps to be rerun independently)\n",
    "RAW_DATA_AT = \"_\".join([PREFIX, \"raw_data\", str(NUM_EXAMPLES)])\n",
    "TRAIN_DATA_AT = \"_\".join([PREFIX, \"train_data\", str(NUM_EXAMPLES)])\n",
    "TEST_DATA_AT = \"_\".join([PREFIX, \"test_data\", str(NUM_EXAMPLES)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data\n",
    "\n",
    "Before we get started, we will download an example dataset to our local machine. This is a big dataset (958MB) so please be patient if you are on a slow connection. For brevity, we put utility functions for working with the dataset in `util.py`. After the download is complete, we will show an example of the data.\n",
    "\n",
    "**Note:** if you see the error \"``AttributeError: module 'PIL.TiffTags' has no attribute 'IFD'``\", this is likely a [Colab issue](https://github.com/facebookresearch/detectron2/issues/2231) which can be solved by restarting your runtime (header menu > Runtime > Restart runtime)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the training data: this is a subset of the Berkeley Deep Drive 100K dataset,\n",
    "# which is available at https://bdd-data.berkeley.edu/ \n",
    "!curl -SL -qq https://storage.googleapis.com/wandb_datasets/BDD100K_seg_demo.zip > BDD100K_seg_demo.zip\n",
    "!unzip -qq BDD100K_seg_demo.zip\n",
    "\n",
    "# Print the label types:\n",
    "print(\"Class Mapping:\")\n",
    "print(list(zip(util.BDD_IDS, util.BDD_CLASSES)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some example training images\n",
    "samples = [\"0004a4c0-d4dff0ad\", \"00067cfb-e535423e\", \"0010bf16-a457685b\", \"001b428f-059bac33\"]\n",
    "for sample_id in samples:\n",
    "  # raw image\n",
    "  util.show_image(\"segment_demo/images/train/{}.jpg\".format(sample_id))\n",
    "  # corresponding color label mask\n",
    "  util.show_image(\"segment_demo/labels/train/{}_train_id.png\".format(sample_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Build the dataset\n",
    "\n",
    "First, let's build a dataset for use in the rest of this project using a `wandb.Run`. A `Run` is an isolated process which can optionally depend on upstream artifacts as well as optionally produce artifacts for later consumption. In this step, we will create a `wandb.Table` during our run and output it in an artifact. This table will contain all of our raw data for later use. W&B also offers rich tools to analyze and visualize such Tables in an interactive UI.\n",
    "\n",
    "[See a live example of the raw dataset in W&B](https://wandb.ai/stacey/dsviz-demo/artifacts/raw_data/bdd_raw_data_500/afcb923f719b14370215/files/raw_examples.table.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Initialize the run\n",
    "run = wandb.init(project=WANDB_PROJECT,\n",
    "                 job_type=\"upload\",\n",
    "                 config={\n",
    "                     \"num_examples\" : NUM_EXAMPLES,\n",
    "                     \"num_train\" : SPLITS[\"train\"],\n",
    "                     \"num_test\" : SPLITS[\"test\"]\n",
    "                 })\n",
    "\n",
    "# Setup a WandB Classes object. This will give additional metadata for visuals\n",
    "class_set = wandb.Classes([{'name': name, 'id': id} \n",
    "                           for name, id in zip(util.BDD_CLASSES, util.BDD_IDS)])\n",
    "\n",
    "# Create an Artifact (versioned folder)\n",
    "artifact = wandb.Artifact(name=RAW_DATA_AT, type=\"raw_data\")\n",
    "\n",
    "# Setup a WandB Table object to hold our dataset\n",
    "columns=[\"id\", \"raw\", \"annotated\", \"color_mask\", \"raw_label\"]\n",
    "# add a column for the pixel fraction of each class label\n",
    "columns.extend([\"%_\" + c for c in util.BDD_CLASSES])\n",
    "table = wandb.Table(\n",
    "    columns=columns\n",
    ")\n",
    "\n",
    "# temporary directory to hold intermediate visualizations\n",
    "TMPDIR = \"tmp_labels\"\n",
    "if not os.path.isdir(TMPDIR):\n",
    "  os.mkdir(TMPDIR)\n",
    "\n",
    "# Fill up the table\n",
    "all_images = [f for f in os.listdir(IMAGE_ROOT)]\n",
    "for ndx in range(wandb.config.num_examples):\n",
    "  img = all_images[ndx]\n",
    "  img_file = os.path.join(IMAGE_ROOT, img)\n",
    "  train_id = img.split(\".\")[0]\n",
    "  label_file = os.path.join(LABEL_ROOT, train_id + \"_train_id.png\")\n",
    "\n",
    "  # First, we will build a wandb.Image to act as our raw example object\n",
    "  #    classes: the classes which map to masks and/or box metadata\n",
    "  #    masks: the mask metadata. In this case, we use a 2d array w\n",
    "  #           here each cell corresponds to the label (this comes directly from the dataset)\n",
    "  raw_img = wandb.Image(img_file)\n",
    "  annotated = wandb.Image(img_file, classes=class_set,\n",
    "                        masks={\"ground_truth\" : {\"mask_data\": np.array(Image.open(label_file))}})\n",
    "  \n",
    "  # Next, we create an additional image which may be helpful during analysis\n",
    "  # and compute the fraction of each image covered by each of the classes\n",
    "  # (so we can find examples with more pixels of cars vs pedestrians vs other\n",
    "  # classes of interest). This additional metadata is optional\n",
    "  color_mask = util.static_label(label_file, train_id)\n",
    "  class_fractions = util.count_pixels(label_file)\n",
    "  raw_label = wandb.Image(label_file)\n",
    "\n",
    "  # use .add_file for the files we need to reference by path\n",
    "  # (in this case the training image and the label)\n",
    "  artifact.add_file(img_file, os.path.join(\"images\", img))\n",
    "  artifact.add_file(label_file, os.path.join(\"labels\", train_id + \"_train_id.png\"))\n",
    "\n",
    "  # Finally, we add a row of our newly constructed data.\n",
    "  row = [train_id, raw_img, annotated, color_mask, raw_label]\n",
    "  row.extend(class_fractions)\n",
    "  table.add_data(*row)\n",
    "    \n",
    "# .add the table to the artifact\n",
    "artifact.add(table, \"raw_examples\")\n",
    "    \n",
    "# Finally, log the artifact\n",
    "print(\"Saving data to WandB...\")\n",
    "run.log_artifact(artifact)\n",
    "run.finish()\n",
    "print(\"... Run Complete\")\n",
    "\n",
    "# clear out the temporary files \n",
    "shutil.rmtree(TMPDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review the dataset in the Dashboard\n",
    "\n",
    "Great, now if you click on the URL above, you should land on a run page. Since we did not log any metrics, there are no charts. Click the database icon (it looks like a stack of hockey pucks) on the left panel to see this run's artifacts. You should see something similar to the following:\n",
    "\n",
    "![Raw Data](https://raw.githubusercontent.com/staceysv/dsviz-demo/master/notebook_images/first_raw_artifact.png)\n",
    "\n",
    "Click on the \"`raw_data`\" row to see an Oveview page like this:\n",
    "\n",
    "![Raw Data](https://raw.githubusercontent.com/staceysv/dsviz-demo/master/notebook_images/raw_examples_select_table.png)\n",
    "\n",
    "Clicking on the \"`raw_examples\" entry under Tables will launch an interactive data explorer to review the table we just built. Here I've advanced to a page where you can see some humans.\n",
    "\n",
    "![Raw Data](https://raw.githubusercontent.com/staceysv/dsviz-demo/master/notebook_images/raw_examples_human_page.png)\n",
    "\n",
    "You can also access this table as a json file under the \"Files\" tab, which lists all the files stored in this Artifact.\n",
    "![Files list](https://raw.githubusercontent.com/staceysv/dsviz-demo/master/notebook_images/raw_files_first_page.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Splitting the data into train and test\n",
    "\n",
    "Next, we will split the data into a train and a test dataset. Similar to before, we will launch a `Run` to perform this operation. Remember, this new execution could happen on a different machine as we will dynamically load the needed resources. In particular, we will load in the raw dataset from the last run and split it into 2 new datasets.\n",
    "\n",
    "See live examples of the [train data](https://wandb.ai/stacey/dsviz-segment/artifacts/train_data/bdd_train_data_200/4bb038903e618f8e0e49) and [test data](https://wandb.ai/stacey/dsviz-segment/artifacts/test_data/bdd_test_data_200/1d6aae785ea663167c82) artifacts in W&B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a run (you can use the job_type to better organize runs)\n",
    "run = wandb.init(project=WANDB_PROJECT, job_type=\"data_split\")\n",
    "\n",
    "# Get the latest version of the artifact. \n",
    "# Notice the name alias follows this convention: \"<ARTIFACT_NAME>:<VERSION>\"\n",
    "# When version is set to \"latest\", then the latest version will always be used.\n",
    "# However, you can pin to a version by using an alias such as \"raw_data:v0\"\n",
    "data_at = run.use_artifact(RAW_DATA_AT + \":latest\")\n",
    "    \n",
    "# Next, we .get the table by the same name that we saved it in the last run.\n",
    "data_table = data_at.get(\"raw_examples\")\n",
    "    \n",
    "# Print a row\n",
    "print(\"\\nExample Data row\\n\", data_table.data[0])\n",
    "    \n",
    "# Show an example image\n",
    "print(\"\\nExample Image\\n\")\n",
    "plt.imshow(data_table.data[0][1].image)\n",
    "plt.show()\n",
    "    \n",
    "# Notice that a new directory was made: artifacts which is managed by wandb\n",
    "print(\"\\nArtifact Directory Contents: \\n\", os.listdir(\"artifacts\"))\n",
    "\n",
    "# Now we can build two separate artifacts for later use. We will first split the raw table into two parts,\n",
    "# then create two different artifacts, each of which will hold our new tables. We create two artifacts so that\n",
    "# in future runs, we can selectively decide which subsets of data to download.\n",
    "train_table = wandb.Table(columns=data_table.columns, data=data_table.data[:SPLITS[\"train\"]])\n",
    "test_table = wandb.Table(columns=data_table.columns, data=data_table.data[SPLITS[\"train\"]:])\n",
    "\n",
    "train_artifact = wandb.Artifact(TRAIN_DATA_AT, \"train_data\")\n",
    "test_artifact = wandb.Artifact(TEST_DATA_AT, \"test_data\")\n",
    "\n",
    "# Save the tables to the artifacts with .add\n",
    "train_artifact.add(train_table, \"train_table\")\n",
    "# Note: technically our test data has ground truth labels in this example :)\n",
    "# we could modify this table to remove the labels\n",
    "test_artifact.add(test_table, \"test_table\")\n",
    "\n",
    "# Log the artifacts out as outputs of the run\n",
    "print(\"Saving data to WandB...\")\n",
    "run.log_artifact(train_artifact)\n",
    "run.log_artifact(test_artifact)\n",
    "run.finish()\n",
    "print(\"... Run Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review the splits in the Dashboard\n",
    "Notice, in this step, the raw_data `wandb.Table` was reinstatiated and the data, images, etc... came along for the ride. This makes it easy for ML practitioners on a team to share data and assets easily. To manage this, you can see that we created an artifacts directory to save local data.\n",
    "\n",
    "Now we have two new datasets. Feel free to browse them similar to our last step. However, this time, click \"Graph View\" rather than \"Files\" to see the lineage of the artifact.\n",
    "\n",
    "Naming and typing in Artifacts are powerful and flexible. In this case, we include a short prefix (\"bdd\" for Berkeley Deep Drive 100K) and the count of items in each artifact name so that these artifacts are easier to identify and track when viewed at a glance, downloaded locally, or compared to the many other versions we may create in this project (with different total counts, data split strategies, etc).\n",
    "\n",
    "![](https://raw.githubusercontent.com/staceysv/dsviz-demo/master/notebook_images/split_train_test.png)\n",
    "\n",
    "![](https://raw.githubusercontent.com/staceysv/dsviz-demo/master/notebook_images/test_data_graph.png)\n",
    "\n",
    "We will come back to this graph view later on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Model Training\n",
    "\n",
    "Now we will train a model to predict semantic segmentation masks. This is a simple UNet in Fastai, intended to train quickly. As you can imagine, the model performance can be improved dramatically.\n",
    "\n",
    "See a [live example in W&B](https://wandb.ai/stacey/dsviz-demo/artifacts/val_epoch_preds/val_pred_55oxnv95/8ea156e10e0099fe58d8/files/val_epoch_results.table.json) of validation predictions logged during training, versioned per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from fastai.vision import *\n",
    "from fastai.callbacks.hooks import *\n",
    "from fastai.callback import Callback\n",
    "import json\n",
    "from wandb.fastai import WandbCallback\n",
    "from functools import partialmethod\n",
    "\n",
    "# where to download files for local training\n",
    "LOCAL_TRAIN_DIR = \"TRAIN_DIR\"\n",
    "\n",
    "# Setup a WandB Classes object. This will give additional metadata for visuals\n",
    "class_set = wandb.Classes([{'name': name, 'id': id} \n",
    "                           for name, id in zip(util.BDD_CLASSES, util.BDD_IDS)])\n",
    "\n",
    "# wrapper for logging masks to W&B\n",
    "def wb_mask(bg_img, pred_mask=[], true_mask=[]):\n",
    "  masks = {}\n",
    "  if len(pred_mask) > 0:\n",
    "    masks[\"prediction\"] = {\"mask_data\" : pred_mask}\n",
    "  if len(true_mask) > 0:\n",
    "    masks[\"ground truth\"] = {\"mask_data\" : true_mask}\n",
    "  return wandb.Image(bg_img, classes=class_set, masks=masks)\n",
    "\n",
    "SMOOTH = 1e-6\n",
    "# IOU loss function\n",
    "def iou(input, target):\n",
    "    target = target.squeeze(1)  # BATCH x 1 x H x W => BATCH x H x W\n",
    "    intersection = (input.argmax(dim=1) & target).float().sum((1, 2))  # Will be zero if Truth=0 or Prediction=0\n",
    "    union = (input.argmax(dim=1) | target).float().sum((1, 2))         # Will be zero if both are 0\n",
    "    iou = (intersection + SMOOTH) / (union + SMOOTH)  # We smooth our division to avoid 0/0\n",
    "    return iou.mean()\n",
    "\n",
    "# Custom callback for logging images to W&B\n",
    "class LogImagesCallback(Callback):\n",
    "\n",
    "  def __init__(self, learn):\n",
    "    self.learn = learn\n",
    "\n",
    "  # log semantic segmentation masks\n",
    "  def on_epoch_end(self, epoch, n_epochs, **kwargs):\n",
    "    # optionally limit all these to store fewer images\n",
    "    # e.g. by adding [:num_log] to every line\n",
    "    train_batch = self.learn.data.train_ds\n",
    "    train_ids = [a.stem for a in self.learn.data.train_ds.items]\n",
    "    valid_batch = self.learn.data.valid_ds\n",
    "    val_ids = [a.stem for a in self.learn.data.valid_ds.items]\n",
    "\n",
    "    train_masks = []\n",
    "    valid_masks = []\n",
    "\n",
    "    # save training and validation predictions\n",
    "    # note: we're training for 1 epoch for brevity, but this approach\n",
    "    # will create a new version of the artifact for each epoch\n",
    "    train_res_at = wandb.Artifact(\"train_pred_\" + wandb.run.id, \"train_epoch_preds\")\n",
    "    val_res_at = wandb.Artifact(\"val_pred_\" + wandb.run.id, \"val_epoch_preds\")\n",
    "    # store all final results in a single artifact across experiments and\n",
    "    # model variants to easily compare predictions\n",
    "    final_model_res_at = wandb.Artifact(\"resnet_pred\", \"model_preds\")\n",
    "\n",
    "\n",
    "    main_columns = [\"id\", \"prediction\", \"ground_truth\"]\n",
    "    # we'll track the IOU for each class\n",
    "    main_columns.extend([\"iou_\" + s for s in util.BDD_CLASSES])\n",
    "    # create tables\n",
    "    train_table = wandb.Table(columns=main_columns)\n",
    "    val_table = wandb.Table(columns=main_columns)\n",
    "    model_res_table = wandb.Table(columns=main_columns)\n",
    "\n",
    "\n",
    "    for batch_masks, batch, batch_ids, table, phase in zip([train_masks, valid_masks],\n",
    "                                                    [train_batch, valid_batch], \n",
    "                                                    [train_ids, val_ids],\n",
    "                                                    [train_table, val_table],\n",
    "                                                    [\"train\", \"val\"]):\n",
    "      for i, img in enumerate(batch):\n",
    "        # log raw image as array\n",
    "        orig_image = img[0]\n",
    "        bg_image = image2np(orig_image.data*255).astype(np.uint8)\n",
    "\n",
    "        # verify prediction from the model\n",
    "        prediction = self.learn.predict(img[0])[0]\n",
    "        prediction_mask = image2np(prediction.data).astype(np.uint8)\n",
    "\n",
    "        # ground truth mask\n",
    "        ground_truth = img[1]\n",
    "        true_mask = image2np(ground_truth.data).astype(np.uint8)\n",
    "\n",
    "        # score masks: what is the IOU for each class?\n",
    "        per_class_scores = [util.iou_flat(prediction_mask, true_mask, i) for i in util.BDD_IDS]\n",
    "        row = [str(batch_ids[i]), wb_mask(bg_image, pred_mask=prediction_mask), \n",
    "                                  wb_mask(bg_image, true_mask=true_mask)]\n",
    "        row.extend(per_class_scores)\n",
    "        table.add_data(*row)\n",
    "        # only for last epoch\n",
    "        if phase == \"val\" and epoch == n_epochs - 1:\n",
    "          model_res_table.add_data(*row)\n",
    "\n",
    "    train_res_at.add(train_table, \"train_epoch_results\")\n",
    "    val_res_at.add(val_table, \"val_epoch_results\")\n",
    "    # by reference\n",
    "    final_model_res_at.add(model_res_table, \"model_results\")\n",
    "    wandb.run.log_artifact(train_res_at)\n",
    "    wandb.run.log_artifact(val_res_at)\n",
    "    wandb.run.log_artifact(final_model_res_at)\n",
    "\n",
    "def train_model():\n",
    "  # default settings / hyperparameters\n",
    "  default_config = {\n",
    "    \"framework\" : \"fastai\",\n",
    "    \"img_size\" : (360, 640),\n",
    "    \"batch_size\" : 8, # keep small in Colab to be manageable\n",
    "    \"epochs\" : 1, # for brevity, increase for better results :)\n",
    "    \"data_split\" : 0.7,\n",
    "    \"pretrained\" : True  # whether to use pretrained encoder\n",
    "  }\n",
    "  run = wandb.init(project=WANDB_PROJECT, job_type=\"train\", \\\n",
    "             config=default_config)\n",
    "  \n",
    "  cfg = wandb.config\n",
    "  # resnet34 may also be manageable in this Colab :)\n",
    "  cfg.encoder = \"resnet18\"\n",
    "  encoder = models.resnet18\n",
    "\n",
    "  # best config for hyperparameters from past sweeps\n",
    "  cfg.weight_decay = 0.08173\n",
    "  cfg.bn_weight_decay = True   # whether weight decay is applied on batch norm layers\n",
    "  cfg.one_cycle = True         # use the \"1cycle\" policy -> https://arxiv.org/abs/1803.09820\n",
    "  cfg.learning_rate = 0.002\n",
    "\n",
    "  # fetch the latest verstion of the training data\n",
    "  train_artifact = run.use_artifact(TRAIN_DATA_AT + \":latest\")\n",
    "  # download it locally (required for this Fastai approach)\n",
    "  train_dir = train_artifact.download(LOCAL_TRAIN_DIR)\n",
    "\n",
    "  # map each image file to its label file\n",
    "  get_label = lambda x: str(x.parents[0]).replace(\"images\", \"labels\") + \"/\"  + str(x.stem) + \"_train_id.png\"\n",
    "\n",
    "  # load data into train & validation sets\n",
    "  src = (SegmentationItemList.from_folder(\"./\" + LOCAL_TRAIN_DIR + \"/images\")\n",
    "       .split_by_rand_pct(cfg.data_split)\n",
    "       .label_from_func(get_label, classes=util.BDD_CLASSES))\n",
    "  # resize, augment, load in batch & normalize (so we can use pre-trained networks)\n",
    "  data = (src.transform(get_transforms(), size=cfg.img_size, tfm_y=True)\n",
    "        .databunch(bs=cfg.batch_size)\n",
    "        .normalize(imagenet_stats))\n",
    "\n",
    "  # define UNet model\n",
    "  learn = unet_learner(\n",
    "    data,\n",
    "    arch=encoder,\n",
    "    pretrained=cfg.pretrained,\n",
    "    metrics=iou,\n",
    "    wd=cfg.weight_decay,\n",
    "    bn_wd=cfg.bn_weight_decay,\n",
    "    callback_fns=partial(WandbCallback, save_model=True, monitor='iou'))\n",
    "  \n",
    "  # train model!\n",
    "  learn.fit_one_cycle(\n",
    "        cfg.epochs,\n",
    "        max_lr=slice(cfg.learning_rate),\n",
    "        callbacks=[LogImagesCallback(learn)])\n",
    "  \n",
    "  # save the trained model as an artifact\n",
    "  # note: make this name more descriptive as you experiment so it's easier to track\n",
    "  model_name = cfg.encoder \n",
    "  saved_model = wandb.Artifact(model_name, type=\"model\")\n",
    "  # export trained model\n",
    "  learn.export(file = Path(model_name + \".pkl\"))\n",
    "  local_model_file = LOCAL_TRAIN_DIR + \"/images/\" + model_name + \".pkl\"\n",
    "  saved_model.add_file(local_model_file, name=model_name)\n",
    "  print(\"Saving data to WandB...\")\n",
    "  run.log_artifact(saved_model)\n",
    "  run.finish()\n",
    "  print(\"... Run Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Model Evaluation\n",
    "\n",
    "Now that we have a trained model, we want to score it on the test data which was held out in step 2. This code is very similar to the training step, with the execption of slightly different naming. The important difference is that we load the saved model from the artifact. Note that the test predictions will be pretty weak if you're training on the TINY dataset for only one epoch :)\n",
    "\n",
    "See a [live example in W&B](https://wandb.ai/stacey/dsviz-seg/artifacts/test_preds/test_preds_2ghlr3bh/b13a107bb3cb1e2a249c/files/test_results.table.json) of such test predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=WANDB_PROJECT, job_type=\"test\")\n",
    "LOCAL_MODEL_DIR = \"MODEL\"\n",
    "\n",
    "# note: this model is hardcoded for convenience\n",
    "model_artifact = run.use_artifact(\"resnet18:latest\")\n",
    "path = model_artifact.get_path(\"resnet18\").download()\n",
    "\n",
    "# a bit of path gymnastics for fastai\n",
    "model_file = path.split(\"/\")[-1]\n",
    "model_load_path = \"/\".join(path.split(\"/\")[:-1])\n",
    "# load model via fastai\n",
    "unet_model = load_learner(Path(model_load_path), model_file)\n",
    "\n",
    "# create test result artifact\n",
    "test_artifact = run.use_artifact(TEST_DATA_AT + \":latest\")\n",
    "\n",
    "# download test images so they are available locally\n",
    "test_dir = test_artifact.download(LOCAL_MODEL_DIR)\n",
    "test_images_path =  Path(test_dir + \"/images/\")\n",
    "print(\"TEST: \", test_images_path)\n",
    "# create test dataset in fastai\n",
    "test_data = ImageList.from_folder(test_images_path)\n",
    "unet_model.data.add_test(test_data, tfms=None, tfm_y=False)\n",
    "\n",
    "test_batch = unet_model.data.test_ds\n",
    "test_ids = unet_model.data.test_ds.items\n",
    "\n",
    "test_res_at = wandb.Artifact(\"test_pred_\" + wandb.run.id, \"test_preds\")\n",
    "test_table = wandb.Table(columns=[\"id\", \"prediction\"])\n",
    "\n",
    "# store predictions across all resnet model variants as one artifact\n",
    "model_test_at = wandb.Artifact(\"resnet_results\", \"model_test\")\n",
    "model_test_table = wandb.Table(columns=[\"id\", \"prediction\"])\n",
    "\n",
    "for i, img in enumerate(test_batch):\n",
    "   # log raw image as array\n",
    "   orig_image = img[0]\n",
    "   bg_image = image2np(orig_image.data*255).astype(np.uint8)\n",
    "\n",
    "   # our prediction\n",
    "   prediction = unet_model.predict(orig_image)[0]\n",
    "   prediction_mask = image2np(prediction.data).astype(np.uint8)\n",
    "   test_id = str(test_ids[i]).split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "   # create prediction mask and log to table\n",
    "   row = [str(test_id), wb_mask(bg_image, pred_mask=prediction_mask)]\n",
    "   test_table.add_data(*row)\n",
    "   model_test_table.add_data(*row)\n",
    "\n",
    "print(\"Saving data to WandB...\")\n",
    "test_res_at.add(test_table, \"test_results\")\n",
    "run.log_artifact(test_res_at)\n",
    "model_test_at.add(model_test_table, \"model_test_results\")\n",
    "run.log_artifact(model_test_at)\n",
    "print(\"... Run Complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Model Analysis\n",
    "\n",
    "This is where it all comes together. In this step, we join the train and test scoring results with the original dataset and output corresponding artifacts. The new idea introduced here is a `wandb.JoinedTable` which allows you to join two `Table`s for further analysis in the UI.\n",
    "\n",
    "[See a live example of all these tables](https://wandb.ai/stacey/dsviz-seg/artifacts/analysis/resnet_summary/b3054ed6229e43371365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init(project=WANDB_PROJECT, job_type=\"analysis\") as run:\n",
    "    \n",
    "    # Retrieve the original raw dataset\n",
    "    dataset_artifact = run.use_artifact(RAW_DATA_AT + \":latest\")\n",
    "    data_table = dataset_artifact.get(\"raw_examples\")\n",
    "    \n",
    "    # Retrieve the train and test score tables\n",
    "    train_artifact = run.use_artifact(\"resnet_pred:latest\")\n",
    "    train_table = train_artifact.get(\"model_results\")\n",
    "    \n",
    "    test_artifact = run.use_artifact(\"resnet_results:latest\")\n",
    "    test_table = test_artifact.get(\"model_test_results\")\n",
    "    \n",
    "    # Join the tables on ID column and log them as outputs.\n",
    "    train_results = wandb.JoinedTable(train_table, data_table, \"id\")\n",
    "    test_results = wandb.JoinedTable(test_table, data_table, \"id\")\n",
    "    \n",
    "    artifact = wandb.Artifact(\"resnet_summary\", \"analysis\")\n",
    "    artifact.add(train_results, \"train_results\")\n",
    "    artifact.add(test_results, \"test_results\")\n",
    "    run.log_artifact(artifact)\n",
    "    \n",
    "    print(\"Saving data to WandB...\")\n",
    "print(\"... Run Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review the model analysis in the Dashboard\n",
    "Now, click on the above **Project** page (second link). This will look like the following:\n",
    "\n",
    "![](https://raw.githubusercontent.com/staceysv/dsviz-demo/master/notebook_images/project_landing.png)\n",
    "\n",
    "Click on the database icon, as previously, to see the artifacts. This time, you are seeing the artifacts for the entire project, with counts of their versions:\n",
    "\n",
    "![](https://raw.githubusercontent.com/staceysv/dsviz-demo/master/notebook_images/many_artifact_types.png)\n",
    "\n",
    "Go ahead and click the \"`model`\" artifact type, \"Files\", and \"`model.pkl`\". The viewer will provide different renderings based on the file type. For a pickled class, you get the following image. For deep networks saved as `.h5` files, you can see all the layers and their attributes.\n",
    "\n",
    "![](https://raw.githubusercontent.com/wandb/dsviz-demo/master/notebook_images/model_view.png)\n",
    "\n",
    "Next, head back to the artifact page, click Database type, expand `summary_results`, and select your most recent version. Click \"Files\" and select one of the join tables:\n",
    "\n",
    "![](https://raw.githubusercontent.com/staceysv/dsviz-demo/master/notebook_images/pred_detail.png)\n",
    "\n",
    "Zooming out a bit, you can toggle the bounding boxes, masks, group, filter, and sort the data:\n",
    "\n",
    "![](https://raw.githubusercontent.com/staceysv/dsviz-demo/master/notebook_images/zoom_out_table.png)\n",
    "\n",
    "![](https://raw.githubusercontent.com/wandb/dsviz-demo/master/notebook_images/grouped.png)\n",
    "\n",
    "Finally, click graph view, and \"explode\". Now, you can visualize the entire process end-to-end:\n",
    "\n",
    "![](https://raw.githubusercontent.com/wandb/dsviz-demo/master/notebook_images/summary_graph.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
