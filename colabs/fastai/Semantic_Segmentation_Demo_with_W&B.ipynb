{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/fastai/Semantic_Segmentation_Demo_with_W&B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "<!--- @wandbcode{fastai-semantic-seg} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ❌ This Notebooks is using Fastai V1 that is no longer available on colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmentation of Driving Scenes\n",
    "\n",
    "<!--- @wandbcode{fastai-semantic-seg} -->\n",
    "\n",
    "This demo shows how to log image masks for a semantic segmentation model. This model version is simpler and smaller, and the training is shorter for demo purposes. Please see [this W&B report](https://app.wandb.ai/stacey/deep-drive/reports/The-View-from-the-Driver's-Seat--Vmlldzo1MTg5NQ) for the more detailed model and [this GithHub repository by Boris Dayma](https://github.com/borisdayma/semantic-segmentation) for the best reference code.\n",
    "![demoshot](https://i.imgur.com/GY969Y8.png). Note that everyone's runs from this demo are logged to a [shared Weights & Biases project page](https://app.wandb.ai/wandb/segment_demo) by default.\n",
    "\n",
    "\n",
    "\n",
    "## Resources\n",
    "\n",
    "* [Semantic segmentation API →](https://docs.wandb.com/library/log#logging-image-masks-semantic-segmentation)\n",
    "* [Read more about this feature →](https://app.wandb.ai/stacey/deep-drive/reports/Image-Masks-for-Semantic-Segmentation--Vmlldzo4MTUwMw)\n",
    "* [Read more about the problem and modeling approaches →](https://app.wandb.ai/stacey/deep-drive/reports/The-View-from-the-Driver's-Seat--Vmlldzo1MTg5NQ)\n",
    "* [Repo: Semantic Segmentation for Self-Driving Cars→](https://github.com/borisdayma/semantic-segmentation)\n",
    "\n",
    "\n",
    "## About\n",
    "\n",
    "Weights & Biases helps you log experiments, visualize and analyze them faster, collaborate with others, and share your findings. Here we use Google Colab as a convenient hosted environment, but you can run your own training scripts from *any local or cloud setup* with W&B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq wandb fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from fastai.vision import *\n",
    "import wandb\n",
    "from fastai.callbacks.hooks import *\n",
    "from fastai.callback import Callback\n",
    "import json\n",
    "\n",
    "import wandb\n",
    "from fastai.callback.wandb import WandbCallback\n",
    "from functools import partialmethod\n",
    "import PIL\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W&B – Login to your wandb account so you can log all your metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the training data: this is a subset of the Berkeley Deep Drive 100K dataset,\n",
    "# which is available at https://bdd-data.berkeley.edu/ \n",
    "!curl -SL -qq https://storage.googleapis.com/wandb_datasets/BDD100K_seg_demo.zip > BDD100K_seg_demo.zip\n",
    "!unzip -qq BDD100K_seg_demo.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segmentation labels extracted from dataset source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://github.com/ucbdrive/bdd-data/blob/master/bdd_data/label.py\n",
    "segmentation_classes = [\n",
    "    'road', 'sidewalk', 'building', 'wall', 'fence', 'pole', 'traffic light',\n",
    "    'traffic sign', 'vegetation', 'terrain', 'sky', 'person', 'rider', 'car',\n",
    "    'truck', 'bus', 'train', 'motorcycle', 'bicycle', 'void'\n",
    "]\n",
    "\n",
    "def labels():\n",
    "    return {i:label in enumerate(segmentation_classes)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = Path('segment_demo')\n",
    "path_lbl = path_data / 'labels'\n",
    "path_img = path_data / 'images'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set low for faster iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_DATA_FRACTION=0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Associate a label to an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_y_fn = lambda x: path_lbl / x.parts[-2] / f'{x.stem}_train_id.png'\n",
    "\n",
    "# Load data into train & validation sets\n",
    "src = (SegmentationItemList.from_folder(path_img).use_partial_data(USE_DATA_FRACTION)\n",
    "       .split_by_folder(train='train', valid='val')\n",
    "       .label_from_func(get_y_fn, classes=segmentation_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fast.ai callback extension to log image masks\n",
    "class LogImagesCallback(Callback):\n",
    "\n",
    "  def __init__(self, learn):\n",
    "    self.learn = learn\n",
    "    self.num_to_log = 5\n",
    "\n",
    "  def on_epoch_end(self, **kwargs):\n",
    "    input_batch = self.learn.data.valid_ds[:self.num_to_log]\n",
    "    mask_list = []\n",
    "    table_data = []\n",
    "    for i, img_pair in enumerate(input_batch):\n",
    "      original_image = img_pair[0]\n",
    "      # the raw background image as a numpy array\n",
    "      bg_image = image2np(original_image.data*255).astype(np.uint8)\n",
    "      # run the model on that image\n",
    "      prediction = learn.predict(original_image)[0]\n",
    "      prediction_mask = image2np(prediction.data).astype(np.uint8)\n",
    "\n",
    "      # ground truth mask\n",
    "      ground_truth = img_pair[1]\n",
    "      true_mask = image2np(ground_truth.data).astype(np.uint8)\n",
    "      # keep a list of composite images\n",
    "      masked_img = wb_mask(bg_image, prediction_mask, true_mask)\n",
    "      mask_list.append(masked_img)\n",
    "      #add row id and image to table list\n",
    "      table_data.append([i, masked_img])\n",
    "\n",
    "    # log all composite images to W&B alongside a table containing those images\n",
    "    masked_img_table = wandb.Table(data=table_data, columns=[\"id\", \"img\"])\n",
    "    wandb.log({\"predictions\" : mask_list, \"prediction_table\": masked_img_table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy metrics for a few different classes\n",
    "# You could define more for other classes, or consider intersection over union, \n",
    "# which seems to give better performance in the latest version of this model\n",
    "void_code = 19\n",
    "# overall accuracy: across all classes, ignore unlabeled pixels\n",
    "def acc(input, target):\n",
    "    target = target.squeeze(1)\n",
    "    mask = target != void_code\n",
    "    try:\n",
    "      i = (input.argmax(dim=1)[mask] == target[mask]).float()\n",
    "      m_i = i.mean()\n",
    "      return m_i\n",
    "    except:\n",
    "      return torch.tensor([0.0])\n",
    "\n",
    "# only consider classes related to traffic sign and traffic lights\n",
    "def traffic_acc(input, target):\n",
    "    target = target.squeeze(1)\n",
    "    mask_pole = target == 5\n",
    "    mask_light = target == 6\n",
    "    mask_sign = target == 7\n",
    "    mask_traffic = mask_pole | mask_light | mask_sign\n",
    "    try:\n",
    "      i = (input.argmax(dim=1)[mask_traffic] == target[mask_traffic]).float()\n",
    "      m_i = i.mean()\n",
    "      return m_i\n",
    "    except:\n",
    "      return torch.tensor([0.0])\n",
    "\n",
    "# only consider cars\n",
    "def car_acc(input, target):\n",
    "    target = target.squeeze(1)\n",
    "    mask = target == 13\n",
    "    try:\n",
    "        intersection = input.argmax(dim=1)[mask] == target[mask]\n",
    "        mean_intersection = intersection.float().mean()\n",
    "        return mean_intersection\n",
    "    except:\n",
    "        return torch.tensor([0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell configures your experiment: you can modify the hyperparameters here,\n",
    "# and make sure to rerun this cell for every new training run you launch\n",
    "# Initialize W&B project \n",
    "wandb.init(project=\"semseg_demo\")\n",
    "\n",
    "# Define hyperparameters\n",
    "config = wandb.config           # for shortening\n",
    "config.framework = \"fast.ai\"    # AI framework used (for when we create other versions)\n",
    "config.img_size = (360, 640)    # dimensions of resized image - can be 1 dim or tuple\n",
    "\n",
    "config.batch_size = 2           # Batch size during training -- setting this super low to avoid CUDA OOM error\n",
    "config.epochs = 4               # Number of epochs for training -- set this to 10+ for better results\n",
    "\n",
    "config.encoder = \"resnet18\"     # could be resnet18 or alexnet (but watch out for CUDA memory)\n",
    "encoder = models.resnet18\n",
    "\n",
    "config.pretrained = True        # whether we use a frozen pre-trained encoder\n",
    "config.weight_decay = 0.097     # weight decay applied on layers\n",
    "config.bn_weight_decay = True   # whether weight decay is applied on batch norm layers\n",
    "config.one_cycle = True         # use the \"1cycle\" policy -> https://arxiv.org/abs/1803.09820\n",
    "config.learning_rate = 0.001    # learning rate\n",
    "\n",
    "# Resize, augment, load in batch & normalize (so we can use pre-trained networks)\n",
    "data = (src.transform(get_transforms(), size=config.img_size, tfm_y=True)\n",
    "        .databunch(bs=config.batch_size)\n",
    "        .normalize(imagenet_stats))\n",
    "\n",
    "# Track how much data we actually use in this run\n",
    "config.num_train = len(data.train_ds) \n",
    "config.num_valid = len(data.valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%wandb\n",
    "# This cell launches the W&B experiment run and shows you the training progress\n",
    "# in real-time\n",
    "\n",
    "# Create model\n",
    "learn = unet_learner(\n",
    "    data,\n",
    "    arch=encoder,\n",
    "    pretrained=config.pretrained,\n",
    "    metrics=[acc, car_acc, traffic_acc],\n",
    "    wd=config.weight_decay,\n",
    "    bn_wd=config.bn_weight_decay,\n",
    "    callback_fns=partial(WandbCallback, monitor='acc'))\n",
    "\n",
    "# Train\n",
    "learn.fit_one_cycle(\n",
    "    config.epochs // 2,\n",
    "    max_lr=slice(config.learning_rate),\n",
    "    callbacks=[LogImagesCallback(learn)])\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(\n",
    "    config.epochs // 2,\n",
    "    max_lr=slice(config.learning_rate / 100,\n",
    "                 config.learning_rate / 10),\n",
    "    callbacks=[LogImagesCallback(learn), WandbCallback(learn)])\n",
    "wandb.run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to See Live Results in Shared Project\n",
    "1. Check out the [project page](https://app.wandb.ai/wandb/neurips-demo/) to see your results in the shared project. \n",
    "1. Press 'option+space' to expand the runs table, comparing all the results from everyone who has tried this script. \n",
    "1. Click on the name of a run to dive deeper into that specific run on its own run page.\n",
    "\n",
    "![project page](https://i.imgur.com/I1PM9YJ.png)\n",
    "\n",
    "\n",
    "## Visualize Relationships\n",
    "\n",
    "Use a parallel coordinates chart to see the relationship between hyperparameters and output metrics. Here, we can see how the learning rate and other metrics saved in \"config\" affect loss and accuracy.\n",
    "\n",
    "![parallel coordinates plot](https://i.imgur.com/cg1uodx.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More about Weights & Biases\n",
    "We're always free for academics and open source projects. Email carey@wandb.com with any questions or feature suggestions. Here are some more resources:\n",
    "\n",
    "1. [Documentation](http://docs.wandb.com) - Python docs\n",
    "2. [Gallery](https://app.wandb.ai/gallery) - example reports in W&B\n",
    "3. [Articles](https://www.wandb.com/articles) - blog posts and tutorials\n",
    "4. [Community](bit.ly/wandb-forum) - join our Slack community forum"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
