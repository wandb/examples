{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simple LightGBM Integration",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/boosting/Simple_LightGBM_Integration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsYA23-ySjCa"
      },
      "source": [
        "<img src=\"https://i.imgur.com/gb6B4ig.png\" width=\"400\" alt=\"Weights & Biases\" />\n",
        "\n",
        "<div><img /></div>\n",
        "\n",
        "<img src=\"https://i.imgur.com/uEtWSEb.png\" width=\"650\" alt=\"Weights & Biases\" />\n",
        "\n",
        "<div><img /></div>\n",
        "\n",
        "# ðŸ‹ï¸â€â™€ï¸ W&B + ðŸ’¡ LightGBM\n",
        "Use Weights & Biases for machine learning experiment tracking, dataset versioning, and project collaboration.\n",
        "\n",
        "Gradient boosting decision trees are the state of the art when it comes to building predictive models for structured data.\n",
        "\n",
        "[LigthGBM](https://github.com/microsoft/LightGBM), a gradient boosting framework by Microsoft, has dethroned xgboost and become the go to GBDT algorithm (along with catboost). It outperforms xgboost in training speeds, memory usage and the size of datasets it can handle. LightGBM does so by using histogram-based algorithms to bucket continuous features into discrete bins during training.\n",
        "\n",
        "\n",
        "## What this notebook covers\n",
        "* Easy integration of Weights and Biases with LightGBM. \n",
        "* `wandb_callback()` callback\n",
        "\n",
        "We want to make it incredible easy for people to look under the hood of their models, so we built a callback that helps you visualize your LightGBMâ€™s performance in just one line of code.\n",
        "\n",
        "**Note**: Sections starting with _Step_ is all you need to integrate W&B."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoSFA4wCy79a"
      },
      "source": [
        "# Install, Import, and Log in"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqY2CQzMQdss"
      },
      "source": [
        "## The Usual Suspects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCS7w4-nKXW5"
      },
      "source": [
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmlhYoBlOxYd"
      },
      "source": [
        "## Step 0: Install W&B"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAZBTFNQq0ys"
      },
      "source": [
        "%%capture\n",
        "!pip install wandb"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tu-KHLAeO3wi"
      },
      "source": [
        "## Step 1: Import W&B and Login"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmVkQbw_q_07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a1be7c3b-fbd1-4df7-fc4f-aa67289c1bed"
      },
      "source": [
        "import wandb\n",
        "from wandb.lightgbm import wandb_callback\n",
        "\n",
        "wandb.login()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3Ln8W0Aru8X"
      },
      "source": [
        "# Download and Prepare Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wF-3RxY2EdOU"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/microsoft/LightGBM/master/examples/regression/regression.train -qq\n",
        "!wget https://raw.githubusercontent.com/microsoft/LightGBM/master/examples/regression/regression.test -qq"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJG1iqoOzUDQ"
      },
      "source": [
        "# load or create your dataset\n",
        "df_train = pd.read_csv('regression.train', header=None, sep='\\t')\n",
        "df_test = pd.read_csv('regression.test', header=None, sep='\\t')\n",
        "\n",
        "y_train = df_train[0]\n",
        "y_test = df_test[0]\n",
        "X_train = df_train.drop(0, axis=1)\n",
        "X_test = df_test.drop(0, axis=1)\n",
        "\n",
        "# create dataset for lightgbm\n",
        "lgb_train = lgb.Dataset(X_train, y_train)\n",
        "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co0IcwRl4goi"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emDWwQdEQQ3Y"
      },
      "source": [
        "### Step 2: Initialize your wandb run. \n",
        "\n",
        "Using `wandb.init()` initialize your W&B run. You can also pass a dictionary of configs. [Check out the official documentation here $\\rightarrow$](https://docs.wandb.com/library/init)\n",
        "\n",
        "You can't deny the importance of configs in your ML/DL workflow. W&B makes sure that you have access to the right config to reproduce your model. \n",
        "\n",
        "[Learn more about configs in this colab notebook $\\rightarrow$](http://wandb.me/config-colab)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rb6UKnUtBMBR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        },
        "outputId": "7d8ec0bb-c246-4d84-b8d8-031732bfefc9"
      },
      "source": [
        "# specify your configurations as a dict\n",
        "params = {\n",
        "    'boosting_type': 'gbdt',\n",
        "    'objective': 'regression',\n",
        "    'metric': ['rmse', 'l2', 'l1', 'huber'],\n",
        "    'num_leaves': 31,\n",
        "    'learning_rate': 0.05,\n",
        "    'feature_fraction': 0.9,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 5,\n",
        "    'verbosity': 0\n",
        "}\n",
        "\n",
        "wandb.init(project='my-lightgbm-integration', config=params);"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcharlesfrye\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.11<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">silver-eon-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/charlesfrye/my-lightgbm-integration\" target=\"_blank\">https://wandb.ai/charlesfrye/my-lightgbm-integration</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/charlesfrye/my-lightgbm-integration/runs/11f74lf2\" target=\"_blank\">https://wandb.ai/charlesfrye/my-lightgbm-integration/runs/11f74lf2</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20201201_214353-11f74lf2</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7fea5dd7a358>"
            ],
            "text/html": [
              "<h1>Run(11f74lf2)</h1><p></p><iframe src=\"https://wandb.ai/charlesfrye/my-lightgbm-integration/runs/11f74lf2\" style=\"border:none;width:100%;height:400px\"></iframe>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROzBbuFRfevY"
      },
      "source": [
        "> Once you have trained your model come back and click on the **Project page**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yECjtzMwdHd"
      },
      "source": [
        "### Step 3: Train with `wandb_callback`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmBJfKH0Fzwn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8785c487-b770-40cd-f7a0-aa1cd2eff4a0"
      },
      "source": [
        "# train \n",
        "# add lightgbm callback\n",
        "gbm = lgb.train(params,\n",
        "                lgb_train,\n",
        "                num_boost_round=30,\n",
        "                valid_sets=lgb_eval,\n",
        "                valid_names=('validation'),\n",
        "                callbacks=[wandb_callback()],\n",
        "                early_stopping_rounds=5)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1]\tvalidation's huber: 0.121741\tvalidation's l1: 0.492417\tvalidation's l2: 0.243481\tvalidation's rmse: 0.493438\n",
            "Training until validation scores don't improve for 5 rounds.\n",
            "[2]\tvalidation's huber: 0.120023\tvalidation's l1: 0.48874\tvalidation's l2: 0.240045\tvalidation's rmse: 0.489944\n",
            "[3]\tvalidation's huber: 0.118318\tvalidation's l1: 0.485042\tvalidation's l2: 0.236636\tvalidation's rmse: 0.486452\n",
            "[4]\tvalidation's huber: 0.116479\tvalidation's l1: 0.480872\tvalidation's l2: 0.232959\tvalidation's rmse: 0.482658\n",
            "[5]\tvalidation's huber: 0.114842\tvalidation's l1: 0.476928\tvalidation's l2: 0.229684\tvalidation's rmse: 0.479254\n",
            "[6]\tvalidation's huber: 0.113471\tvalidation's l1: 0.473545\tvalidation's l2: 0.226942\tvalidation's rmse: 0.476384\n",
            "[7]\tvalidation's huber: 0.111986\tvalidation's l1: 0.469984\tvalidation's l2: 0.223972\tvalidation's rmse: 0.473256\n",
            "[8]\tvalidation's huber: 0.110464\tvalidation's l1: 0.466083\tvalidation's l2: 0.220928\tvalidation's rmse: 0.47003\n",
            "[9]\tvalidation's huber: 0.108975\tvalidation's l1: 0.462164\tvalidation's l2: 0.217949\tvalidation's rmse: 0.46685\n",
            "[10]\tvalidation's huber: 0.10756\tvalidation's l1: 0.458352\tvalidation's l2: 0.21512\tvalidation's rmse: 0.46381\n",
            "[11]\tvalidation's huber: 0.106302\tvalidation's l1: 0.454792\tvalidation's l2: 0.212605\tvalidation's rmse: 0.461091\n",
            "[12]\tvalidation's huber: 0.105259\tvalidation's l1: 0.451692\tvalidation's l2: 0.210519\tvalidation's rmse: 0.458823\n",
            "[13]\tvalidation's huber: 0.104364\tvalidation's l1: 0.448774\tvalidation's l2: 0.208728\tvalidation's rmse: 0.456867\n",
            "[14]\tvalidation's huber: 0.103792\tvalidation's l1: 0.446628\tvalidation's l2: 0.207585\tvalidation's rmse: 0.455615\n",
            "[15]\tvalidation's huber: 0.103127\tvalidation's l1: 0.44419\tvalidation's l2: 0.206254\tvalidation's rmse: 0.454152\n",
            "[16]\tvalidation's huber: 0.102165\tvalidation's l1: 0.441229\tvalidation's l2: 0.20433\tvalidation's rmse: 0.452029\n",
            "[17]\tvalidation's huber: 0.10147\tvalidation's l1: 0.438937\tvalidation's l2: 0.20294\tvalidation's rmse: 0.450489\n",
            "[18]\tvalidation's huber: 0.100693\tvalidation's l1: 0.436752\tvalidation's l2: 0.201385\tvalidation's rmse: 0.44876\n",
            "[19]\tvalidation's huber: 0.10004\tvalidation's l1: 0.434243\tvalidation's l2: 0.20008\tvalidation's rmse: 0.447303\n",
            "[20]\tvalidation's huber: 0.0994941\tvalidation's l1: 0.43204\tvalidation's l2: 0.198988\tvalidation's rmse: 0.446081\n",
            "[21]\tvalidation's huber: 0.09876\tvalidation's l1: 0.429626\tvalidation's l2: 0.19752\tvalidation's rmse: 0.444432\n",
            "[22]\tvalidation's huber: 0.0983575\tvalidation's l1: 0.427844\tvalidation's l2: 0.196715\tvalidation's rmse: 0.443526\n",
            "[23]\tvalidation's huber: 0.0979678\tvalidation's l1: 0.426086\tvalidation's l2: 0.195936\tvalidation's rmse: 0.442646\n",
            "[24]\tvalidation's huber: 0.0972374\tvalidation's l1: 0.423602\tvalidation's l2: 0.194475\tvalidation's rmse: 0.440993\n",
            "[25]\tvalidation's huber: 0.0967275\tvalidation's l1: 0.421795\tvalidation's l2: 0.193455\tvalidation's rmse: 0.439835\n",
            "[26]\tvalidation's huber: 0.0964256\tvalidation's l1: 0.420372\tvalidation's l2: 0.192851\tvalidation's rmse: 0.439148\n",
            "[27]\tvalidation's huber: 0.0960758\tvalidation's l1: 0.418856\tvalidation's l2: 0.192152\tvalidation's rmse: 0.438351\n",
            "[28]\tvalidation's huber: 0.0957369\tvalidation's l1: 0.417253\tvalidation's l2: 0.191474\tvalidation's rmse: 0.437577\n",
            "[29]\tvalidation's huber: 0.0953817\tvalidation's l1: 0.415805\tvalidation's l2: 0.190763\tvalidation's rmse: 0.436765\n",
            "[30]\tvalidation's huber: 0.0951645\tvalidation's l1: 0.414553\tvalidation's l2: 0.190329\tvalidation's rmse: 0.436267\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[30]\tvalidation's huber: 0.0951645\tvalidation's l1: 0.414553\tvalidation's l2: 0.190329\tvalidation's rmse: 0.436267\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQwgtiOC0zo0"
      },
      "source": [
        "# Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vwz_5bmxvLeq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f7bf5e0-8d93-400d-928b-63fbdf136ed2"
      },
      "source": [
        "# predict\n",
        "y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
        "# eval\n",
        "print('The rmse of prediction is:', mean_squared_error(y_test, y_pred) ** 0.5)\n",
        "wandb.log({'rmse_prediction': mean_squared_error(y_test, y_pred) ** 0.5})"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The rmse of prediction is: 0.43626711632808296\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALGl_CS0GCLJ"
      },
      "source": [
        "# Visualize Results\n",
        "\n",
        "Click on the **project page** link above to see your results automatically visualized.\n",
        "\n",
        "<img src=\"https://imgur.com/S6lwSig.png\" alt=\"Viz\" />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvVBjQTbpFhV"
      },
      "source": [
        "# Sweep 101\n",
        "\n",
        "Use Weights & Biases Sweeps to automate hyperparameter optimization and explore the space of possible models.\n",
        "\n",
        "## [Check out Hyperparameter Optimization with XGBoost  using W&B Sweep $\\rightarrow$](http://wandb.me/xgb-colab)\n",
        "\n",
        "Running a hyperparameter sweep with Weights & Biases is very easy. There are just 3 simple steps:\n",
        "\n",
        "1. **Define the sweep:** We do this by creating a dictionary or a [YAML file](https://docs.wandb.com/library/sweeps/configuration) that specifies the parameters to search through, the search strategy, the optimization metric et all.\n",
        "\n",
        "2. **Initialize the sweep:** \n",
        "`sweep_id = wandb.sweep(sweep_config)`\n",
        "\n",
        "3. **Run the sweep agent:** \n",
        "`wandb.agent(sweep_id, function=train)`\n",
        "\n",
        "And voila! That's all there is to running a hyperparameter sweep!\n",
        "\n",
        "<img src=\"https://imgur.com/SVtMfa2.png\" alt=\"Sweep Result\" />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiunZZ3AGUY3"
      },
      "source": [
        "# Example Gallery\n",
        "\n",
        "See examples of projects tracked and visualized with W&B in our [Gallery â†’](https://app.wandb.ai/gallery)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWERarlyGL1V"
      },
      "source": [
        "# Basic Setup\n",
        "1. **Projects**: Log multiple runs to a project to compare them. `wandb.init(project=\"project-name\")`\n",
        "2. **Groups**: For multiple processes or cross validation folds, log each process as a runs and group them together. `wandb.init(group='experiment-1')`\n",
        "3. **Tags**: Add tags to track your current baseline or production model.\n",
        "4. **Notes**: Type notes in the table to track the changes between runs.\n",
        "5. **Reports**: Take quick notes on progress to share with colleagues and make dashboards and snapshots of your ML projects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY39JyftGYqA"
      },
      "source": [
        "# Advanced Setup\n",
        "1. [Environment variables](https://docs.wandb.com/library/environment-variables): Set API keys in environment variables so you can run training on a managed cluster.\n",
        "2. [Offline mode](https://docs.wandb.com/library/technical-faq#can-i-run-wandb-offline): Use `dryrun` mode to train offline and sync results later.\n",
        "3. [On-prem](https://docs.wandb.com/self-hosted): Install W&B in a private cloud or air-gapped servers in your own infrastructure. We have local installations for everyone from academics to enterprise teams.\n",
        "4. [Sweeps](https://docs.wandb.com/sweeps): Set up hyperparameter search quickly with our lightweight tool for tuning."
      ]
    }
  ]
}