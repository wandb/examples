{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/simpletransformers/SimpleTransformersQA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-GaRWp1EPDX"
      },
      "source": [
        "<img src=\"http://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
        "\n",
        "<!--- @wandbcode{simpletransformers-QA} -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "987w2TqZ0jKO"
      },
      "source": [
        "\n",
        "# W&B ðŸ’˜ SimpleTransformers\n",
        "Use Weights & Biases for machine learning experiment tracking, dataset versioning, and project collaboration.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zlShldJaBlN"
      },
      "source": [
        "<img src=\"http://wandb.me/mini-diagram\" width=\"650\" alt=\"Weights & Biases\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTfx_Ar80kHV"
      },
      "source": [
        "## What this notebook covers\n",
        "\n",
        "In this notebook we show you how to integrate\n",
        "[Weights & Biases](https://wandb.ai/site)\n",
        "with your\n",
        "[SimpleTransformers](https://github.com/ThilinaRajapakse/simpletransformers) \n",
        "code to add experiment tracking to your pipeline. This includes:\n",
        "\n",
        "1. dataset and model versioning with W&B,\n",
        "[Artifacts](https://docs.wandb.ai/guides/artifacts)\n",
        "2. storing configuration, hyperparameters, system metrics, and model metrics in an [interactive dashboard](https://docs.wandb.ai/guides/track/app), and\n",
        "3. examining evaluation outputs of your model using\n",
        "[W&B Tables](https://docs.wandb.ai/guides/data-vis).\n",
        "\n",
        "We'll add these features to a typical NLP pipeline:\n",
        "training a question-answering model with a DistilBERT backbone on (a very small subset of) the Stanford QUestion Answering Dataset ([SQuAD](https://rajpurkar.github.io/SQuAD-explorer/)).\n",
        "The extra code required is indicated in the notebook with headers that start with \"Step\".\n",
        "\n",
        "We'll end up with an interactive dashboard ([link](https://wandb.ai/wandb/SimpleTransformers-QA?workspace=user-prashanthkurella))\n",
        "for our QA experiments that looks like this:\n",
        "\n",
        "<img src=\"https://i.imgur.com/60KMnvE.png\" width=\"650\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJMBKrnP082j"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBmYoOqUIP3p"
      },
      "source": [
        "## Step 0 : Install Weights & Biases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZtbjimUR7Yd"
      },
      "source": [
        "Let's get started by installing W&B."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlwMljddINum"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTbgc1O5IgHT"
      },
      "source": [
        "## Step 1: Import `wandb`, log in, and set the project name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWo7moc71DZV"
      },
      "source": [
        "Now that we have installed W&B, let's import it and log in.\n",
        "If you don't have a W&B account, you'll be prompted to create one.\n",
        "W&B is free for open projects, just like GitHub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kd18rHF-IcMq"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()\n",
        "wandb_project = \"SimpleTransformers-QA\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x2HIXUUIwsw"
      },
      "source": [
        "## Install SimpleTransformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqmvfgU4S9MP"
      },
      "source": [
        "SimpleTransformers is a library for quickly spinning up Transformer models\n",
        "for natural language processing tasks.\n",
        "It has an easy to use interface and requires only a minimal amount of code.\n",
        "\n",
        "You can look at [the docs](https://simpletransformers.ai/) for more about what SimpleTransformers can do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-5KBc4xu1xv"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install simpletransformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92C30e6mJHt-"
      },
      "source": [
        "# Download the SQuAD Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AwEyxp3TK78"
      },
      "source": [
        "We'll use a subset of the SQuAD Dataset to train a question-answering model.\n",
        "\n",
        "The SQuAD Dataset consists of a context and set of questions relevant to the context.\n",
        "Our model's task is to understand the context and use it to answer the questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnkth7lku-Kp"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "data_dir = \"data\"\n",
        "raw_data_dir = data_dir + \"/\" + \"raw\"\n",
        "\n",
        "!mkdir -p {raw_data_dir}\n",
        "!curl https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json --output {raw_data_dir}/train.json\n",
        "!curl https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json --output {raw_data_dir}/eval.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHQ0ZFjFJ4X0"
      },
      "source": [
        "## Step 2: Log the raw dataset\n",
        "\n",
        "In order to make our work more reproducible and portable,\n",
        "we'll store, version, and distribute our dataset with\n",
        "[W&B Artifacts](https://docs.wandb.ai/guides/artifacts).\n",
        "\n",
        "To log an Artifact, we need to start a `wandb.Run`\n",
        "with [`wandb.init`](https://docs.wandb.ai/guides/track/launch).\n",
        "We'll give it the `upload-raw-dataset` job type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWd85Ki92B2C"
      },
      "outputs": [],
      "source": [
        "# initialize a run to log the datasets\n",
        "run = wandb.init(\n",
        "    project=wandb_project,\n",
        "    job_type=\"upload-raw-dataset\"\n",
        ")\n",
        "\n",
        "# log the raw data\n",
        "raw_data_artifact = wandb.Artifact(\"raw-data\", \"dataset\")\n",
        "raw_data_artifact.add_dir(raw_data_dir)\n",
        "run.log_artifact(raw_data_artifact)\n",
        "\n",
        "# finish the run\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaXHIP3JLI6q"
      },
      "source": [
        "# Perform the train-test split on the raw dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVKHza9sVjfg"
      },
      "source": [
        "Datasets are typically transformed in an ML pipeline.\n",
        "\n",
        "For example, when training models, we usually want to split out some data to hold for validation.\n",
        "\n",
        "With W&B Artifacts we can track those splits\n",
        "and then reuse them when debugging models, or reproducing and extending results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOgJKCCX1LvD"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "# make the data dirs for splits\n",
        "split_data_dir = data_dir + \"/\" + \"split\"\n",
        "!mkdir -p {split_data_dir}\n",
        "\n",
        "# shuffle and subset train data\n",
        "with open(f\"{raw_data_dir}/train.json\", \"r\") as f:\n",
        "    train_data = json.load(f)\n",
        "train_data = [item for topic in train_data[\"data\"] for item in topic[\"paragraphs\"] ]\n",
        "random.shuffle(train_data)\n",
        "train_data = train_data[:int(len(train_data) * 0.01)]\n",
        "\n",
        "# shuffle and subset eval data\n",
        "with open(f\"{raw_data_dir}/eval.json\", \"r\") as f:\n",
        "    eval_data = json.load(f)\n",
        "eval_data = [item for topic in eval_data[\"data\"] for item in topic[\"paragraphs\"] ]\n",
        "random.shuffle(eval_data)\n",
        "eval_data = eval_data[:int(len(eval_data) * 0.01)]\n",
        "\n",
        "# write the subsets to disk\n",
        "with open(f\"{split_data_dir}/train.json\", \"w\") as f:\n",
        "    json.dump(train_data, f)\n",
        "with open(f\"{split_data_dir}/eval.json\", \"w\") as f:\n",
        "    json.dump(eval_data, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31ZBDIAYK1Fz"
      },
      "source": [
        "## Step 3: Log the train-test split we'll be using "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XenVk7KYSxsb"
      },
      "outputs": [],
      "source": [
        "# initialize a run to save the datasets\n",
        "run = wandb.init(\n",
        "    project=wandb_project,\n",
        "    job_type=\"split-dataset\"\n",
        ")\n",
        "\n",
        "# use the raw data artifact\n",
        "run.use_artifact(\"raw-data:latest\")\n",
        "\n",
        "# log the training data as an artifact\n",
        "train_artifact = wandb.Artifact(\"train-data\", \"dataset\")\n",
        "train_artifact.add_file(f\"{split_data_dir}/train.json\")\n",
        "run.log_artifact(train_artifact)\n",
        "\n",
        "# log the evaluation data as an artifact\n",
        "eval_artifact = wandb.Artifact(\"eval-data\", \"dataset\")\n",
        "eval_artifact.add_file(f\"{split_data_dir}/eval.json\")\n",
        "run.log_artifact(eval_artifact)\n",
        "\n",
        "# finish logging the data logging run\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fax_QTTV1F4"
      },
      "source": [
        "By using Artifacts, we can track which runs produced and used particular resources,\n",
        "like datasets, models, and analyses.\n",
        "The relationships are tracked as a\n",
        "[graph](https://docs.wandb.ai/ref/app/pages/project-page#graph-view-panel).\n",
        "\n",
        "You can view and interact with a complete artifact graph\n",
        "for this project in the browser\n",
        "[here](https://wandb.ai/wandb/SimpleTransformers-QA/artifacts/run_table/run-3n08kirq-evalresults/ce84b13b2961e7b30e13/graph).\n",
        "\n",
        "You'll see square nodes, representing runs,\n",
        "and circular nodes, representing generated artifacts.\n",
        "Arrows connect runs to the artifacts they generated\n",
        "and artifacts to the runs that use them.\n",
        "\n",
        "In the screenshot below,\n",
        "see if you can find the runs used to upload and split the dataset\n",
        "and the dataset artifacts that those runs generated.\n",
        "\n",
        "<img src=\"https://imgur.com/SVZpMWi.png\" width=\"600\" alt=\"Weights & Biases\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2cP0vRULOix"
      },
      "source": [
        "# Configure model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyzdmoLxWgFS"
      },
      "source": [
        "SimpleTransformers makes it easy to run and configure your transformer model training:\n",
        "`train_args` are [\"all you need\"](https://arxiv.org/abs/1706.03762)\n",
        "to train your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKHPtwU_csxc"
      },
      "outputs": [],
      "source": [
        "train_args = {\n",
        "    \"learning_rate\":                   3e-5, # learning rate of our model\n",
        "    \"num_train_epochs\":                   2, # number of epochs \n",
        "    \"max_seq_length\":                   384, # maximum sequence length in tokens\n",
        "    \"doc_stride\":                       128, # stride when processing sentences\n",
        "    \"overwrite_output_dir\":            True, # overwrite the output directory\n",
        "    \"reprocess_input_data\":           False, # reprocess the input data\n",
        "    \"train_batch_size\":                  16, # training batch size\n",
        "    \"gradient_accumulation_steps\":        1, # steps before applying gradients\n",
        "    \"evaluate_during_training\":        True, # run evaluation during training\n",
        "    \"evaluate_during_training_steps\":    40, # steps in training before eval\n",
        "    \"save_eval_checkpoints\":          False, # save evaluation checkpoints\n",
        "    \"eval_batch_size\":                   16, # evaluation batch size\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SyO2VzkLcr7"
      },
      "source": [
        "## Step 4: Include `wandb_project` in `train_args` to use W&B for logging our training progress"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3aeYFONXMdG"
      },
      "source": [
        "SimpleTransformers comes with W&B logging built in -- no extra code required.\n",
        "To enable it you just need to pass the `wandb_project` argument.\n",
        "\n",
        "You can also customize what's passed to the `wandb.init` function\n",
        "used to launch your training run\n",
        "with the `wandb_kwargs` argument.\n",
        "Refer to the docs\n",
        "[here](https://docs.wandb.ai/guides/track/launch)\n",
        "for more info."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBnixIrDLZGo"
      },
      "outputs": [],
      "source": [
        "train_args.update(\n",
        "    {\n",
        "        \"logging_steps\":                      1, # number of steps before logging\n",
        "        \"wandb_project\":          wandb_project, # wandb project name\n",
        "        \"wandb_kwargs\": {\"job_type\": \"training\"} # additional args for wandb init\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ecDJ7RSMDUH"
      },
      "source": [
        "# Initialize the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cErOT8DiXdE-"
      },
      "source": [
        "Another killer feature of SimpleTransformers is that it comes with a bunch of\n",
        "implementations of widely-used transformer architectures, like BERT, ALBERT, and others.\n",
        "\n",
        "It also includes utilities for downloading their pretrained versions and adapting\n",
        "them to specific tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tc8vG5-GzMpR"
      },
      "outputs": [],
      "source": [
        "%%capture --no-display\n",
        "from simpletransformers.question_answering import QuestionAnsweringModel\n",
        "\n",
        "# initialize the model with a distilbert backbone\n",
        "model = QuestionAnsweringModel(\"distilbert\", \"distilbert-base-cased\", args=train_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptI1cBaKMP_h"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAOekPDNYpok"
      },
      "source": [
        "Every time you call the `train_model` function,\n",
        "you launch a new experiment.\n",
        "\n",
        "W&B prints out the links to\n",
        "[project-level](https://docs.wandb.ai/ref/app/pages/project-page)\n",
        "and\n",
        "[run-level](https://docs.wandb.ai/ref/app/pages/run-page)\n",
        "dashboards.\n",
        "\n",
        "Click on those links to view the training progress\n",
        "and compare to other experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjbAvVEa4BtT"
      },
      "outputs": [],
      "source": [
        "%%capture --no-display\n",
        "model.train_model(train_data, eval_data=eval_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8Bib1p4Yj6A"
      },
      "source": [
        "You can reorganize your workspace into an interactive dashboard\n",
        "to share with team members or put in your portfolio.\n",
        "\n",
        "Below is a screenshot of a dashboard made for this project.\n",
        "\n",
        "You can view and interact with it in your browser\n",
        "[here](https://wandb.ai/wandb/SimpleTransformers-QA?workspace=user-prashanthkurella).\n",
        "\n",
        "<img src=\"https://imgur.com/mpVch9C.png\" width=\"600\" alt=\"Weights & Biases\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1S3sDKLYRzR"
      },
      "source": [
        "# Custom Logging for SimpleTransformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "br6UlufIbCLn"
      },
      "source": [
        "SimpleTransformers automatically logs important metrics to W&B.\n",
        "\n",
        "You can also customize what you log using two methods:\n",
        "\n",
        "1. [Resuming](https://docs.wandb.ai/guides/track/advanced/resuming) the run,\n",
        "\"restarting\" the experiment so that you can log additional stuff, including more training. \n",
        "2. Using the [`wandb.api`](https://docs.wandb.ai/guides/track/public-api-guide) to update existing runs with additional metadata.\n",
        "\n",
        "We show both below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-s2ViJuMzdc"
      },
      "source": [
        "## Use resuming to add model checkpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u2714RV3xCW"
      },
      "source": [
        "Runs that have finished can be resumed\n",
        "so that additional information can be added to an experiment.\n",
        "For example, you might be using\n",
        "[pre-emptible compute](https://www.parkmycloud.com/blog/google-preemptible-vms/)\n",
        "where training runs can be stopped prematurely.\n",
        "\n",
        "Here we use it to log the model checkpoints to the training run,\n",
        "since it was responsible for creating them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLHIG9OdLPjT"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "\n",
        "with wandb.init(id=model.wandb_run_id, resume=\"allow\", project=wandb_project) as training_run:\n",
        "    for dir in sorted(os.listdir(\"outputs\")):\n",
        "        if \"checkpoint\" in dir:\n",
        "            artifact = wandb.Artifact(\"model-checkpoints\", type=\"checkpoints\")\n",
        "            artifact.add_dir(\"outputs\" + \"/\" + dir)\n",
        "            training_run.log_artifact(artifact)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fr7EDGBNCaR"
      },
      "source": [
        "## Use resuming to add evaluation results as a `Table`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3DMF8hFcCOb"
      },
      "source": [
        "To evaluate models and their performance,\n",
        "it's important to be able to visualize and analyze model predictions.\n",
        "W&B supports this workflow with\n",
        "[Tables](https://docs.wandb.ai/guides/data-vis).\n",
        "\n",
        "Here, we'll grab our model's predictions on the evaluation data,\n",
        "convert them into a pandas `DataFrame`,\n",
        "and then log them to W&B as a `Table` attached to the resumed run.\n",
        "\n",
        "For more on using Tables for NLP, check out our\n",
        "[video guide](https://www.youtube.com/watch?v=756JcKiDvqo)\n",
        "on applying Tables to the\n",
        "[GoEmotions dataset](https://arxiv.org/abs/2005.00547)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jb_Ac-qaCVY"
      },
      "outputs": [],
      "source": [
        "_, outputs = model.eval_model(eval_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_I7iM_2WisnF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# create an empty dataframe\n",
        "eval_data_df = pd.DataFrame(\n",
        "    columns=[\n",
        "        \"id\",\n",
        "        \"question\",\n",
        "        \"context\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "# load the eval data into the dataframe\n",
        "for context in eval_data:\n",
        "    for qas in context[\"qas\"]:\n",
        "            eval_data_df = eval_data_df.append([\n",
        "                {\n",
        "                    \"id\": qas[\"id\"],\n",
        "                    \"context\": context[\"context\"],\n",
        "                    \"question\": qas[\"question\"]\n",
        "                }\n",
        "            ])\n",
        "\n",
        "# reset index for clear indexing\n",
        "eval_data_df = eval_data_df.reset_index(drop=True)\n",
        "\n",
        "# create an empty results data frame\n",
        "results = pd.DataFrame(\n",
        "    columns=[\n",
        "        \"id\",\n",
        "        \"predicted_answer\",\n",
        "        \"actual_answer\",\n",
        "        \"category\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "# load all the correctly predicted answers\n",
        "for entry in outputs[\"correct_text\"]:\n",
        "    results = results.append([\n",
        "        {\n",
        "            \"id\": entry, \n",
        "            \"predicted_answer\": outputs[\"correct_text\"][entry],\n",
        "            \"actual_answer\": outputs[\"correct_text\"][entry],\n",
        "            \"category\": \"correct\"\n",
        "        }\n",
        "    ])\n",
        "\n",
        "# load all the similar answers\n",
        "for entry in outputs[\"similar_text\"]:\n",
        "    results = results.append([\n",
        "        {\n",
        "            \"id\": entry, \n",
        "            \"predicted_answer\": outputs[\"similar_text\"][entry][\"predicted\"],\n",
        "            \"actual_answer\": outputs[\"similar_text\"][entry][\"truth\"],\n",
        "            \"category\": \"similar\"\n",
        "        }\n",
        "    ])\n",
        "\n",
        "# load all the incorrect answers\n",
        "for entry in outputs[\"incorrect_text\"]:\n",
        "    results = results.append([\n",
        "        {\n",
        "            \"id\": entry, \n",
        "            \"predicted_answer\": outputs[\"incorrect_text\"][entry][\"predicted\"],\n",
        "            \"actual_answer\": outputs[\"incorrect_text\"][entry][\"truth\"],\n",
        "            \"category\": \"incorrect\"\n",
        "        }\n",
        "    ])\n",
        "\n",
        "# join the evaluation data with the predictions\n",
        "results = results.reset_index(drop=True)\n",
        "results = eval_data_df.set_index(\"id\").join(results.set_index(\"id\"))\n",
        "results = results.drop_duplicates()\n",
        "\n",
        "# resume the training run and log the table\n",
        "with wandb.init(resume=model.wandb_run_id, project=wandb_project) as training_run:\n",
        "    training_run.log({\"eval-results\": wandb.Table(dataframe=results)})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx3rfTI9DMLm"
      },
      "source": [
        "Inside the W&B web app,\n",
        "you can interact with logged table data\n",
        "to perform post-hoc analyses,\n",
        "including filtering, grouping, and computing derived metrics.\n",
        "\n",
        "Below is a screenshot of a table that compares the model's outputs\n",
        "to the actual ground truth across multiple runs.\n",
        "\n",
        "You can view and interact with it in your browser\n",
        "[here](https://wandb.ai/wandb/SimpleTransformers-QA/reports/Shared-panel-21-09-10-12-09-00--VmlldzoxMDExNDQw).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PTn95n3fYIo"
      },
      "source": [
        "<img src=\"https://imgur.com/i5wCEfF.png\" width=\"600\" alt=\"Weights & Biases\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSxtJml6MjFn"
      },
      "source": [
        "## Use the API to attach the train-test splits to the training run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IYoIMe02vmP"
      },
      "source": [
        "Logged information from experiments and workflows\n",
        "often needs to be programmatically accessed or updated.\n",
        "For those tasks, we provide\n",
        "[a public API](https://docs.wandb.ai/guides/track/public-api-guide).\n",
        "\n",
        "Here we'll use it to update the training run with\n",
        "the dataset artifacts that we uploaded earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65ZTdRf1eQYv"
      },
      "outputs": [],
      "source": [
        "# initialize the wandb api object\n",
        "api = wandb.Api()\n",
        "\n",
        "# retrieve our training run\n",
        "training_run = api.run(wandb_project + \"/\" + model.wandb_run_id)\n",
        "\n",
        "# retrieve the artifacts we'll be using\n",
        "train_data_artifact = api.artifact(wandb_project + \"/\" + \"train-data:latest\")\n",
        "eval_data_artifact = api.artifact(wandb_project + \"/\" + \"eval-data:latest\")\n",
        "\n",
        "# mark the training run as using the training and eval data artifacts\n",
        "training_run.use_artifact(train_data_artifact)\n",
        "training_run.use_artifact(eval_data_artifact);"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "SimpleTransformersQA",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
