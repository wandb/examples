{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/prompts/WandB_Prompts_Quickstart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "<!--- @wandbcode{prompts-quickstart} -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
    "<!--- @wandbcode{prompts-quickstart} -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Weights & Biases Prompts](https://docs.wandb.ai/guides/prompts?utm_source=code&utm_medium=colab&utm_campaign=prompts)** is a suite of LLMOps tools built for the development of LLM-powered applications. \n",
    "\n",
    "Use W&B Prompts to visualize and inspect the execution flow of your LLMs, analyze the inputs and outputs of your LLMs, view the intermediate results and securely store and manage your prompts and LLM chain configurations.\n",
    "\n",
    "#### [ðŸª„ View Prompts In Action](https://wandb.ai/timssweeney/prompts-demo/)\n",
    "\n",
    "**In this notebook we will demostrate W&B Prompts:**\n",
    "\n",
    "- Using our 1-line LangChain integration\n",
    "- Using our Trace class when building your own LLM Pipelines \n",
    "\n",
    "See here for the full [W&B Prompts documentation](https://docs.wandb.ai/guides/prompts)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"wandb>=0.15.6\" -qqq\n",
    "!pip install \"langchain>=0.0.218\" openai -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "assert langchain.__version__ >= \"0.0.218\", \"Please ensure you are using LangChain v0.0.218 or higher\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This demo requires that you have an [OpenAI key](https://platform.openai.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if os.getenv(\"OPENAI_API_KEY\") is None:\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass(\"Paste your OpenAI key from: https://platform.openai.com/account/api-keys\\n\")\n",
    "assert os.getenv(\"OPENAI_API_KEY\", \"\").startswith(\"sk-\"), \"This doesn't look like a valid OpenAI API key\"\n",
    "print(\"OpenAI API key configured\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W&B Prompts\n",
    "\n",
    "W&B Prompts consists of three main components:\n",
    "\n",
    "**Trace table**: Overview of the inputs and outputs of a chain.\n",
    "\n",
    "**Trace timeline**: Displays the execution flow of the chain and is color-coded according to component types.\n",
    "\n",
    "**Model architecture**: View details about the structure of the chain and the parameters used to initialize each component of the chain.\n",
    "\n",
    "After running this section, you will see a new panel automatically created in your workspace, showing each execution, the trace, and the model architecture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/wandb/examples/master/colabs/prompts/prompts.png\" alt=\"Weights & Biases Prompts image\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maths with LangChain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the `LANGCHAIN_WANDB_TRACING` environment variable as well as any other relevant [W&B environment variables](https://docs.wandb.ai/guides/track/environment-variables). This could includes a W&B project name, team name, and more. See [wandb.init](https://docs.wandb.ai/ref/python/init) for a full list of arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_WANDB_TRACING\"] = \"true\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"langchain-testing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import load_tools, initialize_agent, AgentType"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a standard math Agent using LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0)\n",
    "tools = load_tools([\"llm-math\"], llm=llm)\n",
    "math_agent = initialize_agent(tools, \n",
    "                              llm, \n",
    "                              agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use LangChain as normal by calling your Agent.\n",
    "\n",
    " You will see a Weights & Biases run start and you will be asked for your [Weights & Biases API key](wwww.wandb.ai/authorize). Once your enter your API key, the inputs and outputs of your Agent calls will start to be streamed to the Weights & Biases App."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some sample maths questions\n",
    "questions = [\n",
    "  \"Find the square root of 5.4.\",\n",
    "  \"What is 3 divided by 7.34 raised to the power of pi?\",\n",
    "  \"What is the sin of 0.47 radians, divided by the cube root of 27?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "  try:\n",
    "    # call your Agent as normal\n",
    "    answer = math_agent.run(question)\n",
    "    print(answer)\n",
    "  except Exception as e:\n",
    "    # any errors will be also logged to Weights & Biases\n",
    "    print(e)\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once each Agent execution completes, all calls in your LangChain object will be logged to Weights & Biases"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain Context Manager\n",
    "Depending on your use case, you might instead prefer to use a context manager to manage your logging to W&B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import wandb_tracing_enabled\n",
    "\n",
    "# unset the environment variable and use a context manager instead\n",
    "if \"LANGCHAIN_WANDB_TRACING\" in os.environ:\n",
    "    del os.environ[\"LANGCHAIN_WANDB_TRACING\"]\n",
    "\n",
    "# enable tracing using a context manager\n",
    "with wandb_tracing_enabled():\n",
    "    math_agent.run(\"What is 5 raised to .123243 power?\")  # this should be traced\n",
    "\n",
    "math_agent.run(\"What is 2 raised to .123243 power?\")  # this should not be traced"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Lang Chain Implementation\n",
    "\n",
    "\n",
    "A W&B Trace is created by logging 1 or more \"spans\". A root span is expected, which can accept nested child spans, which can in turn accept their own child spans. A Span represents a unit of work, Spans can have type `AGENT`, `TOOL`, `LLM` or `CHAIN`\n",
    "\n",
    "When logging with Trace, a single W&B run can have multiple calls to a LLM, Tool, Chain or Agent logged to it, there is no need to start a new W&B run after each generation from your model or pipeline, instead each call will be appended to the Trace Table.\n",
    "\n",
    "In this quickstart, we will how to log a single call to an OpenAI model to W&B Trace as a single span. Then we will show how to log a more complex series of nested spans.\n",
    "\n",
    "## Logging with W&B Trace"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call wandb.init to start a W&B run. Here you can pass a W&B project name as well as an entity name (if logging to a W&B Team), as well as a config and more. See wandb.init for the full list of arguments.\n",
    "\n",
    "You will see a Weights & Biases run start and be asked for your [Weights & Biases API key](wwww.wandb.ai/authorize). Once your enter your API key, the inputs and outputs of your Agent calls will start to be streamed to the Weights & Biases App.\n",
    "\n",
    "**Note:** A W&B run supports logging as many traces you needed to a single run, i.e. you can make multiple calls of `run.log` without the need to create a new run each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# start a wandb run to log to\n",
    "wandb.init(project=\"trace-example\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also set the entity argument in wandb.init if logging to a W&B Team.\n",
    "\n",
    "### Logging a single Span\n",
    "Now we will query OpenAI times and log the results to a W&B Trace. We will log the inputs and outputs, start and end times, whether the OpenAI call was successful, the token usage, and additional metadata.\n",
    "\n",
    "You can see the full description of the arguments to the Trace class [here](https://soumik12345.github.io/wandb-addons/prompts/tracer/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import datetime\n",
    "from wandb_addons.prompts import Trace\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# define your conifg\n",
    "model_name = \"gpt-3.5-turbo\"\n",
    "temperature = 0.7\n",
    "system_message = \"You are a helpful assistant that always replies in 3 concise bullet points using markdown.\"\n",
    "\n",
    "queries_ls = [\n",
    "  \"What is the capital of France?\",\n",
    "  \"How do I boil an egg?\" * 10000,  # deliberately trigger an openai error\n",
    "  \"What to do if the aliens arrive?\" \n",
    "]\n",
    "\n",
    "for query in queries_ls:\n",
    "    messages=[\n",
    "      {\"role\": \"system\", \"content\": system_message},\n",
    "      {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "\n",
    "    start_time_ms = datetime.datetime.now().timestamp() * 1000\n",
    "    try:\n",
    "      response = openai.ChatCompletion.create(model=model_name,\n",
    "                                              messages=messages,\n",
    "                                              temperature=temperature\n",
    "                                              )   \n",
    "\n",
    "      end_time_ms = round(datetime.datetime.now().timestamp() * 1000)  # logged in milliseconds\n",
    "      status=\"success\"\n",
    "      status_message=None,\n",
    "      response_text = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "      token_usage = response[\"usage\"].to_dict()\n",
    "      \n",
    "    \n",
    "    except Exception as e:\n",
    "      end_time_ms = round(datetime.datetime.now().timestamp() * 1000)  # logged in milliseconds\n",
    "      status=\"error\"\n",
    "      status_message=str(e)\n",
    "      response_text = \"\"\n",
    "      token_usage = {}\n",
    "\n",
    "    # create a span in wandb\n",
    "    root_span = Trace(\n",
    "          name=\"root_span\",\n",
    "          kind=\"llm\",  # kind can be \"llm\", \"chain\", \"agent\" or \"tool\"\n",
    "          status_code=status,\n",
    "          status_message=status_message,\n",
    "          metadata={\"temperature\": temperature,\n",
    "                    \"token_usage\": token_usage, \n",
    "                    \"model_name\": model_name},\n",
    "          start_time_ms=start_time_ms,\n",
    "          end_time_ms=end_time_ms,\n",
    "          inputs={\"system_prompt\": system_message, \"query\": query},\n",
    "          outputs={\"response\": response_text},\n",
    "          )\n",
    "    \n",
    "    # log the span to wandb\n",
    "    root_span.log(name=\"openai_trace\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging a LLM pipeline using nested Spans\n",
    "\n",
    "In this example we will simulate an Agent being called, which then calls a LLM Chain, which calls an OpenAI LLM and then the Agent \"calls\" a Calculator tool.\n",
    "\n",
    "The inputs, outputs and metadata for each step in the execution of our \"Agent\" is logged in its own span. Spans can have child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# The query our agent has to answer\n",
    "query = \"How many days until the next US election?\"\n",
    "\n",
    "# part 1 - an Agent is started...\n",
    "start_time_ms = round(datetime.datetime.now().timestamp() * 1000)\n",
    "\n",
    "root_span = Trace(\n",
    "      name=\"MyAgent\",\n",
    "      kind=\"agent\",\n",
    "      start_time_ms=start_time_ms,\n",
    "      metadata={\"user\": \"optimus_12\"})\n",
    "\n",
    "\n",
    "# part 2 - The Agent calls into a LLMChain..\n",
    "chain_span = Trace(\n",
    "      name=\"LLMChain\",\n",
    "      kind=\"chain\",\n",
    "      start_time_ms=start_time_ms)\n",
    "\n",
    "# add the Chain span as a child of the root\n",
    "root_span.add_child(chain_span)\n",
    "\n",
    "\n",
    "# part 3 - the LLMChain calls an OpenAI LLM...\n",
    "messages=[\n",
    "  {\"role\": \"system\", \"content\": system_message},\n",
    "  {\"role\": \"user\", \"content\": query}\n",
    "]\n",
    "\n",
    "response = openai.ChatCompletion.create(model=model_name,\n",
    "                                        messages=messages,\n",
    "                                        temperature=temperature)   \n",
    "\n",
    "llm_end_time_ms = round(datetime.datetime.now().timestamp() * 1000)\n",
    "response_text = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "token_usage = response[\"usage\"].to_dict()\n",
    "\n",
    "llm_span = Trace(\n",
    "      name=\"OpenAI\",\n",
    "      kind=\"llm\",\n",
    "      status_code=\"success\",\n",
    "      metadata={\"temperature\":temperature,\n",
    "                \"token_usage\": token_usage, \n",
    "                \"model_name\":model_name},\n",
    "      start_time_ms=start_time_ms,\n",
    "      end_time_ms=llm_end_time_ms,\n",
    "      inputs={\"system_prompt\":system_message, \"query\":query},\n",
    "      outputs={\"response\": response_text},\n",
    "      )\n",
    "\n",
    "# add the LLM span as a child of the Chain span...\n",
    "chain_span.add_child(llm_span)\n",
    "\n",
    "# update the end time of the Chain span\n",
    "chain_span.add_inputs_and_outputs(\n",
    "      inputs={\"query\":query},\n",
    "      outputs={\"response\": response_text})\n",
    "\n",
    "# update the Chain span's end time\n",
    "chain_span.end_time_ms = llm_end_time_ms\n",
    "\n",
    "\n",
    "# part 4 - the Agent then calls a Tool...\n",
    "time.sleep(3)\n",
    "days_to_election = 117\n",
    "tool_end_time_ms = round(datetime.datetime.now().timestamp() * 1000)\n",
    "\n",
    "# create a Tool span \n",
    "tool_span = Trace(\n",
    "      name=\"Calculator\",\n",
    "      kind=\"tool\",\n",
    "      status_code=\"success\",\n",
    "      start_time_ms=llm_end_time_ms,\n",
    "      end_time_ms=tool_end_time_ms,\n",
    "      inputs={\"input\": response_text},\n",
    "      outputs={\"result\": days_to_election})\n",
    "\n",
    "# add the TOOL span as a child of the root\n",
    "root_span.add_child(tool_span)\n",
    "\n",
    "\n",
    "# part 5 - the final results from the tool are added \n",
    "root_span.add_inputs_and_outputs(inputs={\"query\": query},\n",
    "                                 outputs={\"result\": days_to_election})\n",
    "root_span.end_time_ms = tool_end_time_ms\n",
    "\n",
    "\n",
    "# part 6 - log all spans to W&B by logging the root span\n",
    "root_span.log(name=\"openai_trace\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once each Agent execution completes, all calls in your LangChain object will be logged to Weights & Biases"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
