{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèÉ‚Äç‚ôÄÔ∏è W&B & Keras - Analyze Model Predictions\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/keras/Use_WandbEvalCallback_in_your_Keras_workflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "Use Weights & Biases for machine learning experiment tracking, dataset versioning, and project collaboration.\n",
    "\n",
    "<img src=\"http://wandb.me/mini-diagram\" width=\"650\" alt=\"Weights & Biases\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This colab notebook introduces the `WandbEvalCallback` which is an abstract callback that be inherited to build useful callbacks for model prediction visualization and dataset visualization. Refer to the [üí´ `WandbEvalCallback`](https://colab.research.google.com/drive/107uB39vBulCflqmOWolu38noWLxAT6Be#scrollTo=u50GwKJ70WeJ&line=1&uniqifier=1) section for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üå¥ Setup and Installation\n",
    "\n",
    "First, let us install the latest version of Weights and Biases. We will then authenticate this colab instance to use W&B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -qq -U wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Weights and Biases related imports\n",
    "import wandb\n",
    "from wandb.keras import WandbMetricsLogger\n",
    "from wandb.keras import WandbModelCheckpoint\n",
    "from wandb.keras import WandbEvalCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this is your first time using W&B or you are not logged in, the link that appears after running `wandb.login()` will take you to sign-up/login page. Signing up for a [free account](https://wandb.ai/signup) is as easy as a few clicks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üå≥ Hyperparameters\n",
    "\n",
    "Use of proper config system is a recommended best practice for reproducible machine learning. We can track the hyperparameters for every experiment using W&B. In this colab we will be using simple Python `dict` as our config system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "configs = dict(\n",
    "    num_classes = 10,\n",
    "    shuffle_buffer = 1024,\n",
    "    batch_size = 64,\n",
    "    image_size = 28,\n",
    "    image_channels = 1,\n",
    "    earlystopping_patience = 3,\n",
    "    learning_rate = 1e-3,\n",
    "    epochs = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üçÅ Dataset\n",
    "\n",
    "In this colab, we will be using [CIFAR100](https://www.tensorflow.org/datasets/catalog/cifar100) dataset from TensorFlow Dataset catalog. We aim to build a simple image classification pipeline using TensorFlow/Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "train_ds, valid_ds = tfds.load('fashion_mnist', split=['train', 'test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "\n",
    "def parse_data(example):\n",
    "    # Get image\n",
    "    image = example[\"image\"]\n",
    "    # image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "\n",
    "    # Get label\n",
    "    label = example[\"label\"]\n",
    "    label = tf.one_hot(label, depth=configs[\"num_classes\"])\n",
    "\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def get_dataloader(ds, configs, dataloader_type=\"train\"):\n",
    "    dataloader = ds.map(parse_data, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    if dataloader_type==\"train\":\n",
    "        dataloader = dataloader.shuffle(configs[\"shuffle_buffer\"])\n",
    "      \n",
    "    dataloader = (\n",
    "        dataloader\n",
    "        .batch(configs[\"batch_size\"])\n",
    "        .prefetch(AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "trainloader = get_dataloader(train_ds, configs)\n",
    "validloader = get_dataloader(valid_ds, configs, dataloader_type=\"valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéÑ Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def get_model(configs):\n",
    "    backbone = tf.keras.applications.mobilenet_v2.MobileNetV2(weights='imagenet', include_top=False)\n",
    "    backbone.trainable = False\n",
    "\n",
    "    inputs = layers.Input(shape=(configs[\"image_size\"], configs[\"image_size\"], configs[\"image_channels\"]))\n",
    "    resize = layers.Resizing(32, 32)(inputs)\n",
    "    neck = layers.Conv2D(3, (3,3), padding=\"same\")(resize)\n",
    "    preprocess_input = tf.keras.applications.mobilenet.preprocess_input(neck)\n",
    "    x = backbone(preprocess_input)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = layers.Dense(configs[\"num_classes\"], activation=\"softmax\")(x)\n",
    "\n",
    "    return models.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = get_model(configs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåø Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = \"adam\",\n",
    "    loss = \"categorical_crossentropy\",\n",
    "    metrics = [\"accuracy\", tf.keras.metrics.TopKCategoricalAccuracy(k=5, name='top@5_accuracy')]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üí´ `WandbEvalCallback`\n",
    "\n",
    "The `WandbEvalCallback` is an abstract base class to build Keras callbacks for primarily model prediction visualization and secondarily dataset visualization.\n",
    "\n",
    "This is a dataset and task agnostic abstract callback. To use this, inherit from this base callback class and implement the `add_ground_truth` and `add_model_prediction` methods.\n",
    "\n",
    "The `WandbEvalCallback` is a utility class that provides helpful methods to:\n",
    "\n",
    "- create data and prediction `wandb.Table` instances,\n",
    "- log data and prediction Tables as `wandb.Artifact`,\n",
    "- logs the data table `on_train_begin`,\n",
    "- logs the prediction table `on_epoch_end`.\n",
    "\n",
    "As an example, we have implemented `WandbClfEvalCallback` below for an image classification task. This example callback:\n",
    "- logs the validation data (`data_table`) to W&B,\n",
    "- performs inference and logs the prediction (`pred_table`) to W&B on every epoch end.\n",
    "\n",
    "\n",
    "## How the memory footprint is reduced?\n",
    "\n",
    "We log the `data_table` to W&B when the `on_train_begin` method is ivoked. Once it's uploaded as a W&B Artifact, we get a reference to this table which can be accessed using `data_table_ref` class variable. The `data_table_ref` is a 2D list that can be indexed like `self.data_table_ref[idx][n]` where `idx` is the row number while `n` is the column number. Let's see the usage in the example below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class WandbClfEvalCallback(WandbEvalCallback):\n",
    "    def __init__(\n",
    "        self, validloader, data_table_columns, pred_table_columns, num_samples=100\n",
    "    ):\n",
    "        super().__init__(data_table_columns, pred_table_columns)\n",
    "\n",
    "        self.val_data = validloader.unbatch().take(num_samples)\n",
    "\n",
    "    def add_ground_truth(self, logs=None):\n",
    "        for idx, (image, label) in enumerate(self.val_data):\n",
    "            self.data_table.add_data(\n",
    "                idx,\n",
    "                wandb.Image(image),\n",
    "                np.argmax(label, axis=-1)\n",
    "            )\n",
    "\n",
    "    def add_model_predictions(self, epoch, logs=None):\n",
    "        # Get predictions\n",
    "        preds = self._inference()\n",
    "        table_idxs = self.data_table_ref.get_index()\n",
    "\n",
    "        for idx in table_idxs:\n",
    "            pred = preds[idx]\n",
    "            self.pred_table.add_data(\n",
    "                epoch,\n",
    "                self.data_table_ref.data[idx][0],\n",
    "                self.data_table_ref.data[idx][1],\n",
    "                self.data_table_ref.data[idx][2],\n",
    "                pred\n",
    "            )\n",
    "\n",
    "    def _inference(self):\n",
    "      preds = []\n",
    "      for image, label in self.val_data:\n",
    "          pred = self.model(tf.expand_dims(image, axis=0))\n",
    "          argmax_pred = tf.argmax(pred, axis=-1).numpy()[0]\n",
    "          preds.append(argmax_pred)\n",
    "\n",
    "      return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåª Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize a W&B run\n",
    "run = wandb.init(\n",
    "    project = \"intro-keras\",\n",
    "    config = configs\n",
    ")\n",
    "\n",
    "# Train your model\n",
    "model.fit(\n",
    "    trainloader,\n",
    "    epochs = configs[\"epochs\"],\n",
    "    validation_data = validloader,\n",
    "    callbacks = [\n",
    "        WandbMetricsLogger(log_freq=10),\n",
    "        WandbClfEvalCallback(\n",
    "            validloader,\n",
    "            data_table_columns=[\"idx\", \"image\", \"ground_truth\"],\n",
    "            pred_table_columns=[\"epoch\", \"idx\", \"image\", \"ground_truth\", \"prediction\"]\n",
    "        ) # Notice the use of WandbEvalCallback here\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Close the W&B run\n",
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
