{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soumik12345/examples/blob/master/stylegan_nada/StyleGAN-NADA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnAjMMq4dThE"
      },
      "source": [
        "# üî•üî• StyelGAN-NADA + WandB Playground ü™Ñüêù\n",
        "\n",
        "<!--- @wandbcode{stylegan-nada-colab} -->\n",
        "\n",
        "**Original Implementation:** https://github.com/rinongal/StyleGAN-nada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59AYdVbDDGSI"
      },
      "source": [
        "# Step 1: Setup required libraries and models. \n",
        "This may take a few minutes.\n",
        "\n",
        "You may optionally enable downloads with pydrive in order to authenticate and avoid drive download limits when fetching pre-trained ReStyle and StyleGAN2 models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQUmbQKndDml",
        "outputId": "224ea079-923b-49c5-98cb-6d11f91a0730"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Cloning into '/content/restyle'...\n",
            "remote: Enumerating objects: 326, done.\u001b[K\n",
            "remote: Total 326 (delta 0), reused 0 (delta 0), pack-reused 326\u001b[K\n",
            "Receiving objects: 100% (326/326), 28.12 MiB | 18.98 MiB/s, done.\n",
            "Resolving deltas: 100% (121/121), done.\n",
            "--2022-06-23 00:23:05--  https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/1335132/d2f252e2-9801-11e7-9fbf-bc7b4e4b5c83?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220623%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220623T002305Z&X-Amz-Expires=300&X-Amz-Signature=19caa644ca9dbe7c8f9d4156eec6fd4e0b19b2b4e1eafc87e29270b15c748bd9&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=1335132&response-content-disposition=attachment%3B%20filename%3Dninja-linux.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-06-23 00:23:05--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/1335132/d2f252e2-9801-11e7-9fbf-bc7b4e4b5c83?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220623%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220623T002305Z&X-Amz-Expires=300&X-Amz-Signature=19caa644ca9dbe7c8f9d4156eec6fd4e0b19b2b4e1eafc87e29270b15c748bd9&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=1335132&response-content-disposition=attachment%3B%20filename%3Dninja-linux.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 77854 (76K) [application/octet-stream]\n",
            "Saving to: ‚Äòninja-linux.zip‚Äô\n",
            "\n",
            "ninja-linux.zip     100%[===================>]  76.03K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2022-06-23 00:23:05 (9.19 MB/s) - ‚Äòninja-linux.zip‚Äô saved [77854/77854]\n",
            "\n",
            "Archive:  ninja-linux.zip\n",
            "  inflating: /usr/local/bin/ninja    \n",
            "update-alternatives: using /usr/local/bin/ninja to provide /usr/bin/ninja (ninja) in auto mode\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 53 kB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.0)\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.12.19-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.8 MB 15.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 181 kB 60.0 MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.2.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.6.0-py2.py3-none-any.whl (145 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 145 kB 50.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63 kB 1.8 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.6.15)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=648575d324b2f52db0b17d7e894e4a22f22a695a9af914da8bc419cf8188caf6\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb, ftfy\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 ftfy-6.1.1 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.6.0 setproctitle-1.2.3 shortuuid-1.0.9 smmap-5.0.0 wandb-0.12.19\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-4li7m7gs\n",
            "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-4li7m7gs\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.64.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (0.12.0+cu113)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->clip==1.0) (4.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (2022.6.15)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369387 sha256=c9d2987f914583090ade57ad3cb7c1e973df8fa21e75996ce877d7685788aca2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-c3vvgkp9/wheels/fd/b9/c3/5b4470e35ed76e174bff77c92f91da82098d5e35fd5bc8cdac\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n",
            "Cloning into '/content/stylegan_ada'...\n",
            "remote: Enumerating objects: 74, done.\u001b[K\n",
            "remote: Total 74 (delta 0), reused 0 (delta 0), pack-reused 74\u001b[K\n",
            "Unpacking objects: 100% (74/74), done.\n",
            "Cloning into '/content/stylegan_nada'...\n",
            "remote: Enumerating objects: 370, done.\u001b[K\n",
            "remote: Counting objects: 100% (115/115), done.\u001b[K\n",
            "remote: Compressing objects: 100% (74/74), done.\u001b[K\n",
            "remote: Total 370 (delta 56), reused 74 (delta 39), pack-reused 255\u001b[K\n",
            "Receiving objects: 100% (370/370), 23.54 MiB | 28.87 MiB/s, done.\n",
            "Resolving deltas: 100% (105/105), done.\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import os\n",
        "\n",
        "restyle_dir = os.path.join(\"/content\", \"restyle\")\n",
        "stylegan_ada_dir = os.path.join(\"/content\", \"stylegan_ada\")\n",
        "stylegan_nada_dir = os.path.join(\"/content\", \"stylegan_nada\")\n",
        "\n",
        "output_dir = os.path.join(\"/content\", \"output\")\n",
        "\n",
        "output_model_dir = os.path.join(output_dir, \"models\")\n",
        "output_image_dir = os.path.join(output_dir, \"images\")\n",
        "\n",
        "# install requirements\n",
        "!git clone https://github.com/yuval-alaluf/restyle-encoder.git $restyle_dir\n",
        "\n",
        "!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
        "!sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
        "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force\n",
        "\n",
        "!pip install ftfy regex tqdm wandb\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "\n",
        "!git clone https://github.com/NVlabs/stylegan2-ada/ $stylegan_ada_dir\n",
        "!git clone https://github.com/rinongal/stylegan-nada.git $stylegan_nada_dir\n",
        "\n",
        "from argparse import Namespace\n",
        "\n",
        "import sys\n",
        "import wandb\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from glob import glob\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "sys.path.append(restyle_dir)\n",
        "sys.path.append(stylegan_nada_dir)\n",
        "sys.path.append(os.path.join(stylegan_nada_dir, \"ZSSGAN\"))\n",
        "\n",
        "device = 'cuda'\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xpox1UODDWO9"
      },
      "source": [
        "# Step 2: Choose a model type.\n",
        "Model will be downloaded and converted to a pytorch compatible version.\n",
        "\n",
        "Re-runs of the cell with the same model will re-use the previously downloaded version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "ca58N05prDkR",
        "outputId": "5975b4a2-d82c-4e33-aceb-87f2946372cc"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.19"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220623_002330-cnkov18r</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/geekyrakshit/stylegan-nada/runs/cnkov18r\" target=\"_blank\">fancy-universe-20</a></strong> to <a href=\"https://wandb.ai/geekyrakshit/stylegan-nada\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact ffhq:v0, 363.79MB. 1 files... Done. 0:0:4.6\n"
          ]
        }
      ],
      "source": [
        "project = \"stylegan-nada\" #@param {\"type\": \"string\"}\n",
        "source_model_type = 'ffhq' #@param['ffhq', 'cat', 'dog', 'church', 'horse', 'car']\n",
        "\n",
        "artifact_adressed = {\n",
        "    \"car\": \"geekyrakshit/stylegan-nada/car:v0\",\n",
        "    \"horse\": \"geekyrakshit/stylegan-nada/horse:v0\",\n",
        "    \"church\": \"geekyrakshit/stylegan-nada/church:v0\",\n",
        "    \"dog\": \"geekyrakshit/stylegan-nada/dog:v0\",\n",
        "    \"cat\": \"geekyrakshit/stylegan-nada/cat:v0\",\n",
        "    \"ffhq\": \"geekyrakshit/stylegan-nada/ffhq:v0\"\n",
        "}\n",
        "\n",
        "model_names = {\n",
        "    \"ffhq\": \"ffhq.pt\",\n",
        "    \"cat\": \"afhqcat.pkl\",\n",
        "    \"dog\": \"afhqdog.pkl\",\n",
        "    \"church\": \"stylegan2-church-config-f.pkl\",\n",
        "    \"car\": \"stylegan2-car-config-f.pkl\",\n",
        "    \"horse\": \"stylegan2-horse-config-f.pkl\"\n",
        "}\n",
        "\n",
        "dataset_sizes = {\n",
        "    \"ffhq\": 1024,\n",
        "    \"cat\": 512,\n",
        "    \"dog\": 512,\n",
        "    \"church\": 256,\n",
        "    \"horse\": 256,\n",
        "    \"car\": 512,\n",
        "}\n",
        "\n",
        "wandb.init(project=project, job_type=\"train\")\n",
        "config = wandb.config\n",
        "config.source_model_type = source_model_type\n",
        "\n",
        "artifact = wandb.use_artifact(artifact_adressed[source_model_type])\n",
        "pretrained_model_dir = artifact.download()\n",
        "pt_file_name = model_names[source_model_type].split(\".\")[0] + \".pt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQo-Im3jDibt"
      },
      "source": [
        "# Step 3: Train the model.\n",
        "Describe your source and target class. These describe the direction of change you're trying to apply (e.g. \"photo\" to \"sketch\", \"dog\" to \"the joker\" or \"dog\" to \"avocado dog\").\n",
        "\n",
        "Alternatively, upload a directory with a small (~3) set of target style images (there is no need to preprocess them in any way) and set `style_image_dir` to point at them. This will use the images as a target rather than the source/class texts.\n",
        "\n",
        "We reccomend leaving the 'improve shape' button unticked at first, as it will lead to an increase in running times and is often not needed.\n",
        "For more drastic changes, turn it on and increase the number of iterations.\n",
        "\n",
        "As a rule of thumb:\n",
        "- Style and minor domain changes ('photo' -> 'sketch') require ~200-400 iterations.\n",
        "- Identity changes ('person' -> 'taylor swift') require ~150-200 iterations.\n",
        "- Simple in-domain changes ('face' -> 'smiling face') may require as few as 50.\n",
        "- The `style_image_dir` option often requires ~400-600 iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NZmcsUa3sY1n"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from tqdm import notebook\n",
        "\n",
        "from ZSSGAN.model.ZSSGAN import ZSSGAN\n",
        "from ZSSGAN.utils.file_utils import save_images, get_dir_img_list\n",
        "from ZSSGAN.utils.training_utils import mixing_noise\n",
        "\n",
        "from IPython.display import display\n",
        "\n",
        "source_class = \"Human\" #@param {\"type\": \"string\"}\n",
        "config.source_class = source_class\n",
        "\n",
        "target_class = \"The Joker\" #@param {\"type\": \"string\"}\n",
        "config.target_class = target_class\n",
        "\n",
        "style_image_dir = \"\" #@param {'type': 'string'}\n",
        "config.style_image_dir = style_image_dir\n",
        "\n",
        "seed = 3 #@param {\"type\": \"integer\"}\n",
        "config.seed = seed\n",
        "\n",
        "target_img_list = get_dir_img_list(style_image_dir) if style_image_dir else None\n",
        "\n",
        "improve_shape = False #@param{type:\"boolean\"}\n",
        "config.improve_shape = improve_shape\n",
        "\n",
        "model_choice = [\"ViT-B/32\", \"ViT-B/16\"]\n",
        "model_weights = [1.0, 0.0]\n",
        "\n",
        "if improve_shape or style_image_dir:\n",
        "    model_weights[1] = 1.0\n",
        "    \n",
        "mixing = 0.9 if improve_shape else 0.0\n",
        "\n",
        "auto_layers_k = int(2 * (2 * np.log2(dataset_sizes[source_model_type]) - 2) / 3) if improve_shape else 0\n",
        "auto_layer_iters = 1 if improve_shape else 0\n",
        "\n",
        "training_iterations = 251 #@param {type: \"integer\"}\n",
        "config.training_iterations = training_iterations\n",
        "\n",
        "output_interval     = 10 #@param {type: \"integer\"}\n",
        "config.output_interval = output_interval\n",
        "\n",
        "save_interval       = 10 #@param {type: \"integer\"}\n",
        "config.save_interval = save_interval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "76Pv3k4GtEzd"
      },
      "outputs": [],
      "source": [
        "training_args = {\n",
        "    \"size\": dataset_sizes[source_model_type],\n",
        "    \"batch\": 2,\n",
        "    \"n_sample\": 4,\n",
        "    \"output_dir\": output_dir,\n",
        "    \"lr\": 0.002,\n",
        "    \"frozen_gen_ckpt\": os.path.join(pretrained_model_dir, pt_file_name),\n",
        "    \"train_gen_ckpt\": os.path.join(pretrained_model_dir, pt_file_name),\n",
        "    \"iter\": training_iterations,\n",
        "    \"source_class\": source_class,\n",
        "    \"target_class\": target_class,\n",
        "    \"lambda_direction\": 1.0,\n",
        "    \"lambda_patch\": 0.0,\n",
        "    \"lambda_global\": 0.0,\n",
        "    \"lambda_texture\": 0.0,\n",
        "    \"lambda_manifold\": 0.0,\n",
        "    \"auto_layer_k\": auto_layers_k,\n",
        "    \"auto_layer_iters\": auto_layer_iters,\n",
        "    \"auto_layer_batch\": 8,\n",
        "    \"output_interval\": 50,\n",
        "    \"clip_models\": model_choice,\n",
        "    \"clip_model_weights\": model_weights,\n",
        "    \"mixing\": mixing,\n",
        "    \"phase\": None,\n",
        "    \"sample_truncation\": 0.7,\n",
        "    \"save_interval\": save_interval,\n",
        "    \"target_img_list\": target_img_list,\n",
        "    \"img2img_batch\": 16,\n",
        "    \"channel_multiplier\": 2,\n",
        "    \"sg3\": False,\n",
        "    \"sgxl\": False,\n",
        "}\n",
        "config.training_args = training_args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RKc8ePSZtabS",
        "outputId": "338fd1a4-735c-40dd-cff3-53aaf22dc54e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading base models...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 338M/338M [00:01<00:00, 216MiB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 244M/244M [00:01<00:00, 210MiB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 335M/335M [00:02<00:00, 126MiB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done\n"
          ]
        }
      ],
      "source": [
        "args = Namespace(**training_args)\n",
        "\n",
        "resume_training_from_artifact = False #@param{type:\"boolean\"}\n",
        "config.resume_training_from_artifact = resume_training_from_artifact\n",
        "\n",
        "checkpoint_artifact_address = \"geekyrakshit/stylegan-nada/model-winter-frost-8:v14\" #@param {'type': 'string'}\n",
        "config.checkpoint_artifact_address = checkpoint_artifact_address\n",
        "\n",
        "print(\"Loading base models...\")\n",
        "net = ZSSGAN(args)\n",
        "print(\"Done\")\n",
        "\n",
        "g_reg_ratio = 4 / 5\n",
        "\n",
        "g_optim = torch.optim.Adam(\n",
        "    net.generator_trainable.parameters(),\n",
        "    lr=args.lr * g_reg_ratio,\n",
        "    betas=(0 ** g_reg_ratio, 0.99 ** g_reg_ratio),\n",
        ")\n",
        "\n",
        "if resume_training_from_artifact and checkpoint_artifact_address is not None:\n",
        "    artifact = wandb.use_artifact(checkpoint_artifact_address)\n",
        "    artifact_dir = artifact.download()\n",
        "    checkpoint = torch.load(glob(os.path.join(artifact_dir, \"*.pt\"))[0])\n",
        "    net.generator_trainable.generator.load_state_dict(checkpoint['g_ema'])\n",
        "    g_optim.load_state_dict(checkpoint['g_optim'])\n",
        "\n",
        "# Set up output directories.\n",
        "sample_dir = os.path.join(args.output_dir, \"sample\")\n",
        "config.sample_dir = sample_dir\n",
        "\n",
        "ckpt_dir   = os.path.join(args.output_dir, \"checkpoint\")\n",
        "config.ckpt_dir = ckpt_dir\n",
        "\n",
        "os.makedirs(sample_dir, exist_ok=True)\n",
        "os.makedirs(ckpt_dir, exist_ok=True)\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "049a8516735e4a999c41127562dceb1e"
          ]
        },
        "id": "o0Cdt6IhtR8X",
        "outputId": "66749f14-e81d-44a6-c314-abda96dfb2f1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "049a8516735e4a999c41127562dceb1e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/251 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/stylegan_nada/ZSSGAN/op/conv2d_gradfix.py:89: UserWarning: conv2d_gradfix not supported on PyTorch 1.11.0+cu113. Falling back to torch.nn.functional.conv2d().\n",
            "  f\"conv2d_gradfix not supported on PyTorch {torch.__version__}. Falling back to torch.nn.functional.conv2d().\"\n"
          ]
        }
      ],
      "source": [
        "fixed_z = torch.randn(args.n_sample, 512, device=device)\n",
        "\n",
        "for i in notebook.tqdm(range(args.iter)):\n",
        "    net.train()\n",
        "        \n",
        "    sample_z = mixing_noise(args.batch, 512, args.mixing, device)\n",
        "\n",
        "    [sampled_src, sampled_dst], clip_loss = net(sample_z)\n",
        "    wandb.log({\"CLIP-Loss\": clip_loss.item()}, step=i)\n",
        "\n",
        "\n",
        "    net.zero_grad()\n",
        "    clip_loss.backward()\n",
        "\n",
        "    g_optim.step()\n",
        "\n",
        "    if i % output_interval == 0:\n",
        "        net.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            [sampled_src, sampled_dst], loss = net([fixed_z], truncation=args.sample_truncation)\n",
        "\n",
        "            if source_model_type == 'car':\n",
        "                sampled_dst = sampled_dst[:, :, 64:448, :]\n",
        "\n",
        "            sampled_dst = torch.permute(sampled_dst, (0, 2, 3, 1)).cpu()\n",
        "            sampled_dst = [wandb.Image(dst.numpy()) for dst in sampled_dst]\n",
        "            wandb.log({\"Samples\": sampled_dst}, step=i)\n",
        "    \n",
        "    if (args.save_interval > 0) and (i > 0) and (i % args.save_interval == 0):\n",
        "        model_file = f\"{ckpt_dir}/{str(i).zfill(6)}.pt\"\n",
        "        torch.save(\n",
        "            {\n",
        "                \"g_ema\": net.generator_trainable.generator.state_dict(),\n",
        "                \"g_optim\": g_optim.state_dict(),\n",
        "            },\n",
        "            model_file,\n",
        "        )\n",
        "        artifact = wandb.Artifact(f\"model-{wandb.run.name}\", type=\"model\")\n",
        "        artifact.add_file(model_file)\n",
        "        wandb.log_artifact(artifact, aliases=[\"latest\", f\"step_{i}\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjOVRYmrDp3e"
      },
      "source": [
        "# Step 4: Generate samples with the new model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "soOm2PWDooX5",
        "outputId": "117c7f2d-a139-43c9-e6f0-c985a9a65b57"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact model-fancy-universe-20:latest, 330.33MB. 1 files... Done. 0:0:0.8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading models from checkpoint artifact...\n",
            "Done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/stylegan_nada/ZSSGAN/op/conv2d_gradfix.py:89: UserWarning: conv2d_gradfix not supported on PyTorch 1.11.0+cu113. Falling back to torch.nn.functional.conv2d().\n",
            "  f\"conv2d_gradfix not supported on PyTorch {torch.__version__}. Falling back to torch.nn.functional.conv2d().\"\n"
          ]
        }
      ],
      "source": [
        "truncation = 0.7 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "config.truncation = truncation\n",
        "\n",
        "samples = 9\n",
        "config.samples = samples\n",
        "\n",
        "artifact = wandb.use_artifact(f\"model-{wandb.run.name}:latest\")\n",
        "artifact_dir = artifact.download()\n",
        "checkpoint = torch.load(glob(os.path.join(artifact_dir, \"*.pt\"))[0])\n",
        "\n",
        "print(\"Loading models from checkpoint artifact...\")\n",
        "net = ZSSGAN(args)\n",
        "net.generator_trainable.generator.load_state_dict(checkpoint['g_ema'])\n",
        "print(\"Done\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    net.eval()\n",
        "    sample_z = torch.randn(samples, 512, device=device)\n",
        "\n",
        "    [sampled_src, sampled_dst], loss = net([sample_z], truncation=truncation)\n",
        "\n",
        "    if source_model_type == 'car':\n",
        "        sampled_dst = sampled_dst[:, :, 64:448, :]\n",
        "\n",
        "    sampled_dst = torch.permute(sampled_dst, (0, 2, 3, 1)).cpu()\n",
        "    sampled_dst = [wandb.Image(dst.numpy()) for dst in sampled_dst]\n",
        "    wandb.log({\"Generated Samples\": sampled_dst})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgrZKI9fNT5_"
      },
      "source": [
        "## Editing a real image with Re-Style inversion (currently only FFHQ inversion is supported):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af_m5zE8NXbn"
      },
      "source": [
        "### Step 1: Set up Re-Style.\n",
        "\n",
        "This may take a few minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_2x2jYTNM82p",
        "outputId": "4b690c91-2680-4fba-8dee-b51b951d16c6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact restyle:v0, 1825.34MB. 2 files... Done. 0:0:16.8\n"
          ]
        }
      ],
      "source": [
        "from restyle.utils.common import tensor2im\n",
        "from restyle.models.psp import pSp\n",
        "from restyle.models.e4e import e4e\n",
        "\n",
        "\n",
        "artifact = wandb.use_artifact(\"geekyrakshit/stylegan-nada/restyle:v0\")\n",
        "pretrained_model_dir = artifact.download()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4qLGkjDNcEQ"
      },
      "source": [
        "### Step 2: Choose a re-style model\n",
        "\n",
        "We reccomend choosing the e4e model as it performs better under domain translations. Choose pSp for better reconstructions on minor domain changes (typically those that require less than 150 training steps)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VpYwimn2NCmd",
        "outputId": "bc6e2814-a65c-43fc-efac-22cf59713a90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading ReStyle e4e from checkpoint: ./artifacts/restyle:v0/restyle_e4e_ffhq_encode.pt\n",
            "Model successfully loaded!\n"
          ]
        }
      ],
      "source": [
        "encoder_type = 'e4e' #@param['psp', 'e4e']\n",
        "\n",
        "restyle_experiment_args = {\n",
        "    \"model_path\": os.path.join(pretrained_model_dir, f\"restyle_{encoder_type}_ffhq_encode.pt\"),\n",
        "    \"transform\": transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "}\n",
        "\n",
        "model_path = restyle_experiment_args['model_path']\n",
        "ckpt = torch.load(model_path, map_location='cpu')\n",
        "\n",
        "opts = ckpt['opts']\n",
        "\n",
        "opts['checkpoint_path'] = model_path\n",
        "opts = Namespace(**opts)\n",
        "\n",
        "restyle_net = (pSp if encoder_type == 'psp' else e4e)(opts)\n",
        "\n",
        "restyle_net.eval()\n",
        "restyle_net.cuda()\n",
        "print('Model successfully loaded!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsYrzHbLNg1R"
      },
      "source": [
        "### Step 3: Align and invert an image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uh5lxqaMNFJf"
      },
      "outputs": [],
      "source": [
        "def run_alignment(image_path):\n",
        "    import dlib\n",
        "    from scripts.align_faces_parallel import align_face\n",
        "    if not os.path.exists(\"shape_predictor_68_face_landmarks.dat\"):\n",
        "        print('Downloading files for aligning face image...')\n",
        "        os.system('wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2')\n",
        "        os.system('bzip2 -dk shape_predictor_68_face_landmarks.dat.bz2')\n",
        "        print('Done.')\n",
        "    predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
        "    aligned_image = align_face(filepath=image_path, predictor=predictor) \n",
        "    print(\"Aligned image has shape: {}\".format(aligned_image.size))\n",
        "    return aligned_image\n",
        "\n",
        "image_url = \"https://engineering.nyu.edu/sites/default/files/styles/square_large_default_2x/public/2018-06/yann-lecun.jpg\" #@param {\"type\": \"string\"}\n",
        "file_name = \"yann-lecun.jpg\" #@param {\"type\": \"string\"}\n",
        "\n",
        "!wget {image_url}\n",
        "\n",
        "image_path = os.path.join(\"/content\", file_name)\n",
        "original_image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "input_image = run_alignment(image_path)\n",
        "\n",
        "img_transforms = restyle_experiment_args['transform']\n",
        "transformed_image = img_transforms(input_image)\n",
        "\n",
        "def get_avg_image(net):\n",
        "    avg_image = net(net.latent_avg.unsqueeze(0),\n",
        "                    input_code=True,\n",
        "                    randomize_noise=False,\n",
        "                    return_latents=False,\n",
        "                    average_code=True)[0]\n",
        "    avg_image = avg_image.to('cuda').float().detach()\n",
        "    return avg_image\n",
        "\n",
        "opts.n_iters_per_batch = 5\n",
        "opts.resize_outputs = False  # generate outputs at full resolution\n",
        "\n",
        "from restyle.utils.inference_utils import run_on_batch\n",
        "\n",
        "with torch.no_grad():\n",
        "    avg_image = get_avg_image(restyle_net)\n",
        "    result_batch, result_latents = run_on_batch(transformed_image.unsqueeze(0).cuda(), restyle_net, opts, avg_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFsSPhI5NHxL"
      },
      "outputs": [],
      "source": [
        "inverted_latent = torch.Tensor(result_latents[0][4]).cuda().unsqueeze(0).unsqueeze(1)\n",
        "\n",
        "with torch.no_grad():\n",
        "    net.eval()\n",
        "    \n",
        "    [sampled_src, sampled_dst] = net(inverted_latent, input_is_latent=True)[0]\n",
        "    \n",
        "    sampled_src = torch.permute(sampled_src, (0, 2, 3, 1)).cpu().numpy()[0]\n",
        "    sampled_dst = torch.permute(sampled_dst, (0, 2, 3, 1)).cpu().numpy()[0]\n",
        "\n",
        "    table = wandb.Table(\n",
        "        columns=[\"Source-Class-Text\", \"Source-Image\", \"Target-Class-Text\", \"Translated-Image\"],\n",
        "        data=[[source_class, wandb.Image(sampled_src), target_class, wandb.Image(sampled_dst)]]\n",
        "    )\n",
        "\n",
        "    wandb.log({\"Restyle\": table})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4TMdzVKreCF"
      },
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "StyleGAN-NADA + WandB.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOPz8pscEJI5C1jiwdWivHo",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}