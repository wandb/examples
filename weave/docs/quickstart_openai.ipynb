{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# OpenAI\n",
        "\n",
        "**Note**: Do you want to experiment with OpenAI models on Weave without any set up? Try the [LLM Playground](https://weave-docs.wandb.ai/guides/tools/playground)."
      ],
      "metadata": {
        "id": "llQYmLvpDhTr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tracing\n",
        "\n",
        "It’s important to store traces of LLM applications in a central database, both during development and in production. You’ll use these traces for debugging and to help build a dataset of tricky examples to evaluate against while improving your application.\n",
        "\n",
        "Weave can automatically capture traces for the [openai python library](https://platform.openai.com/docs/libraries/python-library).\n",
        "\n",
        "Start capturing by calling `weave.init(<project-name>)` with a project name your choice.\n"
      ],
      "metadata": {
        "id": "e6tEieQSDstj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLFE70kZDYlR"
      },
      "outputs": [],
      "source": [
        "!pip install weave openai -qqq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "import getpass\n",
        "import weave\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key: \")"
      ],
      "metadata": {
        "id": "pOOdG6xRD2kn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If not already logged in, you will be prompted to log into wandb and authorize using your wandb account in the next step."
      ],
      "metadata": {
        "id": "Oau7hqsCECCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weave.init(\"quickstart_openai\")"
      ],
      "metadata": {
        "id": "IC-GZu42D6P6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = openai.OpenAI()\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are AGI. You will be provided with a message, and your task is to respond using emojis only.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"How are you?\"\n",
        "        }\n",
        "    ],\n",
        "    temperature=0.8,\n",
        "    max_tokens=64,\n",
        "    top_p=1\n",
        ")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "id": "7loaqIzxEFwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![openai.png](https://weave-docs.wandb.ai/assets/images/openai-3d16f489ef9afe0b50ee6b83f9b3b2f2.png)](https://wandb.ai/_scott/emoji-bot/weave/calls)"
      ],
      "metadata": {
        "id": "BvGVfDVLEh31"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Track your own ops\n",
        "\n",
        "Wrapping a function with `@weave.op` starts capturing inputs, outputs and app logic so you can debug how data flows through your app. You can deeply nest ops and build a tree of functions that you want to track. This also starts automatically versioning code as you experiment to capture ad-hoc details that haven't been committed to git.\n",
        "\n",
        "Simply create a function decorated with [`@weave.op`](https://weave-docs.wandb.ai/guides/tracking/ops) that calls into [openai python library](https://platform.openai.com/docs/reference/python-sdk?lang=python).\n",
        "\n",
        "In the example below, we have 2 functions wrapped with op. This helps us see how intermediate steps, like the retrieval step in a RAG app, are affecting how our app behaves."
      ],
      "metadata": {
        "id": "rJDZ96QtEpZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests, random\n",
        "PROMPT=\"\"\"Emulate the Pokedex from early Pokémon episodes. State the name of the Pokemon and then describe it.\n",
        "        Your tone is informative yet sassy, blending factual details with a touch of dry humor. Be concise, no more than 3 sentences. \"\"\"\n",
        "POKEMON = ['pikachu', 'charmander', 'squirtle', 'bulbasaur', 'jigglypuff', 'meowth', 'eevee']\n",
        "\n",
        "client = openai.OpenAI()\n",
        "\n",
        "\n",
        "@weave.op\n",
        "def get_pokemon_data(pokemon_name):\n",
        "    # This is a step within your application, like the retrieval step within a RAG app\n",
        "    url = f\"https://pokeapi.co/api/v2/pokemon/{pokemon_name}\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        name = data[\"name\"]\n",
        "        types = [t[\"type\"][\"name\"] for t in data[\"types\"]]\n",
        "        species_url = data[\"species\"][\"url\"]\n",
        "        species_response = requests.get(species_url)\n",
        "        evolved_from = \"Unknown\"\n",
        "        if species_response.status_code == 200:\n",
        "            species_data = species_response.json()\n",
        "            if species_data[\"evolves_from_species\"]:\n",
        "                evolved_from = species_data[\"evolves_from_species\"][\"name\"]\n",
        "        return {\"name\": name, \"types\": types, \"evolved_from\": evolved_from}\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "@weave.op\n",
        "def pokedex(name: str, prompt: str) -> str:\n",
        "    # This is your root op that calls out to other ops\n",
        "    data = get_pokemon_data(name)\n",
        "    if not data: return \"Error: Unable to fetch data\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\",\"content\": prompt},\n",
        "            {\"role\": \"user\", \"content\": str(data)}\n",
        "        ],\n",
        "        temperature=0.7,\n",
        "        max_tokens=100,\n",
        "        top_p=1\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Get data for a specific Pokémon\n",
        "pokemon_data = pokedex(random.choice(POKEMON), PROMPT)"
      ],
      "metadata": {
        "id": "cqxJnmcZEdki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Navigate to Weave and you can click `get_pokemon_data` in the UI to see the inputs & outputs of that step.\n",
        "\n",
        "[![openai-pokedex.png](https://weave-docs.wandb.ai/assets/images/openai-pokedex-1f8a19ae9d4a27f56961822bd4d20125.png)](https://wandb.ai/_scott/pokedex-openai/weave)"
      ],
      "metadata": {
        "id": "LiLWBzasFUeB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a `Model` for easier experimentation\n",
        "\n",
        "Organizing experimentation is difficult when there are many moving pieces. By using the [`Model`](https://weave-docs.wandb.ai/guides/core-types/models) class, you can capture and organize the experimental details of your app like your system prompt or the model you're using. This helps organize and compare different iterations of your app.\n",
        "\n",
        "In addition to versioning code and capturing inputs/outputs, [`Model`](https://weave-docs.wandb.ai/guides/core-types/models)s capture structured parameters that control your application’s behavior, making it easy to find what parameters worked best. You can also use Weave Models with `serve`, and [`Evaluation`](https://weave-docs.wandb.ai/guides/core-types/evaluations)s.\n",
        "\n",
        "In the example below, you can experiment with `model` and `system_message`. Every time you change one of these, you'll get a new _version_ of `GrammarCorrectorModel`."
      ],
      "metadata": {
        "id": "ahOR54_uFlQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarCorrectorModel(weave.Model): # Change to `weave.Model`\n",
        "  model: str\n",
        "  system_message: str\n",
        "\n",
        "  @weave.op()\n",
        "  def predict(self, user_input): # Change to `predict`\n",
        "    client = openai.OpenAI()\n",
        "    response = client.chat.completions.create(\n",
        "      model=self.model,\n",
        "      messages=[\n",
        "          {\n",
        "              \"role\": \"system\",\n",
        "              \"content\": self.system_message\n",
        "          },\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": user_input\n",
        "          }\n",
        "          ],\n",
        "          temperature=0,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "corrector = GrammarCorrectorModel(\n",
        "    model=\"gpt-3.5-turbo-1106\",\n",
        "    system_message = \"You are a grammar checker, correct the following user input.\"\n",
        ")\n",
        "\n",
        "result = corrector.predict(\"That was so easy, it was a piece of pie!\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "iajyNfAJFLwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![openai-model.png](https://weave-docs.wandb.ai/assets/images/openai-model-5255ed680d313b124b583ec73e1446b7.png)](https://wandb.ai/_scott/grammar-openai/weave/calls)"
      ],
      "metadata": {
        "id": "oFEXWrAcGDAW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usage Info\n",
        "\n",
        "The OpenAI calls return usage info as a default when `stream=False`. Weave will track this usage info and log it to weave to render token counts and cost of the call.\n",
        "\n",
        "In case you set `stream=True`, we will automatically patch the call execution with `stream_options={\"include_usage\": True}` argument. This will return the usage info in the last chunk to be rendered in the UI. As a user, the stream iterator will not contain this info.\n",
        "\n",
        "If you explicitly set `stream=True` and `stream_options={\"include_usage\": True}`, the returned stream object will contain the usage info. If you don't want to track the usage info you need to explicitly set `stream_options={\"include_usage\": False}`.\n",
        "\n",
        "## Support for deprecated function calling\n",
        "\n",
        "OpenAI deprecated the `functions` argument in favor of `tool_calls`. Since frameworks like Langchain, LlamaIndex, etc., still support this argument our OpenAI weave integration will trace if you pass list of function schemas to `functions` argument."
      ],
      "metadata": {
        "id": "dCa7LZdKGJvI"
      }
    }
  ]
}