{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructor\n",
    "\n",
    "[Instructor](https://python.useinstructor.com/) is a lightweight library that makes it easy to get structured data like JSON from LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U instructor openai weave -qqq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use OpenAI with Instructor, you need to set the `OPENAI_API_KEY` environment variable. You can get your API key from [platform.openai.com/api-keys](https://platform.openai.com/api-keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracing\n",
    "\n",
    "It’s important to store traces of language model applications in a central location, both during development and in production. These traces can be useful for debugging, and as a dataset that will help you improve your application.\n",
    "\n",
    "Weave will automatically capture traces for [Instructor](https://python.useinstructor.com/) and [OpenaAI](https://platform.openai.com/docs/libraries/python-library). To start tracking, calling `weave.init()` and use the library as normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor\n",
    "import weave\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "# Define your desired output structure\n",
    "class UserInfo(BaseModel):\n",
    "    user_name: str\n",
    "    age: int\n",
    "\n",
    "# Initialize Weave\n",
    "weave.init(project_name=\"quickstart-instructor\")\n",
    "\n",
    "# Patch the OpenAI client\n",
    "client = instructor.from_openai(OpenAI())\n",
    "\n",
    "# Extract structured data from natural language\n",
    "user_info = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    response_model=UserInfo,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"John Doe is 30 years old.\"}],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| [![](https://weave-docs.wandb.ai/assets/images/instructor_lm_trace-25522be59f6e456773b176ce0b23b2f6.gif)](https://wandb.ai/geekyrakshit/quickstart-instructor/weave/calls) |\n",
    "| --- |\n",
    "| Weave will now track and log all LLM calls made using Instructor. You can view the traces in the Weave web interface. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track your own ops\n",
    "\n",
    "Wrapping a function with [`@weave.op`](https://weave-docs.wandb.ai/guides/tracking/ops) starts capturing inputs, outputs and app logic so you can debug how data flows through your app. You can deeply nest ops and build a tree of functions that you want to track. This also starts automatically versioning code as you experiment to capture ad-hoc details that haven't been committed to git.\n",
    "\n",
    "Simply create a function decorated with [`@weave.op`](https://weave-docs.wandb.ai/guides/tracking/ops).\n",
    "\n",
    "In the example below, we have the function `extract_person` which is the metric function wrapped with [`@weave.op`](https://weave-docs.wandb.ai/guides/tracking/ops). This helps us see how intermediate steps, such as OpenAI chat completion call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your desired output structure\n",
    "class Person(BaseModel):\n",
    "    person_name: str\n",
    "    age: int\n",
    "\n",
    "\n",
    "# Extract structured data from natural language\n",
    "@weave.op()\n",
    "def extract_person(text: str) -> Person:\n",
    "    return client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": text},\n",
    "        ],\n",
    "        response_model=Person,\n",
    "    )\n",
    "\n",
    "\n",
    "person = extract_person(\"My name is John and I am 20 years old\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| [![](https://weave-docs.wandb.ai/assets/images/instructor_op_trace-bcceed1c88e0610f8675d059e22e8b0a.png)](https://wandb.ai/geekyrakshit/quickstart-instructor/weave/calls) |\n",
    "| --- |\n",
    "| Decorating the extract_person function with [`@weave.op`](https://weave-docs.wandb.ai/guides/tracking/ops) traces its inputs, outputs, and all internal LM calls made inside the function. Weave also automatically tracks and versions the structured objects generated by [Instructor](https://python.useinstructor.com/). |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a [`Model`](https://weave-docs.wandb.ai/guides/core-types/models) for easier experimentation\n",
    "\n",
    "Organizing experimentation is difficult when there are many moving pieces. By using the [`Model`](https://weave-docs.wandb.ai/guides/core-types/models) class, you can capture and organize the experimental details of your app like your system prompt or the model you're using. This helps organize and compare different iterations of your app.\n",
    "\n",
    "In addition to versioning code and capturing inputs/outputs, a [`Model`](https://weave-docs.wandb.ai/guides/core-types/models) captures structured parameters that control your application’s behavior, making it easy to find what parameters worked best. You can also use Weave a [`Model`](https://weave-docs.wandb.ai/guides/core-types/models) with serve, and [Evaluations](https://weave-docs.wandb.ai/guides/core-types/evaluations).\n",
    "\n",
    "In the example below, you can experiment with `PersonExtractor`. Every time you change one of these, you'll get a new version of `PersonExtractor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Iterable\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "\n",
    "# Define your desired output structure\n",
    "class Person(BaseModel):\n",
    "    person_name: str\n",
    "    age: int\n",
    "\n",
    "\n",
    "# Patch the OpenAI client\n",
    "lm_client = instructor.from_openai(AsyncOpenAI())\n",
    "\n",
    "\n",
    "class PersonExtractor(weave.Model):\n",
    "    openai_model: str\n",
    "    max_retries: int\n",
    "\n",
    "    @weave.op()\n",
    "    async def predict(self, text: str) -> List[Person]:\n",
    "        model = await lm_client.chat.completions.create(\n",
    "            model=self.openai_model,\n",
    "            response_model=Iterable[Person],\n",
    "            max_retries=self.max_retries,\n",
    "            stream=True,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a perfect entity extraction system\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Extract `{text}`\",\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "        return [m async for m in model]\n",
    "\n",
    "\n",
    "model = PersonExtractor(openai_model=\"gpt-4\", max_retries=2)\n",
    "await model.predict(\"John is 30 years old\")\n",
    "\n",
    "## If you're running from a python script, uncomment and use the following code instead of `await`\n",
    "# import asyncio\n",
    "# asyncio.run(model.predict(\"John is 30 years old\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| [![](https://weave-docs.wandb.ai/assets/images/instructor_weave_model-313d13f30307a5f2dad3063349e55c4f.png)](https://wandb.ai/geekyrakshit/quickstart-instructor/weave/calls) |\n",
    "| --- |\n",
    "| Tracing and versioning your calls using a [`Model`](https://weave-docs.wandb.ai/guides/core-types/models) |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
