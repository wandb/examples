{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MistralAI\n",
    "\n",
    "Weave automatically tracks and logs LLM calls made via the [MistralAI Python library](https://github.com/mistralai/client-python). \n",
    "\n",
    "> **Note: We support the new Mistral v1.0 SDK, check the migration guide [here](https://github.com/mistralai/client-python/blob/main/MIGRATION.md)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Traces\n",
    "\n",
    "It’s important to store traces of LLM applications in a central database, both during development and in production. You’ll use these traces for debugging, and as a dataset that will help you improve your application.\n",
    "\n",
    "Weave will automatically capture traces for [mistralai](https://github.com/mistralai/client-python). You can use the library as usual, start by calling `weave.init()`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install weave mistralai -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai import Mistral\n",
    "import weave\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "print(\"Enter your Mistral API key:\")\n",
    "os.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If not already logged in, you will be prompted to log into wandb and authorize using your wandb account in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Weave\n",
    "weave.init(\"quickstart_mistral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"mistral-large-latest\"\n",
    "\n",
    "client = Mistral(api_key=os.environ[\"MISTRAL_API_KEY\"])\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is the best French cheese?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "chat_response = client.chat.complete(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Weave will now track and log all LLM calls made through the MistralAI library. You can view the traces in the Weave web interface.\n",
    "\n",
    "[![mistral_trace.png](https://weave-docs.wandb.ai/assets/images/mistral_trace-3fa22a88515d57264ee6099e52aa206a.png)](https://wandb.ai/capecape/mistralai_project/weave/calls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Wrapping with your own ops\n",
    "\n",
    "Weave ops make results *reproducible* by automatically versioning code as you experiment, and they capture their inputs and outputs. Simply create a function decorated with [`@weave.op`](https://weave-docs.wandb.ai/guides/tracking/ops) that calls into [`mistralai.client.MistralClient.chat()`](https://docs.mistral.ai/capabilities/completion/) and Weave will track the inputs and outputs for you. Let's see how we can do this for our cheese recommender:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op\n",
    "def cheese_recommender(region:str, model:str) -> str:\n",
    "    \"Recommend the best cheese in a given region\"\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"What is the best cheese in {region}?\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    chat_response = client.chat.complete(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "    )\n",
    "    return chat_response.choices[0].message.content\n",
    "\n",
    "cheese_recommender(region=\"France\", model=model)\n",
    "cheese_recommender(region=\"Spain\", model=model)\n",
    "cheese_recommender(region=\"Netherlands\", model=model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "[![mistral_ops.png](https://weave-docs.wandb.ai/assets/images/mistral_ops-8e251fbc90ce13b865c2140b60445abf.png)](https://wandb.ai/capecape/mistralai_project/weave/calls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a `Model` for easier experimentation\n",
    "\n",
    "Organizing experimentation is difficult when there are many moving pieces. By using the [`Model`](https://weave-docs.wandb.ai/guides/core-types/models) class, you can capture and organize the experimental details of your app like your system prompt or the model you're using. This helps organize and compare different iterations of your app. \n",
    "\n",
    "In addition to versioning code and capturing inputs/outputs, [`Model`](https://weave-docs.wandb.ai/guides/core-types/models)s capture structured parameters that control your application’s behavior, making it easy to find what parameters worked best. You can also use Weave Models with [`serve`](https://weave-docs.wandb.ai/guides/tools/serve), and [`Evaluation`](https://weave-docs.wandb.ai/guides/core-types/evaluations)s.\n",
    "\n",
    "In the example below, you can experiment with `model` and `country`. Every time you change one of these, you'll get a new _version_ of `CheeseRecommender`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheeseRecommender(weave.Model): # Change to `weave.Model`\n",
    "    model: str\n",
    "    temperature: float\n",
    "\n",
    "    @weave.op\n",
    "    def predict(self, region:str) -> str: # Change to `predict`\n",
    "        \"Recommend the best cheese in a given region\"\n",
    "        \n",
    "        client = Mistral(api_key=os.environ[\"MISTRAL_API_KEY\"])\n",
    "\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"What is the best cheese in {region}?\",\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        chat_response = client.chat.complete(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=self.temperature\n",
    "        )\n",
    "        return chat_response.choices[0].message.content\n",
    "\n",
    "cheese_model = CheeseRecommender(\n",
    "    model=model,\n",
    "    temperature=0.0\n",
    "    )\n",
    "result = cheese_model.predict(region=\"France\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![mistral_model.png](https://weave-docs.wandb.ai/assets/images/mistral_model-0a89ef6374318020e9da24e9d74f0d52.png)](https://wandb.ai/capecape/mistralai_project/weave/models)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wandb-examples",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
