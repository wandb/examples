{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LiteLLM\n",
    "\n",
    "Weave automatically tracks and logs LLM calls made via LiteLLM, after `weave.init()` is called.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Traces\n",
    "\n",
    "It's important to store traces of LLM applications in a central database, both during development and in production. You'll use these traces for debugging, and as a dataset that will help you improve your application.\n",
    "\n",
    "> **Note: When using LiteLLM, make sure to import the library using `import litellm` and call the completion function with `litellm.completion` instead of `from litellm import completion`. This ensures that all functions and attributes are correctly referenced.**\n",
    "\n",
    "\n",
    "Weave will automatically capture traces for LiteLLM. You can use the library as usual, start by calling `weave.init()`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install weave litellm -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "import weave\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "print(\"Enter your OpenAI API key:\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "print(\"Enter your Anthropic API key:\")\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If not already logged in, you will be prompted to log into wandb and authorize using your wandb account in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Weave\n",
    "weave.init(\"quickstart_litellm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an LLM call to OpenAI\n",
    "openai_response = litellm.completion(\n",
    "    model=\"gpt-4o-mini\", \n",
    "    messages=[{\"role\": \"user\", \"content\": \"Translate 'Hello, how are you?' to French\"}],\n",
    "    max_tokens=1024\n",
    ")\n",
    "print(openai_response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an LLM call to Anthropic\n",
    "claude_response = litellm.completion(\n",
    "    model=\"claude-3-5-sonnet-20240620\", \n",
    "    messages=[{\"role\": \"user\", \"content\": \"Translate 'Hello, how are you?' to French\"}],\n",
    "    max_tokens=1024\n",
    ")\n",
    "print(claude_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weave will now track and log all LLM calls made through LiteLLM. You can view the traces in the Weave web interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping with your own ops\n",
    "\n",
    "Weave ops make results reproducible by automatically versioning code as you experiment, and they capture their inputs and outputs. Simply create a function decorated with `@weave.op` that calls into LiteLLM's completion function and Weave will track the inputs and outputs for you. Here's an example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op\n",
    "def translate(text: str, target_language: str, model: str) -> str:\n",
    "    response = litellm.completion(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"Translate '{text}' to {target_language}\"}],\n",
    "        max_tokens=1024\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(translate(\"Hello, how are you?\", \"French\", \"gpt-3.5-turbo\"))\n",
    "print(translate(\"Hello, how are you?\", \"Spanish\", \"claude-3-5-sonnet-20240620\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a `Model` for easier experimentation\n",
    "\n",
    "Organizing experimentation is difficult when there are many moving pieces. By using the [`Model`](https://weave-docs.wandb.ai/guides/core-types/models) class, you can capture and organize the experimental details of your app like your system prompt or the model you're using. This helps organize and compare different iterations of your app.\n",
    "\n",
    "In addition to versioning code and capturing inputs/outputs, Models capture structured parameters that control your application's behavior, making it easy to find what parameters worked best. You can also use Weave Models with [`serve`](https://weave-docs.wandb.ai/guides/tools/serve), and [Evaluations](https://weave-docs.wandb.ai/guides/core-types/evaluations).\n",
    "\n",
    "In the example below, you can experiment with different models and temperatures:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslatorModel(weave.Model):\n",
    "    model: str\n",
    "    temperature: float\n",
    "  \n",
    "    @weave.op\n",
    "    def predict(self, text: str, target_language: str):\n",
    "        response = litellm.completion(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"You are a translator. Translate the given text to {target_language}.\"},\n",
    "                {\"role\": \"user\", \"content\": text}\n",
    "            ],\n",
    "            max_tokens=1024,\n",
    "            temperature=self.temperature\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "# Create instances with different models\n",
    "gpt_translator = TranslatorModel(model=\"gpt-4o-mini\", temperature=0.3)\n",
    "claude_translator = TranslatorModel(model=\"claude-3-5-sonnet-20240620\", temperature=0.1)\n",
    "\n",
    "# Use different models for translation\n",
    "english_text = \"Hello, how are you today?\"\n",
    "\n",
    "print(\"GPT-4o-mini Translation to French:\")\n",
    "print(gpt_translator.predict(english_text, \"French\"))\n",
    "\n",
    "print(\"\\nClaude-3.5 Sonnet Translation to Spanish:\")\n",
    "print(claude_translator.predict(english_text, \"Spanish\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Calling\n",
    "\n",
    "LiteLLM supports function calling for compatible models. Weave will automatically track these function calls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = litellm.completion(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Translate 'Hello, how are you?' to French\"}],\n",
    "    functions=[\n",
    "        {\n",
    "            \"name\": \"translate\",\n",
    "            \"description\": \"Translate text to a specified language\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"text\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The text to translate\",\n",
    "                    },\n",
    "                    \"target_language\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The language to translate to\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"text\", \"target_language\"],\n",
    "            },\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We automatically capture the functions you used in the prompt and keep them versioned.\n",
    "\n",
    "[![litellm_gif.png](https://weave-docs.wandb.ai/assets/images/litellm_gif-0d244c8a37332356eaa663f46931bf31.gif)](https://wandb.ai/a-sh0ts/weave_litellm_integration/weave/calls)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wandb-examples",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
