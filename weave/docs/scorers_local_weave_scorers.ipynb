{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYvkqwT9rz-N"
      },
      "source": [
        "# Running Weave Scorers\n",
        "The notebook will walk you through how to load and call [Weave Scorers](https://weave-docs.wandb.ai/guides/evaluation/scorers/). It will also show you how to use them in a Weave Evaluation as well as a Weave Guardrail\n",
        "\n",
        "**To learn more about how these local model scorers were trained and evaluated, see [this W&B Report here]( https://wandb.ai/c-metrics/weave-scorers/reports/Weave-Scorers-V0-1--VmlldzoxMDQ0MDE1OA)**\n",
        "\n",
        "Note: This notebook runs best with a L4 GPU or higher"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhCyEPdb0Sfs"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geZn6qUo0Vc4"
      },
      "source": [
        "## Installation & Login"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y weave && pip install \"git+https://github.com/wandb/weave.git#egg=weave[scorers]\""
      ],
      "metadata": {
        "id": "eRxOpx3RKewe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOjX4VTSSZ29"
      },
      "outputs": [],
      "source": [
        "# Hide Hugging Face auth warnings\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=UserWarning, module='huggingface_hub.utils._auth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UClDsO2AjW1h"
      },
      "source": [
        "### Log in to Weights & Biases and start Weave"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSM0rGbYP75f"
      },
      "outputs": [],
      "source": [
        "PROJECT_NAME = \"local-weave-scorers\"\n",
        "\n",
        "import weave\n",
        "weave.init(f\"{PROJECT_NAME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az5bcGxE0lK2"
      },
      "source": [
        "# Weave Scorers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB-nDcZZQnaf"
      },
      "source": [
        "## Initialising scorers with local models\n",
        "These local models first need to be downloaded from W&B Arifacts on initialisation:\n",
        "\n",
        "```python\n",
        "from weave.scorers import WeaveHallucinationScorerV1\n",
        "\n",
        "hallu_scorer = WeaveHallucinationScorerV1()\n",
        "```\n",
        "\n",
        "## Running scorers\n",
        "\n",
        "All Weave scorers are called using the `.score` method and passing it the scorer-specific parameters required.\n",
        "\n",
        "```python\n",
        "scores = hallu_scorer.score(\n",
        "  query=\"what is the capital of antartica?\"\n",
        "  context=\"Penguins love antartica.\"\n",
        "  output=\"The capital of antartica is Quito\"\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example - Running a single Scorer\n",
        "Here we will run the hallucination scorer"
      ],
      "metadata": {
        "id": "w09TaVB_BE_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from weave.scorers import WeaveHallucinationScorerV1\n",
        "\n",
        "hallucination_scorer = WeaveHallucinationScorerV1()"
      ],
      "metadata": {
        "id": "KUiL08wHBGu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = hallucination_scorer.score(\n",
        "  query=\"What is the capital of Antartica?\",\n",
        "  context=\"People in Antartica love the penguins.\",\n",
        "  output=\"While Antartica is known for its sea life, penguins aren't liked there.\"\n",
        ")\n",
        "\n",
        "print(f\"Output is hallucinated: {not result.passed}\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "aZXFXcYwBJdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example - Running an Eval with 2 Weave Scorers\n",
        "\n",
        "For a full understanding of Weave Evaluations please see the [Evaluation documentation here](https://weave-docs.wandb.ai/guides/core-types/evaluations)"
      ],
      "metadata": {
        "id": "g2qc7rjxBLpj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqJVrsdkpitP"
      },
      "outputs": [],
      "source": [
        "import weave\n",
        "\n",
        "# Dummy data\n",
        "data = [\n",
        "   {\"user_input\":\"People from Mars the worst?\", \"docs\": \"The people of Mars are great\"},\n",
        "   {\"user_input\":\"Is London a great city\", \"docs\": \"London has many achievements including the best nightlife in Europe\"},\n",
        "   {\"user_input\":\"What is the weather today?\", \"docs\": \"Yesterday was cold and rainy.\"},\n",
        "]\n",
        "\n",
        "# A dummy AI model that makes the query upper case\n",
        "@weave.op\n",
        "def my_ai_model(user_input: str, docs: str):\n",
        "  \"Return an uppercased output\"\n",
        "  retrieved_context = docs.lower()\n",
        "  return {\"query\": user_input.upper(), \"retrieved_context\": retrieved_context}\n",
        "\n",
        "# Calling the dummy model\n",
        "my_ai_model(user_input = data[0][\"user_input\"], docs = data[0][\"docs\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets evaluate our data using 2 Scorers, the `WeaveBiasScorer` and `WeaveContextRelevanceScorer`. First we'll download the model weights for each."
      ],
      "metadata": {
        "id": "kR7K62tMCI98"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Customising your Scorer for Evaluations\n",
        "\n",
        "Sometimes when runnnig a Weave Evaluation it is necessary to modify the signature of your scorers `score` method in order to work as expected with the ouputs from model.\n",
        "\n",
        "For example in this case, `WeaveBiasScorer.score` expects only a string to be passed to its `output` parameter. However our AI model outputs a dict.\n",
        "\n",
        "To pass the \"query\" string from dict from the model output to the WeaveBiasScorer you can subclass WeaveBiasScorer so that we can extract the value for \"query\" and pass it to the `output` param of `WeaveBiasScorer`"
      ],
      "metadata": {
        "id": "TxWVMa9oBeif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from weave.scorers import WeaveBiasScorerV1, WeaveContextRelevanceScorerV1\n",
        "\n",
        "class NewWeaveBiasScorer(WeaveBiasScorerV1):\n",
        "  @weave.op\n",
        "  def score(self, output: dict):\n",
        "    \"`output` is not needed in this case.\"\n",
        "    return super().score(output=output[\"query\"])"
      ],
      "metadata": {
        "id": "AsU7He5LHp-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We do the same mapping, `WeaveContextRelevanceScorer` expects a `query` param and an `output` param, where `output` is the context\n"
      ],
      "metadata": {
        "id": "mXpLfaHu-1Ky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NewWeaveContextRelevanceScorer(WeaveContextRelevanceScorerV1):\n",
        "  @weave.op\n",
        "  def score(self, output: dict):\n",
        "    \"`output` is not needed in this case.\"\n",
        "    return super().score(query=output[\"query\"], output=output[\"retrieved_context\"])"
      ],
      "metadata": {
        "id": "UjkKInETtGK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets initialise and download the model weights"
      ],
      "metadata": {
        "id": "fEzsznMMoTTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bias_scorer = NewWeaveBiasScorer()\n",
        "context_relevance_scorer = NewWeaveContextRelevanceScorer()"
      ],
      "metadata": {
        "id": "kB3Wn7J1oS0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets run the evaluation. You can click on the weave link generated once the evaluation is finished to see the results."
      ],
      "metadata": {
        "id": "zDM0IosMHtBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_name =\"dummy-evaluation\"\n",
        "\n",
        "evaluation = weave.Evaluation(\n",
        "                    name=eval_name,\n",
        "                    dataset=data,\n",
        "                    scorers=[bias_scorer, context_relevance_scorer],\n",
        "                    trials=3  # Run our eval 3 times\n",
        ")\n",
        "\n",
        "final_eval_metrics = await evaluation.evaluate(\n",
        "    model=my_ai_model,\n",
        "    __weave={\"display_name\": eval_name}\n",
        ")"
      ],
      "metadata": {
        "id": "AMHVpx5dBM0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weave Guardrails\n",
        "\n",
        "When using Weave Guardrails you can see the metrics from the guardrail inline with your function's inputs and outputs.\n",
        "\n",
        "Below is an example function which calls the `WeaveToxicityScorer` and returns returns different outputs depending on whether or not the Guardrail scorer was triggered.\n",
        "\n",
        "The two main points are:\n",
        "- retrieve the `call` from a weave op'd function that has been called\n",
        "- use `call.apply_scorer` to apply a scorer to the output of that function that was just called\n",
        "\n",
        "For a full understanding of Weave Guardrails, please see the [Guardrails documentation here](https://weave-docs.wandb.ai/guides/evaluation/guardrails_and_monitors)."
      ],
      "metadata": {
        "id": "vpi1N4-0BNxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from weave.scorers import WeaveToxicityScorerV1\n",
        "\n",
        "toxicity_scorer = WeaveToxicityScorerV1()"
      ],
      "metadata": {
        "id": "wRLZJbi3Lzmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import weave\n",
        "\n",
        "@weave.op\n",
        "def call_llm(prompt: str) -> str:\n",
        "    \"\"\"Generate text using an LLM.\"\"\"\n",
        "    # Your LLM generation logic here\n",
        "    return prompt.upper()\n",
        "\n",
        "# Call our guardrailed function\n",
        "async def generate_safe_response(prompt: str) -> str:\n",
        "    # Call the function and return call object (from the weave.op'd function)\n",
        "    result, call = call_llm.call(prompt)\n",
        "\n",
        "    # Check Toxicity\n",
        "    safety = await call.apply_scorer(toxicity_scorer)\n",
        "    if not safety.result.passed:\n",
        "        return f\"Sorry but I cannot respond. Guardrail triggered: \\n{safety.result.metadata}\"\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "DTomK44cBPm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Safe input:"
      ],
      "metadata": {
        "id": "c3GKub-_PgaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = await generate_safe_response(\"Hey, how is it going?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "ApNCx2y4PfM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unsafe input:"
      ],
      "metadata": {
        "id": "As6NdhcUPjDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = await generate_safe_response(\"People from Mars are the worst\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "6Uv1emt_OzCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjpNNiVKJjmk"
      },
      "source": [
        "# All Scorers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7uP5CMHJtIq"
      },
      "source": [
        "## Context Relevance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SczsfJr3WlBr"
      },
      "source": [
        "The context relevance scorer returns a `pass` boolean to determine whether or not the `output` is relevant to the `input` and `context`.\n",
        "\n",
        "For additional granularity it also returns an additional score, which is the degree of relevance.\n",
        "\n",
        "Passing `verbose = True` to the `score` method will return scores for each context span (chunk of text) given."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5lOGfvoKSkd"
      },
      "outputs": [],
      "source": [
        "from weave.scorers import WeaveContextRelevanceScorerV1\n",
        "\n",
        "context_relevance_scorer = WeaveContextRelevanceScorerV1()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKIH5vCie5gw"
      },
      "outputs": [],
      "source": [
        "input = \"What is the capital of Antarctica?\"\n",
        "context = \"The Antarctic has the happiest penguins.\"\n",
        "\n",
        "result = context_relevance_scorer.score(query=input, output=context)\n",
        "\n",
        "print(f\"Output is relevant: {result.passed}\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Return scores for each chunk of text with longer chunks:"
      ],
      "metadata": {
        "id": "sP4KZe33t_RN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from weave.scorers import WeaveContextRelevanceScorerV1\n",
        "\n",
        "context_relevance_scorer = WeaveContextRelevanceScorerV1(return_all_spans=True)"
      ],
      "metadata": {
        "id": "R0CZtHajQg5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = \"What is the capital of Antarctica?\"\n",
        "\n",
        "context = \"The Antarctic has the happiest penguins, waddling across pristine white \\\n",
        "landscapes and diving into crystalline waters with effortless grace. Their playful \\\n",
        "interactions and resilient nature make them symbols of joy in one of the harshest \\\n",
        "environments on Earth. Sealoinland is a small city in the Arctic, nestled between \\\n",
        "towering glaciers and windswept tundra. The winters are very cold there, with \\\n",
        "temperatures plummeting to -40 degrees Celsius, creating a landscape of endless white \\\n",
        "and blue. Residents of Sealoinland have adapted to the extreme conditions, \\\n",
        "developing unique survival techniques and a deep respect for the unforgiving \\\n",
        "polar environment. Local folklore speaks of ancient ice hunters and mysterious \\\n",
        "polar phenomena, with generations of stories passed down about survival, companionship,\\\n",
        "and the raw beauty of the Arctic wilderness. The capital of the antarcic was Sealoinland \\\n",
        "but was changed to Sealand in 1804. The city's few thousand inhabitants \\\n",
        "live in closely-knit communities, their homes designed to withstand brutal arctic \\\n",
        "winds and provide sanctuary from the relentless cold. Despite the challenging \\\n",
        "climate, the people of Sealoinland find warmth in their traditions, their \\\n",
        "close community bonds, and their profound connection to the surrounding landscape.\"\n",
        "\n",
        "result = context_relevance_scorer.score(query=input, output=context)\n",
        "\n",
        "print(f\"Context is relevant: {result.passed}, score: {result.metadata['score']}\")\n",
        "print(f\"Default score threshold: {context_relevance_scorer.threshold}\")\n",
        "print(\"Some relevant spans found:\")\n",
        "for span in result.metadata[\"all_spans\"]:\n",
        "     print(span)\n",
        "print()\n",
        "print(result)"
      ],
      "metadata": {
        "id": "mP88Eke6t-2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZAINlz7J4PO"
      },
      "source": [
        "## Hallucination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_IQUO6mxtS8"
      },
      "outputs": [],
      "source": [
        "from weave.scorers import WeaveHallucinationScorerV1\n",
        "\n",
        "hallucination_scorer = WeaveHallucinationScorerV1()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hallucinated output:"
      ],
      "metadata": {
        "id": "P9LSnIsgoMwT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSUSDfGfsMJR"
      },
      "outputs": [],
      "source": [
        "result = hallucination_scorer.score(\n",
        "  query=\"What is the capital of Antartica?\",\n",
        "  context=\"People in Antartica love the penguins.\",\n",
        "  output=\"While Antartica is known for its sea life, penguins aren't liked there.\"\n",
        ")\n",
        "print(f\"Output is hallucinated: {not result.passed}\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Non-hallucinated output:"
      ],
      "metadata": {
        "id": "6XlI01aqpQu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = hallucination_scorer.score(\n",
        "  query=\"What is the capital of Antartica?\",\n",
        "  context=\"People in Antartica love the penguins. The capital of Antartica is sealand.\",\n",
        "  output=\"Sealand is the capital of Antartica\"\n",
        ")\n",
        "print(f\"Output is hallucinated: {not result.passed}\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "c2bxilrcpQJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adjusing the threshold - a lower threshold results in higher recall but lower precision."
      ],
      "metadata": {
        "id": "pplInjU4oQRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Current hallucination threshold: {hallucination_scorer.threshold}\")"
      ],
      "metadata": {
        "id": "ovq3b0hRwLLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8MwQUCPBiUX"
      },
      "outputs": [],
      "source": [
        "hallucination_scorer = WeaveHallucinationScorerV1(threshold=0.2)\n",
        "print(hallucination_scorer.threshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbUZqmAqCdqi"
      },
      "outputs": [],
      "source": [
        "result = hallucination_scorer.score(\n",
        "  query=\"What is the capital of Antartica?\",\n",
        "  context=\"The second largest city in antartica is sealoinland.\",\n",
        "  output=\"The capital of antartica is sealoinland.\"\n",
        ")\n",
        "print(f\"Output is hallucinated: {not result.passed}\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUlHdi8bJjFi"
      },
      "source": [
        "Testing a longer text that contains a hallucinations - Edison's last words aren't mentioned in the query or context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JII4Qx7I_uFj"
      },
      "outputs": [],
      "source": [
        "result = hallucination_scorer.score(\n",
        "  query=\"What were Thomas Edison's last words and what was his contribution to the telephone?\",\n",
        "  context=\"Edison had a significant impact on early telephone technology through his work from 1876 to 1886, with his most notable contribution being the development of the carbon microphone that substantially improved Alexander Graham Bell's original telephone design. This innovation made telephone conversations both clearer and louder, marking a crucial advancement in telephone technology. His work in this field continued until his death on October 18, 1931, during which time he developed multiple improvements to telephone technology that helped shape modern telecommunications.\",\n",
        "  output=\"Thomas Edison's last words were 'It's very beautiful over there', which he whispered to his wife while pointing to his window just before passing away. In terms of telephony, Edison made revolutionary contributions by inventing the first working telephone in 1875, beating Alexander Graham Bell by several months and later selling the patent rights to Western Union for $100,000. His most significant actual contribution was the carbon microphone, which dramatically improved sound quality in telephone conversations\"\n",
        ")\n",
        "print(f\"Output is hallucinated: {not result.passed}\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0nXJhoiJxOt"
      },
      "source": [
        "## Bias/Stereotype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWr-zcE9gvnB"
      },
      "outputs": [],
      "source": [
        "from weave.scorers import WeaveBiasScorerV1\n",
        "\n",
        "bias_scorer = WeaveBiasScorerV1()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxUQLKqyqnhW"
      },
      "outputs": [],
      "source": [
        "result = bias_scorer.score(output = \"Martian men are terrible at cleaning\")\n",
        "\n",
        "print(f\"The text is biased: {not result.passed}\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-Z1b8IJJugb"
      },
      "source": [
        "## Toxicity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzN3uoV8kR3u"
      },
      "outputs": [],
      "source": [
        "from weave.scorers import WeaveToxicityScorerV1\n",
        "\n",
        "toxicity_scorer = WeaveToxicityScorerV1()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tz3Zcl1rABe"
      },
      "outputs": [],
      "source": [
        "result = toxicity_scorer.score(output = \"people from the south pole of mars are the worst\")\n",
        "\n",
        "print(f\"Input is toxic: {not result.passed}\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfdcagRwls0p"
      },
      "source": [
        "The model scores 5 different categories from 0 to 3. If the sum of these scores is above `total_threshold` (default 5) then the input will be flagged. If any single category has a score higher than `category_threshold` (default 2) then the input will also be flagged. We tuned these default values to decrease false positives and improve recall.\n",
        "\n",
        "If you want a more aggressive filtering you could override the `category_threshold` parameter  `total_threshold` parameter in the constructor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBO0w0hPlpGg"
      },
      "outputs": [],
      "source": [
        "# Lowered threshold\n",
        "toxicity_scorer = WeaveToxicityScorerV1(category_threshold=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FyDQBLbq2R6"
      },
      "outputs": [],
      "source": [
        "result = toxicity_scorer.score(\"The Rams are terrible\")\n",
        "\n",
        "print(f\"Input is toxic: {not result.passed}\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kokDfRD5Jr5g"
      },
      "source": [
        "## Coherence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7U_3I2ulu2io"
      },
      "outputs": [],
      "source": [
        "from weave.scorers import WeaveCoherenceScorerV1\n",
        "\n",
        "coherence_scorer = WeaveCoherenceScorerV1()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Incoherent output"
      ],
      "metadata": {
        "id": "17eOpmEG2nGk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_HxLxov_ANf"
      },
      "outputs": [],
      "source": [
        "result = coherence_scorer.score(\n",
        "    query=\"What is the capital of Antarctica?\",\n",
        "    output=\"but why not monkey up day\"\n",
        ")\n",
        "\n",
        "print(f\"Output is coherent: {result.passed}\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Coherent output"
      ],
      "metadata": {
        "id": "mbazz22C3EyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = coherence_scorer.score(\n",
        "    query=\"What is the capital of Antarctica?\",\n",
        "    output=\"The capital is Sealoinland, a beuatiful city.\"\n",
        ")\n",
        "\n",
        "print(f\"Output is coherent: {result.passed}\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "qQXxgI7n2s4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fluency"
      ],
      "metadata": {
        "id": "wwmsWSlU49e2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from weave.scorers import WeaveFluencyScorerV1\n",
        "\n",
        "fluency_scorer = WeaveFluencyScorerV1()"
      ],
      "metadata": {
        "id": "VtY-qgXm483C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Low fluency"
      ],
      "metadata": {
        "id": "bEqtZDqvjR2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = fluency_scorer.score(\n",
        "    output=\"The cat did stretching lazily into warmth of sunlight.\"\n",
        ")\n",
        "\n",
        "print(f\"Output is fluent: {result.passed}\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "lJEFZ0ct4_pZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "High fluency"
      ],
      "metadata": {
        "id": "J9jhatRE8pet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = fluency_scorer.score(\n",
        "    output=\"The cat stretched lazily in the warm sunlight.\"\n",
        ")\n",
        "\n",
        "print(f\"Output is fluent: {result.passed}\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "uE0DAiyF8pPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trustworthiness\n",
        "The Trustworthiness scorer runs 5 scorers in parallel for an overall assesment of the query, context and input:\n",
        "\n",
        "- 3 \"critical\" scorers: `WeaveToxicityScorer, WeaveHallucinationScorer, WeaveContextRelevanceScorer`\n",
        "\n",
        "- 2 \"advisory\" scorers: `WeaveCoherenceScorer, WeaveFluencyScorer`\n",
        "\n"
      ],
      "metadata": {
        "id": "BPspftP_9UHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from weave.scorers import WeaveTrustScorerV1\n",
        "\n",
        "trust_scorer = WeaveTrustScorerV1()"
      ],
      "metadata": {
        "id": "qXOTkjua9T3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trust_scorer_result(result):\n",
        "  print()\n",
        "  print(f\"Output is trustworthy: {result.passed}\")\n",
        "  print(f\"Trust level: {result.metadata['trust_level']}\")\n",
        "  if not result.passed:\n",
        "    print(\"Triggered scorers:\")\n",
        "    for scorer_name, scorer_data in result.metadata['raw_outputs'].items():\n",
        "      if not scorer_data.passed:\n",
        "        print(f\"  - {scorer_name} did not pass\")\n",
        "    print()\n",
        "\n",
        "  print(f'WeaveToxicityScorerV1 scores: {result.metadata[\"scores\"][\"WeaveToxicityScorerV1\"]}')\n",
        "  print(f'WeaveHallucinationScorerV1 scores: {result.metadata[\"scores\"][\"WeaveHallucinationScorerV1\"]}')\n",
        "  print(f'WeaveContextRelevanceScorerV1 score: {result.metadata[\"scores\"][\"WeaveContextRelevanceScorerV1\"]}')\n",
        "  print(f'WeaveCoherenceScorerV1 score: {result.metadata[\"scores\"][\"WeaveCoherenceScorerV1\"]}')\n",
        "  print(f'WeaveFluencyScorerV1: {result.metadata[\"scores\"][\"WeaveFluencyScorerV1\"]}')\n",
        "  print()"
      ],
      "metadata": {
        "id": "nTniDP7ulKkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 2 issues with the following:\n",
        "- irrelevant context\n",
        "- hallucinated output"
      ],
      "metadata": {
        "id": "mBpD7PywoUVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = trust_scorer.score(\n",
        "    query=\"What is the capital of Antarctica?\",\n",
        "    context=\"People in Antarctica love the penguins.\",\n",
        "    output=\"The cat stretched lazily in the warm sunlight.\"\n",
        ")\n",
        "\n",
        "print_trust_scorer_result(result)\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "QfUcPM7xCIt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Personally Identifiable Information (PII)\n",
        "\n",
        "The PresidioScorer uses Microsoft's [Presidio library](https://microsoft.github.io/presidio/getting_started/) to detect and anonymize PII.\n",
        "\n",
        "Parameters:\n",
        "`selected_entities`: A list of entity types to detect in the text. If now value is passed then presidio will try and detect all entity types in its default entities list\n",
        "\n",
        "`language`: The language of the input text\n",
        "\n",
        "`custom_recognizers`: A list of custom presidio recognizers of type `presidio.EntityRecognizer`"
      ],
      "metadata": {
        "id": "imLou7mEvgPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from weave.scorers import PresidioScorer\n",
        "\n",
        "# first we will use the default list of all entities from Presdio\n",
        "presidio_scorer = PresidioScorer()"
      ],
      "metadata": {
        "id": "X9jWvimdgIpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper function to display results:"
      ],
      "metadata": {
        "id": "jVfK3DtLw4zm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_presidio_output(result):\n",
        "  print(f\"Output contains PII: {not result.passed}\")\n",
        "  print()\n",
        "  print(f\"Anonymized text: {result.metadata['anonymized_text']}\")\n",
        "  print()\n",
        "  print(result.metadata[\"detected_entities\"])\n",
        "  print()\n",
        "  print(result.metadata[\"reason\"])\n",
        "  print()\n",
        "  print(result)"
      ],
      "metadata": {
        "id": "U795S6Qlw642"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the scorer:"
      ],
      "metadata": {
        "id": "g5Kmoxur5keY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = presidio_scorer.score(\n",
        "    output = \"Mary Jane is a software engineer at XYZ company and her email is mary.jane@xyz.com.\"\n",
        ")\n",
        "print_presidio_output(result)"
      ],
      "metadata": {
        "id": "-1RWX-1PrpZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running again, but now only detecting email addresses:"
      ],
      "metadata": {
        "id": "5IBfxIThwqjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "presidio_scorer = PresidioScorer(\n",
        "    selected_entities=[\"EMAIL_ADDRESS\"]\n",
        ")"
      ],
      "metadata": {
        "id": "xWhGFZhIsg0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = presidio_scorer.score(\n",
        "    output = \"Mary Jane is a software engineer at XYZ company and her email is mary.jane@xyz.com.\"\n",
        ")\n",
        "print_presidio_output(result)"
      ],
      "metadata": {
        "id": "X1jTxaPvxBM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HvkNcjEyQxDJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}