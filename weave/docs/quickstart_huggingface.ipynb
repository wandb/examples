{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uk66cbmTQx3B"
      },
      "source": [
        "# Hugging Face\n",
        "\n",
        "[Hugging Face Hub](https://hf.co/) is a machine learning platform for creators and collaborators containing pre-trained models and datasets for your projects.\n",
        "It also offers an easy and unified access to serverless AI inference through multiple inference providers, like [Together AI](https://together.ai), [Sambanova](https://sambanova.ai) and [Fireworks AI](https://fireworks.ai).\n",
        "\n",
        "You can easily browse supported providers and models directly on the Hub - for example, all Fireworks AI supported models can be found [here](https://huggingface.co/models?inference_provider=fireworks-ai&sort=trending).\n",
        "\n",
        "The `huggingface_hub` library provides a simple and efficient interface for running inference on Hugging Face models across providers through the [`InferenceClient`](https://huggingface.co/docs/huggingface_hub/en/package_reference/inference_client).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmRj2HguXO8G"
      },
      "source": [
        "Let's first install `huggingface_hub` and `weave` libraries:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0zegKFmQx3C",
        "outputId": "96b7a519-e51a-4dc3-b805-bfb5b499f558"
      },
      "outputs": [],
      "source": [
        "!pip install -U huggingface_hub -qqq\n",
        "!pip install -U git+https://github.com/wandb/weave.git -qqq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mElL7jxvQx3C"
      },
      "source": [
        "# Authentication\n",
        "\n",
        "With a single [Hugging Face Token](https://huggingface.co/docs/hub/security-tokens), you can access inference through multiple providers. Your calls are routed through Hugging Face and the usage is billed directly to your Hugging Face account at the standard provider API rates.\n",
        "\n",
        "To get started:\n",
        "\n",
        "1. Create your Hugging Face Token at https://huggingface.co/settings/tokens.\n",
        "2. Set the `HF_TOKEN` environment variable by either:\n",
        "   - Adding it to your Google Colab secrets.\n",
        "   - Or using the code below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvmuBwGEQx3C"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = getpass.getpass(\"Enter your Hugging Face Hub Token: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7LsENvrQx3C"
      },
      "source": [
        "# Tracing\n",
        "\n",
        "It’s important to store traces of language model applications in a central location, both during development and in production. These traces can be useful for debugging, and as a dataset that will help you improve your application.\n",
        "\n",
        "To use a model from the Hugging Face Hub, you need to specify the provider when initializing the `InferenceClient` object. You can find the list of supported providers [here](https://huggingface.co/docs/huggingface_hub/en/guides/inference#supported-providers-and-tasks).\n",
        "\n",
        "The following example shows how to use the `Llama-3.2-11B-Vision-Instruct` model through Together AI. Weave will automatically capture traces for [`InferenceClient`](https://huggingface.co/docs/huggingface_hub/en/package_reference/inference_client).\n",
        "\n",
        "To start tracking, call `weave.init()` and use the library as normal.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vCzF91cQx3D",
        "outputId": "11072054-2d3a-4f65-e53b-380cc23c5dbd"
      },
      "outputs": [],
      "source": [
        "import weave\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "\n",
        "weave.init(project_name=\"quickstart-huggingface\")\n",
        "huggingface_client = InferenceClient(provider=\"together\")\n",
        "image_url = \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\"\n",
        "response = huggingface_client.chat_completion(\n",
        "    model=\"meta-llama/Llama-3.2-11B-Vision-Instruct\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n",
        "                {\"type\": \"text\", \"text\": \"Describe this image in one sentence.\"},\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        "    max_tokens=500,\n",
        "    seed=42,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf1382m7Qx3D"
      },
      "source": [
        "## Track your own ops\n",
        "\n",
        "Wrapping a function with @weave.op starts capturing inputs, outputs and app logic so you can debug how data flows through your app. You can deeply nest ops and build a tree of functions that you want to track. This also starts automatically versioning code as you experiment to capture ad-hoc details that haven't been committed to git.\n",
        "\n",
        "Simply create a function decorated with [`@weave.op`](https://weave-docs.wandb.ai/guides/tracking/ops).\n",
        "\n",
        "In the example below, we have the functions `generate_image`, `check_image_correctness`, and `generate_image_and_check_correctness` which are wrapped with `@weave.op` that generates an image and checks if it is correct for a given prompt.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwxi8-OvQx3D"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def encode_image(pil_image):\n",
        "    import io\n",
        "    buffer = io.BytesIO()\n",
        "    pil_image.save(buffer, format=\"JPEG\")\n",
        "    buffer.seek(0)\n",
        "    encoded_image = base64.b64encode(buffer.read()).decode(\"utf-8\")\n",
        "    return f\"data:image/jpeg;base64,{encoded_image}\"\n",
        "\n",
        "\n",
        "@weave.op\n",
        "def generate_image(prompt: str):\n",
        "    return huggingface_client.text_to_image(\n",
        "        prompt=prompt,\n",
        "        model=\"black-forest-labs/FLUX.1-schnell\",\n",
        "        num_inference_steps=4,\n",
        "    )\n",
        "\n",
        "\n",
        "@weave.op\n",
        "def check_image_correctness(image: Image.Image, image_generation_prompt: str):\n",
        "    return huggingface_client.chat_completion(\n",
        "        model=\"meta-llama/Llama-3.2-11B-Vision-Instruct\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image_url\", \"image_url\": {\"url\": encode_image(image)}},\n",
        "                    {\n",
        "                        \"type\": \"text\",\n",
        "                        \"text\": f\"Is this image correct for the prompt: {image_generation_prompt}? Answer with only one word: yes or no\",\n",
        "                    },\n",
        "                ],\n",
        "            }\n",
        "        ],\n",
        "        max_tokens=500,\n",
        "        seed=42,\n",
        "    ).choices[0].message.content\n",
        "\n",
        "\n",
        "@weave.op\n",
        "def generate_image_and_check_correctness(prompt: str):\n",
        "    image = generate_image(prompt)\n",
        "    return {\n",
        "        \"image\": image,\n",
        "        \"is_correct\": check_image_correctness(image, prompt),\n",
        "    }\n",
        "\n",
        "\n",
        "response = generate_image_and_check_correctness(\"A cute puppy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXtmIoK_Qx3D"
      },
      "source": [
        "## Create a [`Model`](https://weave-docs.wandb.ai/guides/core-types/models) for easier experimentation\n",
        "\n",
        "Organizing experimentation is difficult when there are many moving pieces. By using the [`Model`](https://weave-docs.wandb.ai/guides/core-types/models) class, you can capture and organize the experimental details of your app like your system prompt or the model you're using. This helps organize and compare different iterations of your app.\n",
        "\n",
        "In addition to versioning code and capturing inputs/outputs, a [`Model`](https://weave-docs.wandb.ai/guides/core-types/models) captures structured parameters that control your application’s behavior, making it easy to find what parameters worked best. You can also use Weave a [`Model`](https://weave-docs.wandb.ai/guides/core-types/models) with serve, and [Evaluations](https://weave-docs.wandb.ai/guides/core-types/evaluations).\n",
        "\n",
        "In the example below, you can experiment with `CityVisitRecommender`. Every time you change one of these, you'll get a new version of `CityVisitRecommender`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2fjP9xJQx3D"
      },
      "outputs": [],
      "source": [
        "import rich\n",
        "\n",
        "\n",
        "class CityVisitRecommender(weave.Model):\n",
        "    model: str\n",
        "    temperature: float = 0.7\n",
        "    max_tokens: int = 500\n",
        "    seed: int = 42\n",
        "\n",
        "    @weave.op()\n",
        "    def predict(self, city: str) -> str:\n",
        "        return huggingface_client.chat_completion(\n",
        "            model=self.model,\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are a helpful assistant meant to suggest places to visit in a city\",\n",
        "                },\n",
        "                {\"role\": \"user\", \"content\": city},\n",
        "            ],\n",
        "            max_tokens=self.max_tokens,\n",
        "            temperature=self.temperature,\n",
        "            seed=self.seed,\n",
        "        ).choices[0].message.content\n",
        "\n",
        "\n",
        "city_visit_recommender = CityVisitRecommender(\n",
        "    model=\"meta-llama/Llama-3.2-11B-Vision-Instruct\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=500,\n",
        "    seed=42,\n",
        ")\n",
        "rich.print(city_visit_recommender.predict(\"New York City\"))\n",
        "rich.print(city_visit_recommender.predict(\"Paris\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
