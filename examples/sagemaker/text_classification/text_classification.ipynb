{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/gb6B4ig.png\" width=\"400\" alt=\"Weights & Biases\" />\n",
    "\n",
    "<!--- @wandbcode{sagemaker-hf} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/examples/sagemaker/text_classification/text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with Sagemaker & Weights & Biases\n",
    "\n",
    "This notebook will demonstrate how to:\n",
    "- log the datasets to W&B Tables for EDA\n",
    "- train on the [`banking77`](https://huggingface.co/datasets/banking77) dataset\n",
    "- log experiment results to Weights & Biases\n",
    "- log the validation predictions to W&B Tables for model evaluation\n",
    "- save the raw dataset, processed dtaset and model weights to W&B Artifacts\n",
    "\n",
    "Note, this notebook should be run in a SageMaker notebook instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/Za9P1sr.png\" width=\"400\" alt=\"Weights & Biases\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker is a comprehensive machine learning service. It is a tool that helps data scientists and developers to prepare, build, train, and deploy high-quality machine learning (ML) models by providing a rich set of orchestration tools and features.\n",
    "\n",
    "## Credit\n",
    "\n",
    "This notebook is based on the Hugging Face & AWS SageMaker examples that can be [found here](https://huggingface.co/docs/sagemaker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qqq wandb --upgrade\n",
    "!pip install -qq \"sagemaker>=2.48.0\" \"transformers>=4.6.1\" \"datasets[s3]>=1.6.2\" --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights & Biases Setup for AWS SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **only** additional piece of setup needed to use W&B with SageMaker is to make your W&B API key available to SageMaker. In this case we save it to a file in the same directory as our training script. This will be named `secrets.env` and W&B will then use this to authenticate on each of the instances that SageMaker spins up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.sagemaker_auth(path=\"scripts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Dataset for Exporatory Analysis in W&B Tables\n",
    "\n",
    "Here we log the `train` and `eval` datasets to separtate W&B Tables. After this is run, we can explore these tables in the W&B UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(name='log_dataset_to_table', project='hf-sagemaker', job_type='TableLogging')\n",
    "\n",
    "raw_dataset = load_dataset('banking77')\n",
    "label_list = raw_dataset['train'].features[\"label\"].names\n",
    "\n",
    "# ✍️ Log the training and eval datasets as a Weights & Biases Tables to Artifacts ✍️\n",
    "for split in ['train','test']:\n",
    "    \n",
    "    ds = raw_dataset[split]\n",
    "    \n",
    "    # Create W&B Table\n",
    "    dataset_table = wandb.Table(columns=['id', 'label_id', 'label', 'text'])\n",
    "\n",
    "    # Ensure different row ids when logging train and eval data\n",
    "    if split == 'test':\n",
    "        idx_step = len(raw_dataset['train'])\n",
    "        nm = 'eval'\n",
    "    else:\n",
    "        idx_step = 0\n",
    "        nm = 'train'\n",
    "\n",
    "    # Add each row of data to the table\n",
    "    for index in range(len(ds)):\n",
    "        idx = index + idx_step\n",
    "        lbl = ds[index]['label']\n",
    "        row = [idx, lbl, label_list[lbl], ds[index]['text']]\n",
    "        dataset_table.add_data(*row)\n",
    "\n",
    "    wandb.log({f'{nm} table': dataset_table})\n",
    "    \n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with SageMaker and W&B\n",
    "\n",
    "### SageMaker Role\n",
    "\n",
    "First we need to get our SageMaker role permissions. If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an Estimator and start a training job\n",
    "Here we will use the `HuggingFace` estimator from SageMaker, which includes an image of the main libraries necessary when training Hugging Face models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "model = 'distilbert-base-uncased'\n",
    "warmup_steps = 100\n",
    "lr = 1e-4\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={\n",
    "    'output_dir': 'tmp',\n",
    "    'overwrite_output_dir': True,\n",
    "    'model_name_or_path': model,\n",
    "    'dataset_name': 'banking77',\n",
    "    'do_train': True,\n",
    "    'per_device_train_batch_size': 16,\n",
    "    'per_device_eval_batch_size': 16,\n",
    "    'gradient_accumulation_steps': 2,\n",
    "    'learning_rate': lr,\n",
    "    'warmup_steps': warmup_steps,\n",
    "    'fp16': True,\n",
    "    'logging_steps': 10,\n",
    "    'max_steps': 1200,\n",
    "    'eval_steps': 100,\n",
    "    'evaluation_strategy' : 'steps',\n",
    "    'save_steps': 600,\n",
    "    'save_total_limit' : 2,\n",
    "    'load_best_model_at_end': True,\n",
    "    'metric_for_best_model': 'accuracy',\n",
    "    'report_to': 'wandb',    # ✍️\n",
    "    }\n",
    "\n",
    "hyperparameters['run_name'] = f\"{model}_{lr}_{warmup_steps}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator = HuggingFace(entry_point='run_text_classification.py',\n",
    "                            source_dir='./scripts',\n",
    "                            instance_type= 'ml.p3.2xlarge', \n",
    "                            instance_count=1,\n",
    "                            role=role,\n",
    "                            transformers_version='4.6',\n",
    "                            pytorch_version='1.7',\n",
    "                            py_version='py36',\n",
    "                            hyperparameters = hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator.fit(wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperParameter Tuning with SageMaker and Weights & Biases\n",
    "\n",
    "We can alsp use SageMaker's `HyperparameterTuner` to run hyperparameter search and log the results to Weights & Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_jobs=50\n",
    "max_parallel_jobs=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'banking77_artifacts'  # Pre-tokenized dataset that will be downloaded from W&B Artifacts\n",
    "model = 'roberta-large'\n",
    "warmup_steps = None\n",
    "lr = 1e-5\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={\n",
    "    'output_dir': 'tmp',\n",
    "    'overwrite_output_dir': True,\n",
    "    'model_name_or_path': model,\n",
    "    'dataset_name': dataset_name,\n",
    "    'do_train': True,\n",
    "    'per_device_train_batch_size': 4,\n",
    "    'per_device_eval_batch_size': 4,\n",
    "    'gradient_accumulation_steps': 8,\n",
    "    'learning_rate': lr,\n",
    "    'warmup_steps': warmup_steps,\n",
    "    'fp16': True,\n",
    "    'logging_steps': 10,\n",
    "    'max_steps': 1200,\n",
    "    'evaluation_strategy' : 'steps',\n",
    "    'eval_steps': 100,\n",
    "    'save_strategy': \"steps\", # \"no\"\n",
    "    'save_steps': 600,\n",
    "    'save_total_limit' : 1,\n",
    "    'load_best_model_at_end': True,\n",
    "    'metric_for_best_model': 'accuracy',\n",
    "    'report_to': 'wandb',    # ✍️\n",
    "    'run_name': 'hpt'  # will set run name \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point='run_text_classification.py',\n",
    "    source_dir='./scripts',\n",
    "    instance_type= 'ml.p3.2xlarge',\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    transformers_version='4.6',\n",
    "    pytorch_version='1.7',\n",
    "    py_version='py36',\n",
    "    hyperparameters = hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {\n",
    "    'learning_rate': ContinuousParameter(1e-5, 1e-4),\n",
    "    'warmup_steps': IntegerParameter(48, 320),\n",
    "    'model_name_or_path': CategoricalParameter([\"google/electra-large-discriminator\",\n",
    "                                                \"roberta-large\", \n",
    "                                                \"albert-large-v2\",\n",
    "                                               ])\n",
    "}\n",
    "\n",
    "objective_metric_name = 'eval_accuracy'\n",
    "objective_type = 'Maximize'\n",
    "metric_definitions = [\n",
    "    {\"Name\": \"train_runtime\", \"Regex\": \"train_runtime.*=\\D*(.*?)\"},\n",
    "    {\"Name\": \"eval_accuracy\", \"Regex\": \"eval_accuracy.*=\\D*(.*?)\"},\n",
    "    {\"Name\": \"eval_loss\", \"Regex\": \"eval_loss.*=\\D*(.*?)\"},\n",
    "]\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    huggingface_estimator,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges,\n",
    "    metric_definitions,\n",
    "    max_jobs=max_jobs,\n",
    "    max_parallel_jobs=max_parallel_jobs,\n",
    "    objective_type=objective_type\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.fit(wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Versioning with W&B Artifacts\n",
    "\n",
    "Weights and Biases Artifacts enable you to log end-to-end training pipelines to ensure your experiments are always reproducible.\n",
    "\n",
    "Data privacy is critical to Weights & Biases and so we support the creation of Artifacts from reference locations such as your own private cloud such as AWS S3 or Google Cloud Storage. Local, on-premises of W&B are also available upon request.\n",
    "\n",
    "By default, W&B stores artifact files in a private Google Cloud Storage bucket located in the United States. All files are encrypted at rest and in transit. For sensitive files, we recommend a private W&B installation or the use of reference artifacts.\n",
    "\n",
    "### Artifacts - Log Raw Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'banking77'\n",
    "dataset_path = Path('data')\n",
    "raw_dataset_path = dataset_path/f'{dataset_name}_raw'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log to W&B Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project='hf-sagemaker', name='log_raw_dataset', job_type='dataset-logging')\n",
    "\n",
    "# Download data and save to disk\n",
    "raw_datasets = load_dataset(dataset_name)\n",
    "raw_datasets.save_to_disk(raw_dataset_path)\n",
    "\n",
    "# Upload data to W&B Artifacts\n",
    "dataset_artifact = wandb.Artifact(f'{dataset_name}_raw', type='raw_dataset')\n",
    "dataset_artifact.add_dir(raw_dataset_path)\n",
    "wandb.log_artifact(dataset_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artifacts - Log Train/Eval Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our train/eval paths\n",
    "train_dataset_path = dataset_path/f'{dataset_name}_train'\n",
    "eval_dataset_path = dataset_path/f'{dataset_name}_eval'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log to W&B Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project='hf-sagemaker', name='log_train_eval_split', job_type='train-eval-split')\n",
    "\n",
    "# Download the raw dataset from W&B Artifacts\n",
    "artifact = wandb.use_artifact('morgan/hf-sagemaker/banking77_raw:v0', type='raw_dataset')\n",
    "artifact_dir = artifact.download(raw_dataset_path)\n",
    "\n",
    "# Load the raw dataset into a Hugging Face Datasets object\n",
    "raw_datasets = load_from_disk(artifact_dir)\n",
    "\n",
    "# Log the train and eval datasets as separate objects\n",
    "for split in ['train', 'test']:\n",
    "    ds = raw_datasets[split]\n",
    "    \n",
    "    if split == 'test':\n",
    "        split = 'eval'\n",
    "        \n",
    "    nm = f'{dataset_name}_{split}'    \n",
    "    \n",
    "    # Save the Hugging Face Datasets object to disk\n",
    "    ds.save_to_disk(dataset_path/nm)\n",
    "\n",
    "    # Upload the train or eval split to W&B Artifacts\n",
    "    artifact = wandb.Artifact(nm, type=f'{split}_dataset')\n",
    "    artifact.add_dir(dataset_path/nm)\n",
    "    wandb.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artifacts - Dataset Preprocessing: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    # Tokenize the texts\n",
    "    result = tokenizer(examples['text'], padding=padding, max_length=max_seq_length, truncation=True)\n",
    "\n",
    "    # Map labels to IDs (not necessary for GLUE tasks)\n",
    "    if \"label\" in examples:\n",
    "        result[\"label\"] = examples[\"label\"]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models we'll be using\n",
    "models = [\"google/electra-large-discriminator\", \"roberta-large\", \"albert-large-v2\"]\n",
    "padding = \"max_length\"    \n",
    "max_seq_length=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "wandb.init(project='hf-sagemaker', name='tokenization', job_type='train-eval-tokenization')\n",
    "\n",
    "for split in ['train', 'eval']:\n",
    "    # Define our train/eval paths\n",
    "    ds_path = dataset_path/f'{dataset_name}_{split}'\n",
    "    \n",
    "    # Download the raw dataset from W&B Artifacts and load to HF Datasets object\n",
    "    artifact = wandb.use_artifact(f'morgan/hf-sagemaker/banking77_{split}:v0', type=f'{split}_dataset')\n",
    "    artifact_dir = artifact.download(ds_path)\n",
    "    dataset = load_from_disk(artifact_dir)\n",
    "    \n",
    "    for model_name in models:\n",
    "        nm = f\"{split}_{model_name.split('/')[-1]}_tokenized\"\n",
    "        pth = ds_path/f'{dataset_name}_{nm}'\n",
    "        \n",
    "        # Get tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        max_seq_length = min(max_seq_length, tokenizer.model_max_length)\n",
    "\n",
    "        # Do tokenization\n",
    "        tok_dataset = dataset.map(preprocess_function, batched=True)\n",
    "        \n",
    "        # Save the Hugging Face Datasets object to disk\n",
    "        tok_dataset.save_to_disk(pth)\n",
    "\n",
    "        # Upload the train or eval split to W&B Artifacts\n",
    "        artifact = wandb.Artifact(nm, type=f'{split}_tokenized_dataset')\n",
    "        artifact.add_dir(pth)\n",
    "        wandb.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "2489627badc7a7cfb9b1fd9058a0cd16669cf762e901e786f01c7f1075020f48"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('ml': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
