{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20860c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import SentencePieceBPETokenizer, SentencePieceUnigramTokenizer\n",
    "from tokenizers.trainers import UnigramTrainer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from transformers import PreTrainedTokenizerFast, PreTrainedTokenizer\n",
    "import datasets\n",
    "import pandas as pd\n",
    "from datasets import load_from_disk\n",
    "from pathlib import Path\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202d2317",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project='protobert', job_type=\"tokenizer_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e472e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_at = run.use_artifact('uniref_1m:latest')\n",
    "dataset_dir = Path(data_at.download())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4be9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = load_from_disk(dataset_dir/'uniref_1m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9cfb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tokenizer = SentencePieceBPETokenizer()\n",
    "tokenizer.train_from_iterator(sample_dataset[\"text\"], vocab_size=1000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c280c1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da91241",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save('proteins-tmp')\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file='proteins-tmp')\n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", 2),\n",
    "    (\"<s>\", 0),\n",
    ")\n",
    "tokenizer.mask_token = \"<mask>\"\n",
    "tokenizer.cls_token = \"</s>\"\n",
    "tokenizer.sep_token = \"<s>\"\n",
    "tokenizer.pad_token = \"<pad>\"\n",
    "tokenizer.unk_token = \"<unk>\"\n",
    "\n",
    "tokenizer.save_pretrained('proteins-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25df71ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = tokenizer('ASDFAFDGADFGADFGHAG')\n",
    "tokenizer.decode(o['input_ids'])\n",
    "for i in o['input_ids']:\n",
    "    print(f'{i}: {tokenizer.decode(i)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedeed59",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_at = wandb.Artifact('uniref_1m_tokenizer', type=\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d2330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_at.add_dir('proteins-base', name='uniref_1m_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1af6ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_artifact(tok_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ff15a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
